{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-v1', 'wikitext-2-v1', 'wikitext-103-raw-v1', 'wikitext-2-raw-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-v1')`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m book_builder \u001B[38;5;241m=\u001B[39m \u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_dataset_builder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwikitext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(book_builder\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdescription)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(book_builder\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits)\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/datasets/load.py:1518\u001B[0m, in \u001B[0;36mload_dataset_builder\u001B[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(error_msg)\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;66;03m# Instantiate the dataset builder\u001B[39;00m\n\u001B[0;32m-> 1518\u001B[0m builder_instance: DatasetBuilder \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1525\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_auth_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_auth_token\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1526\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbuilder_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1527\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/datasets/builder.py:1357\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder.__init__\u001B[0;34m(self, writer_batch_size, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1356\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, writer_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 1357\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m     \u001B[38;5;66;03m# Batch size used by the ArrowWriter\u001B[39;00m\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;66;03m# It defines the number of samples that are kept in memory before writing them\u001B[39;00m\n\u001B[1;32m   1360\u001B[0m     \u001B[38;5;66;03m# and also the length of the arrow chunks\u001B[39;00m\n\u001B[1;32m   1361\u001B[0m     \u001B[38;5;66;03m# None means that the ArrowWriter will use its default value\u001B[39;00m\n\u001B[1;32m   1362\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_writer_batch_size \u001B[38;5;241m=\u001B[39m writer_batch_size \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/datasets/builder.py:322\u001B[0m, in \u001B[0;36mDatasetBuilder.__init__\u001B[0;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001B[0m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data_dir \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    321\u001B[0m     config_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata_dir\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m data_dir\n\u001B[0;32m--> 322\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_builder_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    324\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    325\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    326\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    328\u001B[0m \u001B[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001B[39;00m\n\u001B[1;32m    329\u001B[0m \u001B[38;5;66;03m# Prefill datasetinfo\u001B[39;00m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.virtualenvs/nnets/lib/python3.10/site-packages/datasets/builder.py:450\u001B[0m, in \u001B[0;36mDatasetBuilder._create_builder_config\u001B[0;34m(self, config_name, custom_features, **config_kwargs)\u001B[0m\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBUILDER_CONFIGS) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    449\u001B[0m     example_of_usage \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mload_dataset(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBUILDER_CONFIGS[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 450\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    451\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConfig name is missing.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    452\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mPlease pick one among the available configs: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder_configs\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    453\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mExample of usage:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexample_of_usage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    454\u001B[0m     )\n\u001B[1;32m    455\u001B[0m builder_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBUILDER_CONFIGS[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    456\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo config specified, defaulting to the single config: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbuilder_config\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Config name is missing.\nPlease pick one among the available configs: ['wikitext-103-v1', 'wikitext-2-v1', 'wikitext-103-raw-v1', 'wikitext-2-raw-v1']\nExample of usage:\n\t`load_dataset('wikitext', 'wikitext-103-v1')`"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "book_builder = datasets.load_dataset_builder(\"wikitext\")\n",
    "print(book_builder.info.description)\n",
    "print(book_builder.info.splits)\n",
    "print(book_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-15a8e8a993ee0984.arrow\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-3af97c1d5bae50bf.arrow\n"
     ]
    }
   ],
   "source": [
    "wrapper = DatasetWrapper()\n",
    "\n",
    "wiki = datasets.load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train[:1%]\")\n",
    "wiki = wiki.map(combine_wikitext, batched=True, remove_columns=wiki.column_names)\n",
    "tokenizer = wrapper.get_tokenizer(wiki[wrapper.data_key])\n",
    "tokenized = wiki.map(lambda examples: get_tokens_and_ids(examples, tokenizer, wrapper.data_key, wrapper.ids_key, wrapper.tokens_key), batched=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chunked_tokens', 'chunked_input_ids'],\n    num_rows: 3111\n})"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 19/19 [00:00<00:00, 641.52ba/s]\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.20s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.94ba/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.00s/ba]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/ba]\n"
     ]
    }
   ],
   "source": [
    "wrapper = DatasetWrapper(512)\n",
    "data = wrapper.process_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "{'chunked_tokens': ['▁S',\n  'en',\n  'j',\n  '▁no',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁3',\n  '▁:',\n  '▁',\n  '<unk>',\n  '▁Ch',\n  'ron',\n  'ic',\n  'les',\n  '▁(',\n  '▁Japan',\n  'es',\n  'e',\n  '▁:',\n  '▁3',\n  '▁,',\n  '▁lit',\n  '▁.',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁of',\n  '▁the',\n  '▁Battle',\n  'f',\n  'iel',\n  'd',\n  '▁3',\n  '▁)',\n  '▁,',\n  '▁commonly',\n  '▁referred',\n  '▁to',\n  '▁as',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁Ch',\n  'ron',\n  'ic',\n  'les',\n  '▁III',\n  '▁outside',\n  '▁Japan',\n  '▁,',\n  '▁is',\n  '▁a',\n  '▁t',\n  'act',\n  'ical',\n  '▁ro',\n  'le',\n  '▁@-@',\n  '▁play',\n  'ing',\n  '▁video',\n  '▁game',\n  '▁devel',\n  'oped',\n  '▁by',\n  '▁Se',\n  'ga',\n  '▁and',\n  '▁M',\n  'edia',\n  '.',\n  'V',\n  'ision',\n  '▁for',\n  '▁the',\n  '▁Pl',\n  'ay',\n  'S',\n  't',\n  'ation',\n  '▁Port',\n  'able',\n  '▁.',\n  '▁Releas',\n  'ed',\n  '▁in',\n  '▁January',\n  '▁2011',\n  '▁in',\n  '▁Japan',\n  '▁,',\n  '▁it',\n  '▁is',\n  '▁the',\n  '▁third',\n  '▁game',\n  '▁in',\n  '▁the',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁series',\n  '▁.',\n  '▁Em',\n  'ploy',\n  'ing',\n  '▁the',\n  '▁same',\n  '▁f',\n  'usion',\n  '▁of',\n  '▁t',\n  'act',\n  'ical',\n  '▁and',\n  '▁real',\n  '▁@-@',\n  '▁time',\n  '▁game',\n  'pl',\n  'ay',\n  '▁as',\n  '▁its',\n  '▁pre',\n  'de',\n  'cess',\n  'ors',\n  '▁,',\n  '▁the',\n  '▁story',\n  '▁run',\n  's',\n  '▁par',\n  'alle',\n  'l',\n  '▁to',\n  '▁the',\n  '▁first',\n  '▁game',\n  '▁and',\n  '▁follows',\n  '▁the',\n  '▁\"',\n  '▁N',\n  'am',\n  'el',\n  'ess',\n  '▁\"',\n  '▁,',\n  '▁a',\n  '▁p',\n  'en',\n  'al',\n  '▁milit',\n  'ary',\n  '▁un',\n  'it',\n  '▁ser',\n  'ving',\n  '▁the',\n  '▁nation',\n  '▁of',\n  '▁G',\n  'all',\n  'ia',\n  '▁during',\n  '▁the',\n  '▁S',\n  'ec',\n  'ond',\n  '▁Euro',\n  'p',\n  'an',\n  '▁War',\n  '▁who',\n  '▁perform',\n  '▁secret',\n  '▁black',\n  '▁operations',\n  '▁and',\n  '▁are',\n  '▁p',\n  'itted',\n  '▁against',\n  '▁the',\n  '▁Im',\n  'per',\n  'ial',\n  '▁un',\n  'it',\n  '▁\"',\n  '▁',\n  '<unk>',\n  '▁R',\n  'av',\n  'en',\n  '▁\"',\n  '▁.',\n  '▁\\n',\n  '▁The',\n  '▁game',\n  '▁began',\n  '▁develop',\n  'ment',\n  '▁in',\n  '▁2010',\n  '▁,',\n  '▁c',\n  'arry',\n  'ing',\n  '▁over',\n  '▁a',\n  '▁large',\n  '▁port',\n  'ion',\n  '▁of',\n  '▁the',\n  '▁work',\n  '▁done',\n  '▁on',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁Ch',\n  'ron',\n  'ic',\n  'les',\n  '▁II',\n  '▁.',\n  '▁Wh',\n  'ile',\n  '▁it',\n  '▁retain',\n  'ed',\n  '▁the',\n  '▁standard',\n  '▁feat',\n  'ures',\n  '▁of',\n  '▁the',\n  '▁series',\n  '▁,',\n  '▁it',\n  '▁also',\n  '▁under',\n  'w',\n  'ent',\n  '▁multiple',\n  '▁ad',\n  'just',\n  'ments',\n  '▁,',\n  '▁such',\n  '▁as',\n  '▁making',\n  '▁the',\n  '▁game',\n  '▁more',\n  '▁for',\n  'g',\n  'iving',\n  '▁for',\n  '▁series',\n  '▁new',\n  'com',\n  'ers',\n  '▁.',\n  '▁Char',\n  'ac',\n  'ter',\n  '▁design',\n  'er',\n  '▁',\n  '<unk>',\n  '▁H',\n  'on',\n  'j',\n  'ou',\n  '▁and',\n  '▁compos',\n  'er',\n  '▁H',\n  'it',\n  'os',\n  'h',\n  'i',\n  '▁S',\n  'ak',\n  'im',\n  'ot',\n  'o',\n  '▁both',\n  '▁returned',\n  '▁from',\n  '▁previous',\n  '▁ent',\n  'ries',\n  '▁,',\n  '▁along',\n  '▁with',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁Ch',\n  'ron',\n  'ic',\n  'les',\n  '▁II',\n  '▁director',\n  '▁T',\n  'akes',\n  'h',\n  'i',\n  '▁O',\n  'z',\n  'a',\n  'wa',\n  '▁.',\n  '▁A',\n  '▁large',\n  '▁team',\n  '▁of',\n  '▁writ',\n  'ers',\n  '▁hand',\n  'led',\n  '▁the',\n  '▁s',\n  'cript',\n  '▁.',\n  '▁The',\n  '▁game',\n  \"▁'s\",\n  '▁open',\n  'ing',\n  '▁theme',\n  '▁was',\n  '▁sung',\n  '▁by',\n  '▁May',\n  \"▁'\",\n  'n',\n  '▁.',\n  '▁\\n',\n  '▁It',\n  '▁met',\n  '▁with',\n  '▁pos',\n  'itive',\n  '▁sal',\n  'es',\n  '▁in',\n  '▁Japan',\n  '▁,',\n  '▁and',\n  '▁was',\n  '▁praised',\n  '▁by',\n  '▁both',\n  '▁Japan',\n  'es',\n  'e',\n  '▁and',\n  '▁w',\n  'estern',\n  '▁critics',\n  '▁.',\n  '▁After',\n  '▁release',\n  '▁,',\n  '▁it',\n  '▁received',\n  '▁down',\n  'l',\n  'o',\n  'ad',\n  'able',\n  '▁cont',\n  'ent',\n  '▁,',\n  '▁along',\n  '▁with',\n  '▁an',\n  '▁exp',\n  'and',\n  'ed',\n  '▁edition',\n  '▁in',\n  '▁November',\n  '▁of',\n  '▁that',\n  '▁year',\n  '▁.',\n  '▁It',\n  '▁was',\n  '▁also',\n  '▁adapted',\n  '▁into',\n  '▁m',\n  'ang',\n  'a',\n  '▁and',\n  '▁an',\n  '▁original',\n  '▁video',\n  '▁an',\n  'im',\n  'ation',\n  '▁series',\n  '▁.',\n  '▁Due',\n  '▁to',\n  '▁low',\n  '▁sal',\n  'es',\n  '▁of',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁Ch',\n  'ron',\n  'ic',\n  'les',\n  '▁II',\n  '▁,',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁Ch',\n  'ron',\n  'ic',\n  'les',\n  '▁III',\n  '▁was',\n  '▁not',\n  '▁local',\n  'ized',\n  '▁,',\n  '▁but',\n  '▁a',\n  '▁fan',\n  '▁transl',\n  'ation',\n  '▁comp',\n  'at',\n  'ible',\n  '▁with',\n  '▁the',\n  '▁game',\n  \"▁'s\",\n  '▁exp',\n  'and',\n  'ed',\n  '▁edition',\n  '▁was',\n  '▁released',\n  '▁in',\n  '▁2014',\n  '▁.',\n  '▁M',\n  'edia',\n  '.',\n  'V',\n  'ision',\n  '▁would',\n  '▁return',\n  '▁to',\n  '▁the',\n  '▁f',\n  'ranch',\n  'ise',\n  '▁with',\n  '▁the',\n  '▁develop',\n  'ment',\n  '▁of',\n  '▁V',\n  'al',\n  'ky',\n  'ri',\n  'a',\n  '▁:',\n  '▁A',\n  'z',\n  'ure',\n  '▁Rev',\n  'ol',\n  'ution',\n  '▁for',\n  '▁the',\n  '▁Pl',\n  'ay',\n  'S',\n  't',\n  'ation'],\n 'chunked_input_ids': [169,\n  127,\n  72,\n  759,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  330,\n  290,\n  113,\n  3,\n  369,\n  631,\n  148,\n  552,\n  248,\n  2895,\n  135,\n  67,\n  290,\n  330,\n  121,\n  965,\n  129,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  137,\n  118,\n  1362,\n  68,\n  2206,\n  66,\n  330,\n  249,\n  121,\n  4158,\n  2813,\n  149,\n  201,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  369,\n  631,\n  148,\n  552,\n  906,\n  3225,\n  2895,\n  121,\n  187,\n  115,\n  114,\n  539,\n  411,\n  507,\n  151,\n  217,\n  801,\n  136,\n  803,\n  3056,\n  2354,\n  2394,\n  250,\n  683,\n  1563,\n  145,\n  186,\n  922,\n  16,\n  56,\n  1578,\n  178,\n  118,\n  2338,\n  234,\n  53,\n  82,\n  204,\n  3120,\n  491,\n  129,\n  3248,\n  124,\n  144,\n  1022,\n  848,\n  144,\n  2895,\n  121,\n  223,\n  187,\n  118,\n  2043,\n  3056,\n  144,\n  118,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  2119,\n  129,\n  1622,\n  2705,\n  136,\n  118,\n  1095,\n  138,\n  3727,\n  137,\n  114,\n  539,\n  411,\n  145,\n  1240,\n  217,\n  581,\n  3056,\n  339,\n  234,\n  201,\n  409,\n  470,\n  456,\n  860,\n  518,\n  121,\n  118,\n  3778,\n  1525,\n  81,\n  3003,\n  3653,\n  74,\n  149,\n  118,\n  613,\n  3056,\n  145,\n  4015,\n  118,\n  146,\n  261,\n  227,\n  240,\n  287,\n  146,\n  121,\n  115,\n  140,\n  127,\n  142,\n  4791,\n  371,\n  417,\n  132,\n  636,\n  1162,\n  118,\n  4841,\n  137,\n  222,\n  321,\n  366,\n  838,\n  118,\n  169,\n  485,\n  516,\n  1890,\n  78,\n  139,\n  2332,\n  565,\n  2756,\n  4094,\n  4073,\n  3262,\n  145,\n  313,\n  140,\n  1807,\n  1355,\n  118,\n  3769,\n  340,\n  430,\n  417,\n  132,\n  146,\n  113,\n  3,\n  221,\n  404,\n  127,\n  146,\n  129,\n  162,\n  183,\n  3056,\n  891,\n  4305,\n  299,\n  144,\n  733,\n  121,\n  134,\n  2257,\n  136,\n  679,\n  115,\n  1440,\n  1388,\n  152,\n  137,\n  118,\n  720,\n  2261,\n  174,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  369,\n  631,\n  148,\n  552,\n  741,\n  129,\n  1195,\n  551,\n  223,\n  2774,\n  124,\n  118,\n  1217,\n  1479,\n  1999,\n  137,\n  118,\n  2119,\n  121,\n  223,\n  455,\n  785,\n  85,\n  158,\n  3418,\n  374,\n  1988,\n  1129,\n  121,\n  611,\n  201,\n  1104,\n  118,\n  3056,\n  421,\n  178,\n  69,\n  1260,\n  178,\n  2119,\n  878,\n  4489,\n  257,\n  129,\n  3924,\n  168,\n  214,\n  899,\n  120,\n  113,\n  3,\n  218,\n  119,\n  72,\n  157,\n  145,\n  2386,\n  120,\n  218,\n  132,\n  271,\n  70,\n  71,\n  169,\n  307,\n  165,\n  190,\n  77,\n  736,\n  3373,\n  272,\n  2828,\n  954,\n  3119,\n  121,\n  2094,\n  213,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  369,\n  631,\n  148,\n  552,\n  741,\n  1648,\n  160,\n  1259,\n  70,\n  71,\n  251,\n  88,\n  63,\n  4591,\n  129,\n  180,\n  1440,\n  2413,\n  137,\n  648,\n  257,\n  3028,\n  992,\n  118,\n  122,\n  1269,\n  129,\n  183,\n  3056,\n  212,\n  1471,\n  136,\n  1916,\n  215,\n  4651,\n  250,\n  844,\n  176,\n  76,\n  129,\n  162,\n  563,\n  649,\n  213,\n  1414,\n  3616,\n  1470,\n  135,\n  144,\n  2895,\n  121,\n  145,\n  215,\n  3335,\n  250,\n  736,\n  2895,\n  135,\n  67,\n  145,\n  128,\n  3626,\n  1142,\n  129,\n  1604,\n  1211,\n  121,\n  223,\n  1445,\n  1055,\n  74,\n  77,\n  184,\n  491,\n  395,\n  158,\n  121,\n  2094,\n  213,\n  197,\n  1333,\n  284,\n  124,\n  1946,\n  144,\n  1450,\n  137,\n  182,\n  901,\n  129,\n  563,\n  215,\n  455,\n  4345,\n  819,\n  150,\n  408,\n  63,\n  145,\n  197,\n  1151,\n  803,\n  197,\n  165,\n  204,\n  2119,\n  129,\n  4957,\n  149,\n  2287,\n  1470,\n  135,\n  137,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  369,\n  631,\n  148,\n  552,\n  741,\n  121,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  369,\n  631,\n  148,\n  552,\n  906,\n  215,\n  283,\n  3733,\n  851,\n  121,\n  405,\n  115,\n  2997,\n  4184,\n  204,\n  391,\n  126,\n  913,\n  213,\n  118,\n  3056,\n  212,\n  1333,\n  284,\n  124,\n  1946,\n  215,\n  1436,\n  144,\n  3169,\n  129,\n  186,\n  922,\n  16,\n  56,\n  1578,\n  559,\n  1645,\n  149,\n  118,\n  138,\n  1743,\n  1003,\n  213,\n  118,\n  4305,\n  299,\n  137,\n  616,\n  142,\n  3525,\n  244,\n  63,\n  290,\n  180,\n  88,\n  351,\n  2759,\n  211,\n  3069,\n  178,\n  118,\n  2338,\n  234,\n  53,\n  82,\n  204]}"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_tokens_and_ids(examples, chunk_size, ids_key, tokens_key):\n",
    "    chunked_ids = []\n",
    "    chunked_tokens = []\n",
    "    for id, token in zip(examples[ids_key], examples[tokens_key]):\n",
    "        chunked_ids += [id[i:i + chunk_size] for i in range(0, len(id), chunk_size)]\n",
    "        chunked_tokens += [token[i:i + chunk_size] for i in range(0, len(token), chunk_size)]\n",
    "    return {f\"chunked_{tokens_key}\": chunked_tokens, f\"chunked_{ids_key}\": chunked_ids}\n",
    "\n",
    "def get_tokens_and_ids(examples, tokenizer, text_key, ids_key, tokens_key):\n",
    "    tokens = []\n",
    "    ids = []\n",
    "    for example in examples[text_key]:\n",
    "        token = tokenizer.tokenize(example, padding=False)\n",
    "        id = tokenizer.convert_tokens_to_ids(token)\n",
    "        tokens.append(token)\n",
    "        ids.append(id)\n",
    "    return {tokens_key: tokens, ids_key: ids}\n",
    "\n",
    "def calc_token_ratio(examples, ids_key, data_key):\n",
    "    ratios = []\n",
    "    for id, data in zip(examples[ids_key], examples[data_key]):\n",
    "        ratios.append(len(id) / len(data))\n",
    "    return {\"token_ratio\": ratios}\n",
    "\n",
    "class DatasetWrapper:\n",
    "    dataset_name = \"wikitext\"\n",
    "    data_config = \"wikitext-103-v1\"\n",
    "    download_split = \"train\"\n",
    "    download_split_pct = \"1%\"\n",
    "    data_key = \"text\"\n",
    "    ids_key = \"input_ids\"\n",
    "    tokens_key = \"tokens\"\n",
    "    max_token_ratio = .33\n",
    "\n",
    "    def __init__(self, model_max_length=512):\n",
    "        self.tokenizer_filename = f\"{self.dataset_name}_{self.download_split_pct}_tokenizer\"\n",
    "        self.model_max_length = model_max_length\n",
    "\n",
    "    def dataset_info(self):\n",
    "        data = datasets.load_dataset_builder(self.dataset_name)\n",
    "        print(data.info.description)\n",
    "        print(data.info.splits)\n",
    "        print(data.info.features)\n",
    "\n",
    "    def process_dataset(self):\n",
    "        data = self.load_dataset()\n",
    "        if self.combine_func is not None:\n",
    "            data = data.map(lambda x: self.combine_func(x, text_key=self.data_key), batched=True, remove_columns=data.column_names)\n",
    "\n",
    "        tokenizer = self.get_tokenizer(data[self.data_key])\n",
    "        tokenized = self.tokenize_dataset(data, tokenizer)\n",
    "        tokenized = self.filter_tokenized_text(tokenized)\n",
    "        data_chunks = self.chunk_tokens(tokenized)\n",
    "        return data_chunks\n",
    "\n",
    "    def combine_func(self, examples, text_key=\"text\"):\n",
    "        entries = []\n",
    "        entry = \"\"\n",
    "        for sentence in examples[text_key]:\n",
    "            if re.match(\"^ \\= \\w\", sentence):\n",
    "                if entry:\n",
    "                    entries.append(entry)\n",
    "                entry = \"\"\n",
    "            else:\n",
    "                entry += sentence\n",
    "        entries.append(entry)\n",
    "        return {text_key: entries}\n",
    "\n",
    "    def load_dataset(self):\n",
    "        split_str = self.download_split\n",
    "        if self.download_split_pct:\n",
    "            split_str = f\"{self.download_split}[:{self.download_split_pct}]\"\n",
    "        if not self.data_config:\n",
    "            data = datasets.load_dataset(self.dataset_name, split=split_str)\n",
    "        else:\n",
    "            data = datasets.load_dataset(self.dataset_name, self.data_config, split=split_str)\n",
    "        return data\n",
    "\n",
    "    def tokenize_dataset(self, data, tokenizer):\n",
    "        data = data.map(lambda examples: get_tokens_and_ids(examples, tokenizer, self.data_key, self.ids_key, self.tokens_key), batched=True)\n",
    "        return data\n",
    "\n",
    "    def filter_tokenized_text(self, data):\n",
    "        data = data.map(lambda x: calc_token_ratio(x, self.ids_key, self.data_key), batched=True)\n",
    "        data = data.filter(lambda x: x[\"token_ratio\"] < self.max_token_ratio)\n",
    "        return data\n",
    "\n",
    "    def chunk_tokens(self, data):\n",
    "        data_chunks = data.map(lambda examples: chunk_tokens_and_ids(examples, self.model_max_length, self.ids_key, self.tokens_key), batched=True, remove_columns=data.column_names)\n",
    "        return data_chunks\n",
    "\n",
    "    def get_tokenizer(self, data=None):\n",
    "        if os.path.exists(self.tokenizer_filename):\n",
    "            tokenizer = self.load_tokenizer(self.tokenizer_filename)\n",
    "        else:\n",
    "            tokenizer = self.train_tokenizer(data, self.tokenizer_filename)\n",
    "        return tokenizer\n",
    "\n",
    "    def train_tokenizer(self, text, save_file, vocab_size=5000, min_frequency=2):\n",
    "        special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
    "        sptokenizer = SentencePieceBPETokenizer()\n",
    "        sptokenizer.train_from_iterator(\n",
    "            text,\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            show_progress=True,\n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_object=sptokenizer, special_tokens=special_tokens)\n",
    "        for attr, token in zip([\"bos_token\", \"pad_token\", \"eos_token\", \"unk_token\"], special_tokens):\n",
    "            setattr(tokenizer, attr, token)\n",
    "            setattr(tokenizer, f\"{attr}_id\", sptokenizer.token_to_id(token))\n",
    "        tokenizer.save_pretrained(save_file)\n",
    "        return tokenizer\n",
    "\n",
    "    def load_tokenizer(self, tokenizer_file):\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_file)\n",
    "        return tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
