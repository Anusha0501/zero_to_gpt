{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified\n",
      " Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike\n",
      " License.\n",
      "\n",
      "{'test': SplitInfo(name='test', num_bytes=1295575, num_examples=4358, shard_lengths=None, dataset_name='wikitext'), 'train': SplitInfo(name='train', num_bytes=545141915, num_examples=1801350, shard_lengths=[1653000, 148350], dataset_name='wikitext'), 'validation': SplitInfo(name='validation', num_bytes=1154751, num_examples=3760, shard_lengths=None, dataset_name='wikitext')}\n",
      "{'text': Value(dtype='string', id=None)}\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "book_builder = datasets.load_dataset_builder(\"wikitext\", \"wikitext-103-v1\")\n",
    "print(book_builder.info.description)\n",
    "print(book_builder.info.splits)\n",
    "print(book_builder.info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_tokens_and_ids(examples, chunk_size, ids_key, tokens_key):\n",
    "    chunked_ids = []\n",
    "    chunked_tokens = []\n",
    "    for id, token in zip(examples[ids_key], examples[tokens_key]):\n",
    "        chunked_ids += [id[i:i + chunk_size] for i in range(0, len(id), chunk_size)]\n",
    "        chunked_tokens += [token[i:i + chunk_size] for i in range(0, len(token), chunk_size)]\n",
    "    return {f\"chunked_{tokens_key}\": chunked_tokens, f\"chunked_{ids_key}\": chunked_ids}\n",
    "\n",
    "def get_tokens_and_ids(examples, tokenizer, text_key, ids_key, tokens_key):\n",
    "    tokens = []\n",
    "    ids = []\n",
    "    for example in examples[text_key]:\n",
    "        token = tokenizer.tokenize(example, padding=False)\n",
    "        id = tokenizer.convert_tokens_to_ids(token)\n",
    "        tokens.append(token)\n",
    "        ids.append(id)\n",
    "    return {tokens_key: tokens, ids_key: ids}\n",
    "\n",
    "def calc_token_ratio(examples, ids_key, data_key):\n",
    "    ratios = []\n",
    "    for id, data in zip(examples[ids_key], examples[data_key]):\n",
    "        ratios.append(len(id) / len(data))\n",
    "    return {\"token_ratio\": ratios}\n",
    "\n",
    "class DatasetWrapper:\n",
    "    dataset_name = \"wikitext\"\n",
    "    data_config = \"wikitext-103-v1\"\n",
    "    download_split = \"train\"\n",
    "    download_split_pct = \"1%\"\n",
    "    data_key = \"text\"\n",
    "    ids_key = \"input_ids\"\n",
    "    tokens_key = \"tokens\"\n",
    "    max_token_ratio = .33\n",
    "\n",
    "    def __init__(self, model_max_length=512, processes=None):\n",
    "        self.tokenizer_filename = f\"{self.dataset_name}_{self.download_split_pct}_tokenizer\"\n",
    "        self.model_max_length = model_max_length\n",
    "        self.processes = processes\n",
    "\n",
    "    def dataset_info(self):\n",
    "        data = datasets.load_dataset_builder(self.dataset_name)\n",
    "        print(data.info.description)\n",
    "        print(data.info.splits)\n",
    "        print(data.info.features)\n",
    "\n",
    "    def process_dataset(self):\n",
    "        data = self.load_dataset()\n",
    "        if self.combine_func is not None:\n",
    "            data = data.map(lambda x: self.combine_func(x, text_key=self.data_key), batched=True, remove_columns=data.column_names)\n",
    "\n",
    "        tokenizer = self.get_tokenizer(data[self.data_key])\n",
    "        tokenized = self.tokenize_dataset(data, tokenizer)\n",
    "        tokenized = self.filter_tokenized_text(tokenized)\n",
    "        data_chunks = self.chunk_tokens(tokenized)\n",
    "        return data_chunks\n",
    "\n",
    "    def combine_func(self, examples, text_key=\"text\"):\n",
    "        entries = []\n",
    "        entry = \"\"\n",
    "        for sentence in examples[text_key]:\n",
    "            if re.match(\"^ \\= \\w\", sentence):\n",
    "                if entry:\n",
    "                    entries.append(entry)\n",
    "                entry = \"\"\n",
    "            else:\n",
    "                entry += sentence\n",
    "        entries.append(entry)\n",
    "        return {text_key: entries}\n",
    "\n",
    "    def load_dataset(self):\n",
    "        split_str = self.download_split\n",
    "        if self.download_split_pct:\n",
    "            split_str = f\"{self.download_split}[:{self.download_split_pct}]\"\n",
    "        if not self.data_config:\n",
    "            data = datasets.load_dataset(self.dataset_name, split=split_str, num_proc=self.processes)\n",
    "        else:\n",
    "            data = datasets.load_dataset(self.dataset_name, self.data_config, split=split_str, num_proc=self.processes)\n",
    "        return data\n",
    "\n",
    "    def tokenize_dataset(self, data, tokenizer):\n",
    "        data = data.map(lambda examples: get_tokens_and_ids(examples, tokenizer, self.data_key, self.ids_key, self.tokens_key), batched=True, num_proc=self.processes)\n",
    "        return data\n",
    "\n",
    "    def filter_tokenized_text(self, data):\n",
    "        data = data.map(lambda x: calc_token_ratio(x, self.ids_key, self.data_key), batched=True, num_proc=self.processes)\n",
    "        data = data.filter(lambda x: x[\"token_ratio\"] < self.max_token_ratio, num_proc=self.processes)\n",
    "        return data\n",
    "\n",
    "    def chunk_tokens(self, data):\n",
    "        data_chunks = data.map(lambda examples: chunk_tokens_and_ids(examples, self.model_max_length, self.ids_key, self.tokens_key), batched=True, remove_columns=data.column_names, num_proc=self.processes)\n",
    "        return data_chunks\n",
    "\n",
    "    def get_tokenizer(self, data=None):\n",
    "        if os.path.exists(self.tokenizer_filename):\n",
    "            tokenizer = self.load_tokenizer(self.tokenizer_filename)\n",
    "        else:\n",
    "            tokenizer = self.train_tokenizer(data, self.tokenizer_filename)\n",
    "        return tokenizer\n",
    "\n",
    "    def train_tokenizer(self, text, save_file, vocab_size=5000, min_frequency=2):\n",
    "        special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
    "        sptokenizer = SentencePieceBPETokenizer()\n",
    "        sptokenizer.train_from_iterator(\n",
    "            text,\n",
    "            vocab_size=vocab_size,\n",
    "            min_frequency=min_frequency,\n",
    "            show_progress=True,\n",
    "            special_tokens=special_tokens\n",
    "        )\n",
    "\n",
    "        tokenizer = PreTrainedTokenizerFast(tokenizer_object=sptokenizer, special_tokens=special_tokens)\n",
    "        for attr, token in zip([\"bos_token\", \"pad_token\", \"eos_token\", \"unk_token\"], special_tokens):\n",
    "            setattr(tokenizer, attr, token)\n",
    "            setattr(tokenizer, f\"{attr}_id\", sptokenizer.token_to_id(token))\n",
    "        tokenizer.save_pretrained(save_file)\n",
    "        return tokenizer\n",
    "\n",
    "    def load_tokenizer(self, tokenizer_file):\n",
    "        tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_file)\n",
    "        return tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "from multiprocessing import Manager\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "def hash_tokens(tokens):\n",
    "    m = MinHash(num_perm=256)\n",
    "    for t in tokens:\n",
    "        m.update(t.encode())\n",
    "    return m\n",
    "\n",
    "def hash_examples(examples, idxs, queue):\n",
    "    hashes = []\n",
    "    for idx, ex in zip(idxs, examples[\"chunked_tokens\"]):\n",
    "        min_hash = hash_tokens(ex)\n",
    "        hashes.append((idx, min_hash))\n",
    "    queue.put(hashes)\n",
    "\n",
    "def index_hashes(examples, index, dup_store):\n",
    "    for example in examples:\n",
    "        idx, min_hash = example\n",
    "        key = idx\n",
    "        if key in index.keys:\n",
    "            continue\n",
    "        process_duplicates(min_hash, key, dup_store, index)\n",
    "        index.insert(key, min_hash)\n",
    "\n",
    "def process_duplicates(min_hash, key, dup_store, index):\n",
    "    close_duplicates = index.query(min_hash)\n",
    "    # Assign input hash to at most one duplicate cluster\n",
    "    if len(close_duplicates) > 0:\n",
    "        for base_duplicate in close_duplicates:\n",
    "            if base_duplicate in dup_store:\n",
    "                dup_store[base_duplicate].add(key)\n",
    "                break\n",
    "        else:\n",
    "            dup_store[close_duplicates[0]].add(key)\n",
    "\n",
    "def find_extremes(cluster, data, thresh=.9):\n",
    "    extremes = set()\n",
    "    for dup in cluster:\n",
    "        for elem in extremes:\n",
    "            d1 = set(data[dup][\"chunked_tokens\"])\n",
    "            d2 = set(data[elem][\"chunked_tokens\"])\n",
    "            sim = (d1 & d2) / (d1 | d2)\n",
    "            if sim > thresh:\n",
    "                break\n",
    "        else:\n",
    "            extremes.add(dup)\n",
    "    return extremes\n",
    "\n",
    "class Deduplicator:\n",
    "    def __init__(self, threshold=.9, num_perm=256, processes=None):\n",
    "        self.index = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        self.dup_store = defaultdict(set)\n",
    "        self.processes = processes\n",
    "\n",
    "    def deduplicate(self, data):\n",
    "        manager = Manager()\n",
    "        hash_queue = manager.Queue()\n",
    "        data.map(lambda xs, idxs: hash_examples(xs, idxs, hash_queue), batched=True, with_indices=True, num_proc=self.processes)\n",
    "        # Used to end the processing later on\n",
    "        hash_queue.put(None)\n",
    "\n",
    "        # Add hashes to queue\n",
    "        while True:\n",
    "            examples = hash_queue.get()\n",
    "            if examples is None:\n",
    "                break\n",
    "            index_hashes(examples, self.index, self.dup_store)\n",
    "\n",
    "        duplicate_indices = set(chain.from_iterable(self.dup_store.values()))\n",
    "        extremes = self.get_extremes(data)\n",
    "        duplicate_indices = duplicate_indices - extremes\n",
    "        filtered = data.filter(lambda x, idx: idx not in duplicate_indices, with_indices=True, num_proc=self.processes)\n",
    "        return filtered\n",
    "\n",
    "    def get_extremes(self, data):\n",
    "        extremes = map(lambda x: find_extremes(x, data), self.dup_store.values())\n",
    "        extremes = set(chain.from_iterable(extremes))\n",
    "        return extremes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-03792397f5d3a78f.arrow\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-1e53970faad85a3a.arrow\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6c1519a2a7c93855.arrow\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-8a573e0ed1fb6b50.arrow\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-e5895f575f882728.arrow\n",
      "100%|██████████| 4/4 [00:11<00:00,  2.81s/ba]\n",
      "Loading cached processed dataset at /Users/vik/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-0f071c60ecbe9384.arrow\n"
     ]
    }
   ],
   "source": [
    "wrapper = DatasetWrapper(512)\n",
    "data = wrapper.process_dataset()\n",
    "\n",
    "dup = Deduplicator()\n",
    "deduped = dup.deduplicate(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['chunked_tokens', 'chunked_input_ids'],\n    num_rows: 3111\n})"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
