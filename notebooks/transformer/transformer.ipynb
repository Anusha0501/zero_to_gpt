{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Based on this paper - https://arxiv.org/pdf/1706.03762.pdf\n",
    "# Might want to move layer norm inside the residual block - https://arxiv.org/pdf/2002.04745.pdf\n",
    "# Layer normalization - https://arxiv.org/pdf/1607.06450.pdf\n",
    "# TODO: Investigate learning rate warmup - https://arxiv.org/abs/2002.04745\n",
    "#!pip install torch torchtext sentencepiece datasets wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "sys.path.append(os.path.abspath(\"../../nnets\"))\n",
    "from net_utils import get_module_list\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 5000\n",
    "TRAIN_SIZE = 5000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset opus_books (/Users/vik/.cache/huggingface/datasets/opus_books/en-es/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Combining:   0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e793cf3cf1674fd9a6551a7e34acdcf3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "Tokenizing:   0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f459b3187305444d83c68d19783272ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filtering:   0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fa458f71eea4a93910c815e0eec4ae4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Filtering:   0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "451b4b3fcea14ba28f0d7b7954646031"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Splitting:   0%|          | 0/5 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "66430370aca9464cb9744972f85944d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wrapper import OpusBooksDataset\n",
    "from padding import PaddingSampler, pad_collate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_wrapper = OpusBooksDataset(tokenizer_vocab=SP_VOCAB_SIZE, download_split_pct=\"5%\")\n",
    "train_dataset = train_wrapper.process_dataset()\n",
    "train_dataset = train_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_split = train_dataset[\"train\"]\n",
    "train = DataLoader(train_split, batch_size=BATCH_SIZE, sampler=PaddingSampler(train_split[\"en_lens\"]), collate_fn=pad_collate)\n",
    "\n",
    "valid_split = train_dataset[\"test\"]\n",
    "valid = DataLoader(valid_split, batch_size=BATCH_SIZE, sampler=PaddingSampler(valid_split[\"en_lens\"]), collate_fn=pad_collate)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.head_units = int(input_units/attention_heads)\n",
    "        self.mask = mask\n",
    "\n",
    "        k = math.sqrt(1/self.input_units)\n",
    "        self.in_proj_weight = nn.Parameter(torch.rand(3, input_units, self.attention_heads * self.head_units) * 2 * k - k)\n",
    "        self.in_proj_bias = nn.Parameter(torch.rand(3, input_units) * 2 * k - k)\n",
    "\n",
    "        self.out_proj_weight = nn.Parameter(torch.rand(self.attention_heads * self.head_units, input_units) * 2 * k - k)\n",
    "        self.out_proj_bias = nn.Parameter(torch.rand(input_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # convert to 4d tensor with batch_size, attn_heads, seq_len, embedding_dim\n",
    "        proj_queries = torch.einsum(\"...se, eo->...so\", queries, self.in_proj_weight[0]) + self.in_proj_bias[0]\n",
    "        proj_queries = proj_queries.view(queries.shape[0], queries.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "\n",
    "        proj_keys = torch.einsum(\"...se, eo->...so\", keys, self.in_proj_weight[1]) + self.in_proj_bias[1]\n",
    "        proj_keys = proj_keys.view(keys.shape[0], keys.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "\n",
    "        proj_values = torch.einsum(\"...se, eo->...so\", values, self.in_proj_weight[2]) + self.in_proj_bias[2]\n",
    "        proj_values = proj_values.view(values.shape[0], values.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "\n",
    "        attention = torch.einsum(\"baqh, bahk->baqk\", proj_queries, torch.transpose(proj_keys, -1, -2)) / np.sqrt(proj_keys.shape[-1])\n",
    "        if self.mask:\n",
    "            # Prevent decoder queries from looking at tokens that come after\n",
    "            # Do this by setting attention to negative infinity, so it is softmaxed to zero in the next step\n",
    "            mask = torch.full((attention.shape[-2], attention.shape[-1]), -torch.inf, device=DEVICE)\n",
    "            attention += torch.triu(mask, diagonal=1)\n",
    "\n",
    "        # Softmax on last dimension\n",
    "        # Sequence-wise softmax, so attention between one sequence and other sequences sums to 1\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        weighted_values = torch.einsum(\"baqk, bake->baqe\", attention, proj_values)\n",
    "\n",
    "        # Swap attention head and sequence axis, then reshape to batch, seq, embedding\n",
    "        weighted_values = weighted_values.swapaxes(1,2).reshape(queries.shape[0], queries.shape[1], -1)\n",
    "        weighted_values = torch.einsum(\"...se, eo->...so\", weighted_values, self.out_proj_weight) + self.out_proj_bias\n",
    "        return weighted_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.mha = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, dropout_p)\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = get_module_list(2, nn.LayerNorm, self.input_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_values = self.dropouts[0](self.mha(x, x, x))\n",
    "        x = self.lns[0](x + weighted_values)\n",
    "\n",
    "        reprojected = self.dropouts[1](self.linear2(self.relu(self.linear1(x))))\n",
    "        x = self.lns[1](x + reprojected)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.hidden_units = hidden_units\n",
    "\n",
    "        self.in_attn = MultiHeadAttention(self.input_units, self.attention_heads, mask=True)\n",
    "        self.context_attn = MultiHeadAttention(self.input_units, self.attention_heads)\n",
    "        self.dropouts = get_module_list(3, nn.Dropout, dropout_p)\n",
    "        self.linear1 = nn.Linear(self.input_units, hidden_units)\n",
    "        self.linear2 = nn.Linear(hidden_units, self.input_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lns = get_module_list(3, nn.LayerNorm, self.input_units)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        weighted_values = self.dropouts[0](self.in_attn(x, x, x))\n",
    "        x = self.lns[0](x + weighted_values)\n",
    "\n",
    "        decoder_values = self.dropouts[1](self.context_attn(x, context, context))\n",
    "        x = self.lns[1](x + decoder_values)\n",
    "\n",
    "        reprojected = self.dropouts[2](self.linear2(self.relu(self.linear1(x))))\n",
    "        x = self.lns[2](x + reprojected)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, attention_heads, padding_idx, max_len=256, blocks=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.hidden_units = hidden_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.blocks = blocks\n",
    "\n",
    "        self.output_embedding = nn.Linear(hidden_units, input_units)\n",
    "        self.embedding = nn.Embedding(input_units, hidden_units, padding_idx=padding_idx)\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, .1)\n",
    "        self.encoders = get_module_list(self.blocks, EncoderBlock, hidden_units, attention_heads)\n",
    "        self.decoders = get_module_list(self.blocks, DecoderBlock, hidden_units, attention_heads)\n",
    "        self.pos_encoding = self.encoding(max_len, self.hidden_units).to(DEVICE)\n",
    "\n",
    "\n",
    "    def forward(self, x, y, enc_outputs=None):\n",
    "        if enc_outputs is None:\n",
    "            # 3D with batch, seq, embeddings\n",
    "            # TODO: Tie input and output embedding weights\n",
    "            enc_outputs = self.dropouts[0](self.embedding(x) + self.pos_encoding[:x.shape[1]])\n",
    "\n",
    "            for i in range(self.blocks):\n",
    "                enc_outputs = self.encoders[i](enc_outputs)\n",
    "\n",
    "        dec_outputs = self.dropouts[1](self.embedding(y) + self.pos_encoding[:y.shape[1]])\n",
    "        for i in range(self.blocks):\n",
    "            dec_outputs = self.decoders[i](dec_outputs, enc_outputs)\n",
    "\n",
    "        token_vectors = self.output_embedding(dec_outputs)\n",
    "        return token_vectors, enc_outputs\n",
    "\n",
    "    def encoding(self, seq_len, embed_len):\n",
    "        encodings = torch.zeros((seq_len, embed_len))\n",
    "        for i in range(seq_len):\n",
    "            all = torch.exp(torch.arange(0, embed_len, 2) * (-math.log(10000.0) / embed_len))\n",
    "            encodings[i, 0::2] = torch.sin(i * all)\n",
    "            encodings[i, 1::2] = torch.cos(i * all)\n",
    "        return encodings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def generate(sequence, pred, target, wrapper):\n",
    "    prompts = wrapper.decode_ids(sequence.cpu())\n",
    "    texts = wrapper.decode_ids(torch.argmax(pred, dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_ids(target.cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mvikp\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/vik/Personal/nnets/notebooks/transformer/wandb/run-20230202_163626-80wu2uxl</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/vikp/transformer/runs/80wu2uxl\" target=\"_blank\">baseline-one-new-data</a></strong> to <a href=\"https://wandb.ai/vikp/transformer\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/vikp/transformer\" target=\"_blank\">https://wandb.ai/vikp/transformer</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/vikp/transformer/runs/80wu2uxl\" target=\"_blank\">https://wandb.ai/vikp/transformer/runs/80wu2uxl</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"transformer\", notes=\"Baseline performance one block updated dataset\", name=\"baseline-one-new-data\")\n",
    "\n",
    "# TODO: Profile and improve perf - https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "model = Transformer(SP_VOCAB_SIZE, 512, 8, blocks=1, padding_idx=train_wrapper.tokenizer.pad_token_id).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=train_wrapper.tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "wandb.watch(model, log_freq=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/77 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72945de22cc64d9e94cc5f51bfa5fa71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.22905117582965207\n",
      "And with this pleasing anticipation, she sat down to reconsider the past, recall the words and endeavour to comprehend all the feelings of Edward; and, of course, to reflect on her own with discontent. | Y con este agradable vaticinio se sentó a reconsiderar el pasado, recordar las palabras e intentar comprender los sentimientos de Edward; y, por supuesto, a reflexionar sobre su propio descontento. | - de de de de de de que de de de de de de de de que de de que de de de de que de de de de que de de que que de que de que la que de de de de que de de de de de\n",
      "She vowed at first she would never trim me up a new bonnet, nor do any thing else for me again, so long as she lived; but now she is quite come to, and we are as good friends as ever. | Primero juró que nunca más volvería a arreglarme ninguna toca nueva ni jamás haría ninguna otra cosa por mí; pero ahora ya se ha aplacado y estamos tan amigas como siempre. | - de de que que de de de de de de que la de de de de que de de de de de de de de de que de que de de de de que de la de de de de de de de de de de de de de\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77efa703daa240cd87416c5b676a132f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.21411480340692732\n",
      "ONE person I was sure would represent me as capable of any thing-- What I felt was dreadful!--My resolution was soon made, and at eight o'clock this morning I was in my carriage. | ¡Lo que sentí fue atroz! Rápidamente tomé una decisión, y hoy a las ocho de la mañana ya me encontraba en mi carruaje. | - de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de\n",
      "The son, a steady respectable young man, was amply provided for by the fortune of his mother, which had been large, and half of which devolved on him on his coming of age. | El hijo, un joven serio y respetable, tenía el futuro asegurado por la fortuna de su madre, que era cuantiosa, y de cuya mitad había entrado en posesión al cumplir su mayoría de edad. | - de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de que de\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/77 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46b02167de3347139016fe653cd484a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "DISPLAY_BATCHES = 2\n",
    "PRINT_VALID = True\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    for batch in tqdm(train):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        sequence = batch[\"en_ids\"].to(torch.long)\n",
    "        target = batch[\"es_ids\"].to(torch.long)\n",
    "        # Setup target properly\n",
    "        prev_target = torch.roll(target, 1, -1)\n",
    "        prev_target[:,0] = train_wrapper.tokenizer.bos_token_id\n",
    "\n",
    "        pred, _ = model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "        # If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\n",
    "        # Similar reshape with target to be batch * sequence vector of class indices\n",
    "        loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.reshape(-1).to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_loss = train_loss / len(train) / BATCH_SIZE\n",
    "        wandb.log({\"loss\": mean_loss})\n",
    "        print(f\"Epoch {epoch} train loss: {mean_loss}\")\n",
    "        sents = generate(sequence, pred, target, train_wrapper)\n",
    "        for sent in sents[:DISPLAY_BATCHES]:\n",
    "            print(sent)\n",
    "\n",
    "        if PRINT_VALID and epoch % 10 ==0:\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            # Deactivate dropout layers\n",
    "            model.eval()\n",
    "            for batch in tqdm(valid):\n",
    "                # Inference token by tokens\n",
    "                sequence = batch[\"en_ids\"].to(torch.long)\n",
    "                target = batch[\"es_ids\"].to(torch.long)\n",
    "                # Setup target properly\n",
    "                prev_target = torch.roll(target, 1, -1)\n",
    "                prev_target[:,0] = train_wrapper.tokenizer.bos_token_id\n",
    "                outputs = prev_target[:,0].unsqueeze(1).to(DEVICE)\n",
    "                enc_outputs = None\n",
    "                # TODO: Investigate memory leak with valid generation\n",
    "                for i in range(target.shape[1]):\n",
    "                    pred, enc_outputs = model(sequence, outputs, enc_outputs=enc_outputs)\n",
    "                    last_output = torch.argmax(pred, dim=2)\n",
    "                    outputs = torch.cat((outputs, last_output[:,-1:]), dim=1)\n",
    "                loss = loss_fn(pred.reshape(-1, pred.shape[-1]), target.view(-1).to(DEVICE))\n",
    "                valid_loss += loss.item()\n",
    "            mean_loss = valid_loss / len(valid) / BATCH_SIZE\n",
    "            wandb.log({\"valid_loss\": mean_loss})\n",
    "            print(f\"Valid loss: {mean_loss}\")\n",
    "            sents = generate(sequence, pred, target, train_wrapper)\n",
    "            for sent in sents[:DISPLAY_BATCHES]:\n",
    "                print(sent)\n",
    "            # Reactivate dropout\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, ) as prof:\n",
    "    model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
