{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Based on this paper - https://arxiv.org/pdf/1706.03762.pdf\n",
    "# Might want to move layer norm inside the residual block - https://arxiv.org/pdf/2002.04745.pdf\n",
    "# Layer normalization - https://arxiv.org/pdf/1607.06450.pdf\n",
    "# TODO: Investigate learning rate warmup - https://arxiv.org/abs/2002.04745\n",
    "#!pip install torch torchtext sentencepiece datasets wandb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vik/.virtualenvs/nnets/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../../data\"))\n",
    "sys.path.append(os.path.abspath(\"../../nnets\"))\n",
    "from net_utils import get_module_list\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "SP_VOCAB_SIZE = 5000\n",
    "TRAIN_SIZE = 5000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/Users/vik/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n",
      "100%|██████████| 3/3 [00:00<00:00, 47.49it/s]\n",
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=tokens.txt --model_prefix=cnn_dailymail --vocab_size=5000 --model_type=unigram\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tokens.txt\n",
      "  input_format: \n",
      "  model_prefix: cnn_dailymail\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 5000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(319) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(174) LOG(INFO) Loading corpus: tokens.txt\n",
      "trainer_interface.cc(375) LOG(INFO) Loaded all 9579 sentences\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(390) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(395) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(456) LOG(INFO) all chars count=479969\n",
      "trainer_interface.cc(467) LOG(INFO) Done: 99.9556% characters are covered.\n",
      "trainer_interface.cc(477) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(478) LOG(INFO) Final character coverage=0.999556\n",
      "trainer_interface.cc(510) LOG(INFO) Done! preprocessed 9579 sentences.\n",
      "unigram_model_trainer.cc(138) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(142) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(193) LOG(INFO) Initialized 29939 seed sentencepieces\n",
      "trainer_interface.cc(516) LOG(INFO) Tokenizing input sentences with whitespace: 9579\n",
      "trainer_interface.cc(526) LOG(INFO) Done! 17665\n",
      "unigram_model_trainer.cc(488) LOG(INFO) Using 17665 sentences for EM training\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=12519 obj=12.7854 num_tokens=36663 num_tokens/piece=2.92859\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=10779 obj=10.9544 num_tokens=37064 num_tokens/piece=3.43854\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=8081 obj=10.9733 num_tokens=39373 num_tokens/piece=4.87229\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=8075 obj=10.9012 num_tokens=39418 num_tokens/piece=4.88149\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=6054 obj=11.1673 num_tokens=43264 num_tokens/piece=7.14635\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=6054 obj=11.0829 num_tokens=43273 num_tokens/piece=7.14784\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=0 size=5500 obj=11.1838 num_tokens=44589 num_tokens/piece=8.10709\n",
      "unigram_model_trainer.cc(504) LOG(INFO) EM sub_iter=1 size=5500 obj=11.1563 num_tokens=44602 num_tokens/piece=8.10945\n",
      "trainer_interface.cc(604) LOG(INFO) Saving model: cnn_dailymail.model\n",
      "trainer_interface.cc(615) LOG(INFO) Saving vocabs: cnn_dailymail.vocab\n"
     ]
    }
   ],
   "source": [
    "from text_data import CNNDatasetDecoderOnly\n",
    "\n",
    "class Wrapper(CNNDatasetDecoderOnly):\n",
    "    split_lengths = [TRAIN_SIZE, math.floor(TRAIN_SIZE * .1), 100]\n",
    "    x_length = 15\n",
    "    target_length = 15\n",
    "\n",
    "wrapper = Wrapper(SP_VOCAB_SIZE)\n",
    "datasets = wrapper.generate_datasets(BATCH_SIZE)\n",
    "train = datasets[\"train\"]\n",
    "valid = datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Add in ROPE embedding\n",
    "class ROPE(nn.Module):\n",
    "    def __init__(self, embedding_dim, seq_len):\n",
    "        super(ROPE, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.cos_embeds = torch.zeros(seq_len, embedding_dim, device=DEVICE)\n",
    "        self.sin_embeds = torch.zeros(seq_len, embedding_dim, device=DEVICE)\n",
    "\n",
    "        embed_pos = 10000 ** (-2 * torch.ceil((torch.arange(0, embedding_dim) + 1) / 2) / embedding_dim)\n",
    "        for i in range(0, seq_len):\n",
    "            self.cos_embeds[i] = torch.cos((i + 1) * embed_pos)\n",
    "            self.sin_embeds[i] = torch.sin((i + 1) * embed_pos)\n",
    "\n",
    "        self.indices = torch.zeros(self.embedding_dim, device=DEVICE, dtype=torch.long)\n",
    "        self.mask = torch.zeros(self.embedding_dim, device=DEVICE, dtype=torch.int)\n",
    "        for i in range(0, embedding_dim, 2):\n",
    "            self.indices[i] = i + 1\n",
    "            self.indices[i+1] = i\n",
    "\n",
    "            self.mask[i] = -1\n",
    "            self.mask[i+1] = 1\n",
    "\n",
    "\n",
    "    def rotate(self, x):\n",
    "        return x[...,self.indices] * self.mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        current_val = x * self.cos_embeds[:x.shape[-2],:]\n",
    "        next_val = self.rotate(x) * self.sin_embeds[:x.shape[-2],:]\n",
    "        return current_val + next_val\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, mask=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_units = input_units\n",
    "        self.attention_heads = attention_heads\n",
    "        self.head_units = int(input_units/attention_heads)\n",
    "        self.mask = mask\n",
    "\n",
    "        k = math.sqrt(1/self.input_units)\n",
    "        # Drop bias\n",
    "        # Single kv head\n",
    "        self.kv_proj_weight = nn.Parameter(torch.rand(2, input_units, self.head_units) * 2 * k - k)\n",
    "        self.q_proj_weight = nn.Parameter(torch.rand(input_units, self.attention_heads * self.head_units) * 2 * k - k)\n",
    "        self.out_proj_weight = nn.Parameter(torch.rand(self.attention_heads * self.head_units, input_units) * 2 * k - k)\n",
    "\n",
    "        # 1024 is max sequence length\n",
    "        self.rope = ROPE(self.head_units, 1024)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        # convert to 4d tensor with batch_size, attn_heads, seq_len, embedding_dim\n",
    "        proj_queries = torch.einsum(\"...se, eo->...so\", queries, self.q_proj_weight)\n",
    "        proj_queries = proj_queries.view(queries.shape[0], queries.shape[1], self.attention_heads, self.head_units).swapaxes(1,2)\n",
    "        proj_queries = self.rope(proj_queries)\n",
    "\n",
    "        proj_keys = torch.einsum(\"...se, eo->...so\", keys, self.kv_proj_weight[0])\n",
    "        proj_keys = proj_keys.view(keys.shape[0], keys.shape[1], self.head_units)\n",
    "        proj_keys = self.rope(proj_keys)\n",
    "\n",
    "        proj_values = torch.einsum(\"...se, eo->...so\", values, self.kv_proj_weight[0])\n",
    "        proj_values = proj_values.view(values.shape[0], values.shape[1], self.head_units)\n",
    "\n",
    "        attention = torch.einsum(\"baqh, bhk->baqk\", proj_queries, torch.transpose(proj_keys, -1, -2)) / np.sqrt(proj_keys.shape[-1])\n",
    "        if self.mask:\n",
    "            # Prevent decoder queries from looking at tokens that come after\n",
    "            # Do this by setting attention to negative infinity, so it is softmaxed to zero in the next step\n",
    "            mask = torch.full((attention.shape[-2], attention.shape[-1]), -torch.inf, device=DEVICE)\n",
    "            attention += torch.triu(mask, diagonal=1)\n",
    "\n",
    "        # Softmax on last dimension\n",
    "        # Sequence-wise softmax, so attention between one sequence and other sequences sums to 1\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        weighted_values = torch.einsum(\"baqk, bke->baqe\", attention, proj_values)\n",
    "\n",
    "        # Swap attention head and sequence axis, then reshape to batch, seq, embedding\n",
    "        weighted_values = weighted_values.swapaxes(1,2).reshape(queries.shape[0], queries.shape[1], -1)\n",
    "        weighted_values = torch.einsum(\"...se, eo->...so\", weighted_values, self.out_proj_weight)\n",
    "        return weighted_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units):\n",
    "        super(SwiGLU, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_units, hidden_units, bias=False)\n",
    "        self.linear2 = nn.Linear(input_units, hidden_units, bias=False)\n",
    "        self.linear3 = nn.Linear(hidden_units, input_units, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.linear1(x)\n",
    "        swish = self.sigmoid(x1) * x1\n",
    "        x2 = self.linear2(x)\n",
    "        swiglu = self.linear3(swish * x2)\n",
    "        return swiglu\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_units, attention_heads, hidden_units=2048, dropout_p=.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.in_attn = MultiHeadAttention(input_units, attention_heads, mask=True)\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, dropout_p)\n",
    "        # Drop bias\n",
    "        self.lns = get_module_list(2, nn.LayerNorm, input_units)\n",
    "        # Switch to swiglu from two linear layers\n",
    "        self.swiglu = SwiGLU(input_units, hidden_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weighted_values = self.dropouts[0](self.in_attn(x, x, x))\n",
    "        # Pre normalization\n",
    "        x = x + self.lns[0](weighted_values)\n",
    "\n",
    "        reprojected = self.dropouts[1](self.swiglu(x))\n",
    "        # Pre normalization\n",
    "        x = x + self.lns[1](reprojected)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, attention_heads, max_len=256, blocks=1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.blocks = blocks\n",
    "        self.dropouts = get_module_list(2, nn.Dropout, .1)\n",
    "        self.decoders = get_module_list(blocks, DecoderBlock, hidden_units, attention_heads)\n",
    "\n",
    "        self.embedding = nn.Parameter(torch.empty(input_units, hidden_units))\n",
    "        nn.init.xavier_uniform_(self.embedding)\n",
    "        self.input_units = input_units\n",
    "\n",
    "    # Tie input output weights\n",
    "    def embed(self, x, reverse=False):\n",
    "        if reverse:\n",
    "            return x @ self.embedding.T\n",
    "        else:\n",
    "            embedded = self.embedding[x.to(torch.long).view(-1)]\n",
    "            return embedded.view(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        dec_outputs = self.dropouts[1](self.embed(x))\n",
    "        for i in range(self.blocks):\n",
    "            dec_outputs = self.decoders[i](dec_outputs)\n",
    "\n",
    "        token_vectors = self.embed(dec_outputs, reverse=True)\n",
    "        return token_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def generate(sequence, pred, target, wrapper):\n",
    "    prompts = wrapper.decode_batch(sequence[:,:wrapper.x_length].cpu())\n",
    "    texts = wrapper.decode_batch(torch.argmax(pred[:,wrapper.x_length:], dim=2).cpu())\n",
    "    correct_texts = wrapper.decode_batch(target[:,wrapper.x_length:].cpu())\n",
    "\n",
    "    displays = []\n",
    "    for p, t, ct in zip(prompts, texts, correct_texts):\n",
    "        displays.append(f\"{p} | {ct} | {t}\")\n",
    "    return displays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:9t1zrup9) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch_time</td><td>▁▃█</td></tr><tr><td>loss</td><td>█▂▁</td></tr><tr><td>valid_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch_time</td><td>43.27716</td></tr><tr><td>loss</td><td>1.09793</td></tr><tr><td>valid_loss</td><td>1.69331</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">new-init</strong> at: <a href=\"https://wandb.ai/vikp/decoder-only/runs/9t1zrup9\" target=\"_blank\">https://wandb.ai/vikp/decoder-only/runs/9t1zrup9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20230129_200230-9t1zrup9/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:9t1zrup9). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.13.9"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/Users/vik/Personal/nnets/notebooks/eff_transformer/wandb/run-20230129_200528-sbp5wxmp</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href=\"https://wandb.ai/vikp/decoder-only/runs/sbp5wxmp\" target=\"_blank\">new-init</a></strong> to <a href=\"https://wandb.ai/vikp/decoder-only\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href=\"https://wandb.ai/vikp/decoder-only\" target=\"_blank\">https://wandb.ai/vikp/decoder-only</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href=\"https://wandb.ai/vikp/decoder-only/runs/sbp5wxmp\" target=\"_blank\">https://wandb.ai/vikp/decoder-only/runs/sbp5wxmp</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "wandb.init(project=\"decoder-only\", notes=\"Custom embed different init\", name=\"new-init\")\n",
    "\n",
    "# TODO: Profile and improve perf - https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "model = Transformer(wrapper.vocab_size, 512, 8, blocks=6).to(DEVICE)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=wrapper.pad_token)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "wandb.watch(model, log_freq=100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:41,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.21963264644145966\n",
      "President Bush calls India's PM to push a proposed nuclear partner | ship . Indian government won confidence vote in face of anger over U. | . .sssss .s . .sss to\n",
      "17-year-old shot by unknown assailant in A | thens suburb of Peristeri . Police said no officers | . . . . . . . . .s-s the- the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.20257370918989182\n",
      "Timothy Stanley: GOP senators' letter to Iranian leaders seem | s extraordinary . But undermining a president's foreign policy | .sssssss of of of the the the the\n",
      "Pope has talked of retirement before, but this time he says he | think papacy will end after no more than five years . Francis | .'''ssssssssss the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train loss: 0.1904661668785687\n",
      "Amnesty International: Taliban first targets unpopular landlords, bureaucrat | s . Taliban spokesman in Swat Valley calls Pakistani government as \"un-I | . . NEW NEW NEW NEW than than .dd thanI1-\n",
      "Comedian Chris Rock to release \"Kill the Messenger\" DVD January | 20 . There are no Barack Obama jokes, Rock says, | . . Ones \"ssssssssss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:41,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train loss: 0.16845574697543836\n",
      "Paula Abdul told Ladies' Home Journal she had | painkiller addiction . She went to spa to wean  | tedsted . Hetent beed beeded\n",
      "Three Austin cave explorers are safe and are out of the cave, officials | said . The University of Texas students went into Airman' | . . . They thed thesss thes the thes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train loss: 0.14628591640242214\n",
      "Rate your favorite U.S. city by taking the America' | s Favorite Cities survey . The survey ends on July | of .g ofs . C . . The 15ss of 15\n",
      "Lifeway stores put Christian magazine behind counter . Magazine featured female | pastors on its cover . Lifeway has no respect for freedom of | ss . London . . Thes: saysss thes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train loss: 0.12625501659409752\n",
      "Fatty particles called triglycerides are | important for heart health . One in every three Americans has high trigly | important by the event . The in Iran 2007 suspect; inleeds\n",
      "Guinea President Lansana Conte dies; 40 days of mourning | declared . Army captain says government institutions dissolve | sd . Theaptainaptains dis in diss ofc of of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train loss: 0.1085823541057521\n",
      "CNN's Sean Callebs talks about his month living | on a food-stamp budget . He lost some weight but learn | to a to .P .d . . Thees the wife to arm\n",
      "NEW: Peterson to media on handcuffs, chains: \" | I got the bling. Can't complain\" Drew Peterson | re thinkt spokes Cant PetersonSts,, Peterson Peterson Peterson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train loss: 0.09393615660996273\n",
      "Israeli offensive caused $1.9 billion in economic destruction in Gaza | , official says . It could take a year for Gaza's economy to | . survey says . U has take for foreign has U iss for has\n",
      "Woman organizes dinners at restaurants for people with food allergie | s . If you have a food allergy, call ahead and tell | s . A rev had have rev have want haves haves and have\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:41,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train loss: 0.08209123503545235\n",
      "Teacher Tracey Wygal was morbidly ob | ese when she weighed 295 pounds . A doctor presc | whe where whe weigh weigh weigh weigh pounds . A weigh poundss weigh\n",
      "\"Several heavy metals\" found in levels abo | ve safe drinking-water standards . TVA pledges cleanup; | health Dec Laden standards standards . Form Laden pledges pledge .;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train loss: 0.07151736380725071\n",
      "U.S. Marshals honor federal judge whose husband and | mother were killed . Family killed by man angry at judge's decision to | were killed killed . One killed man man angry fro css decision,\n",
      "A large chunk of the Wilkins ice shelf | in Antarctica broke away last month . Only a narrow strip of ice | Antarctica Antarctica Antarcticaly and month . On On narrow narrow and of narrowice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train loss: 0.06157481385202243\n",
      "California attorney general's comments are prejudicial, lawyer says | . Brown saying too much about Anna Nicole Smith case, K | . Brownle it too Smithst made protect Nicole Smith is is S\n",
      "Partygoer used make-up to darken skin, went as | escaped prisoner . Acting Immigration chief judged costume contest | escape prisoner prisoner prisoner prisonerand cost prisoner judge judge judge costdum contest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:41,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 train loss: 0.05273360897754801\n",
      "D-Day soldiers remember the horrors of war and fallen comrad | es . One tells how he survived despite being wound | sho . One tells how he survived despite being wound\n",
      "D.C. schools chief Michelle Rhee closed 23 schools, fire | d 36 principals in first year . \"We are always go | d in,rincipals . year . . We are always a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:01,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.33604849874973297\n",
      "Police raided Robert Durst's Houston condo, his lawyer | says . The millionaire real estate heir was arrested over the | ekoe . Ow wasn't in his four-\n",
      "Actor Ashton Kutcher complained on Facebook that men' | s rooms don't have diapering tables . | this week . Bas him sope him to considering fro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 train loss: 0.045656734961887886\n",
      "More than 30 people have died, 50 have been injured since Friday, | reports say . Indigenous people protest government plan to sell land to | ens say . Indiging G Indig protests to to report  to\n",
      "Amnesty International: Taliban first targets unpopular landlords, bureaucrat | s . Taliban spokesman in Swat Valley calls Pakistani government as \"un-I | s . Taliban spokesman in Swat Valley calls as government as \"I-I\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:40,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 train loss: 0.03944810434918979\n",
      "\"No Country for Old Men\" wins four awards, including | best picture and director . Four acting awards go to Europeans: | women picture picture director . Mc Europeann awards were to Europeans go\n",
      "Police say Bruce Jeffrey Pardo had hit list after divorce proceedings were | final . Original target was Pardo's ex-wif | final . Or Orn Or Ord 'sierifwal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:41,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 train loss: 0.033981164570512444\n",
      "Shahid Afridi claims six victims to pave the way | for Pakistan to beat Australia . Pakistan reach required target to win first one | Pakistan beat . beat Australia . Pakistan- required target to win one\n",
      "Lord Taylor of Blackburn and Lord Truscott barred for corruption charges | . They allegedly agreed to take cash to influence specific legislation | . They agree agree agreed to influence to to meet specific toing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [00:42,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 train loss: 0.02959378093224147\n",
      "NEW: Preacher says he found funeral for mom, four kids \" | difficult\" 300 mourners attended service at Redlands Community | difficult\" 300 mournes attended service at Redlands Community\n",
      "Elkhart, Indiana, entrepreneurs sweating out tough economic times . | Several express doubt the federal government can get commerce moving | Several express doubt in federal fed can com commerce coming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [00:24,  3.44it/s]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "DISPLAY_BATCHES = 2\n",
    "OUT_SEQUENCE_LEN = wrapper.y_length\n",
    "PRINT_VALID = True\n",
    "ACCUMULATE_STEPS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Run over the training examples\n",
    "    train_loss = 0\n",
    "    match_pct = 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    start = time.time()\n",
    "    for batch, (sequence, target, prev_target) in tqdm(enumerate(train)):\n",
    "        pred = model(sequence.to(DEVICE))\n",
    "\n",
    "        # If you use a batch, need to reshape pred to be batch * sequence, embedding_len to be compatible\n",
    "        # Similar reshape with target to be batch * sequence vector of class indices\n",
    "        cpred = pred[:, wrapper.x_length:]\n",
    "        ctarget = target[:, wrapper.x_length:]\n",
    "        loss = loss_fn(cpred.reshape(-1, cpred.shape[-1]), ctarget.reshape(-1).to(DEVICE))\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Accumulate gradients\n",
    "        # This seems to perform worse than no accumulation over a\n",
    "        # small data set.  Test with larger set.\n",
    "        if batch % ACCUMULATE_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "    end = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        mean_loss = train_loss / len(train) / BATCH_SIZE\n",
    "        wandb.log({\"loss\": mean_loss, \"epoch_time\": end - start})\n",
    "        print(f\"Epoch {epoch} train loss: {mean_loss}\")\n",
    "        sents = generate(sequence, pred, target, wrapper)\n",
    "        for sent in sents[:DISPLAY_BATCHES]:\n",
    "            print(sent)\n",
    "\n",
    "        if PRINT_VALID and epoch % 10 ==0:\n",
    "            # Compute validation loss.  Unless you have a lot of training data, the validation loss won't decrease.\n",
    "            valid_loss = 0\n",
    "            # Deactivate dropout layers\n",
    "            model.eval()\n",
    "            for batch, (sequence, target, prev_target) in tqdm(enumerate(valid)):\n",
    "                # Inference token by tokens\n",
    "                outputs = sequence[:,:(wrapper.x_length + 1)].to(DEVICE)\n",
    "                # TODO: Investigate memory leak with valid generation\n",
    "                for i in range(OUT_SEQUENCE_LEN):\n",
    "                    pred = model(outputs)\n",
    "                    last_output = torch.argmax(pred, dim=2)\n",
    "                    outputs = torch.cat((outputs, last_output[:,-1:]), dim=1)\n",
    "\n",
    "                cpred = pred[:, wrapper.x_length:]\n",
    "                ctarget = target[:, wrapper.x_length:]\n",
    "                loss = loss_fn(cpred.reshape(-1, cpred.shape[-1]), ctarget.reshape(-1).to(DEVICE))\n",
    "                valid_loss += loss.item()\n",
    "            mean_loss = valid_loss / len(valid) / BATCH_SIZE\n",
    "            wandb.log({\"valid_loss\": mean_loss})\n",
    "            print(f\"Valid loss: {mean_loss}\")\n",
    "            sents = generate(sequence, pred, target, wrapper)\n",
    "            for sent in sents[:DISPLAY_BATCHES]:\n",
    "                print(sent)\n",
    "            # Reactivate dropout\n",
    "            model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, ) as prof:\n",
    "    model(sequence.to(DEVICE), prev_target.to(DEVICE))\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "x = torch.rand(4, 3)\n",
    "x[torch.tensor([0,1,0])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
