{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Based on this paper - https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statistics import mean\n",
    "import math\n",
    "\n",
    "data = pd.read_csv(\"../../data/clean_weather.csv\")\n",
    "data = data.ffill()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "PREDICTORS = [\"tmax\", \"tmin\", \"rain\"]\n",
    "TARGET = \"tmax_tomorrow\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
    "\n",
    "np.random.seed(0)\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Rnn\n",
    "# Input -> hidden\n",
    "# hidden -> hidden\n",
    "# hidden -> output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_conf)):\n",
    "        np.random.seed(0)\n",
    "        k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
    "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "        layers.append(\n",
    "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "        )\n",
    "    return layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    hiddens = []\n",
    "    outputs = []\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
    "        output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
    "        for j in range(x.shape[0]):\n",
    "            input_x = x[j,:][np.newaxis,:] @ i_weight\n",
    "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias\n",
    "            # Activation.  tanh avoids outputs getting larger and larger.\n",
    "            hidden_x = np.tanh(hidden_x)\n",
    "            # Store hidden for use in backprop\n",
    "            hidden[j,:] = hidden_x\n",
    "\n",
    "            # Output layer\n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            output[j,:] = output_x\n",
    "        hiddens.append(hidden)\n",
    "        outputs.append(output)\n",
    "    return hiddens, outputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "def mse(actual, predicted):\n",
    "    return np.mean((actual-predicted)**2)\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return (predicted - actual)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def backward(layers, x, lr, grad, hiddens):\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = hiddens[i]\n",
    "        next_h_grad = None\n",
    "        i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "        for j in range(x.shape[0] - 1, -1, -1):\n",
    "            # 1,1\n",
    "            out_grad = grad[j,:][:,np.newaxis]\n",
    "\n",
    "            # Output updates\n",
    "            # 3x1 @ 1x1 = 3,1\n",
    "            # np.newaxis creates a size 0 axis\n",
    "            o_weight_grad += hidden[j,:][:, np.newaxis] @ out_grad\n",
    "            # 1,1\n",
    "            o_bias_grad += out_grad\n",
    "\n",
    "            # Propagate gradient to hidden unit\n",
    "            # 1,1 @ 1,3 = 1,3\n",
    "            ho_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j < x.shape[0] - 1:\n",
    "                # Then we multiply the gradient by the hidden weights to pull gradient from next hidden state to current hidden state\n",
    "                hh_grad = next_h_grad @ h_weight.T\n",
    "                # Add the gradients together to combine output contribution and hidden contribution\n",
    "                h_grad = hh_grad + ho_grad\n",
    "            else:\n",
    "                h_grad = ho_grad\n",
    "\n",
    "            # Pull the gradient across the current hidden nonlinearity\n",
    "            # derivative of tanh is 1 - tanh(x) ** 2\n",
    "            # So we take the output of tanh (next hidden state), and plug in\n",
    "            tanh_deriv = 1 - hidden[j] ** 2\n",
    "\n",
    "            # next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\n",
    "            # Effect is to pull value across nonlinearity\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "            # Store to compute hh grad for previous sequence position\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            if j > 0:\n",
    "                # 3,1 @ 1,3\n",
    "                # Multiply input from previous layer by post-nonlinearity grad at current layer\n",
    "                h_weight_grad += hidden[j-1][:, np.newaxis] @ h_grad\n",
    "                # (1,3) @ 3,3\n",
    "                h_bias_grad += h_grad\n",
    "\n",
    "            # 3,1 (with newaxis) @ h_grad\n",
    "            i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
    "\n",
    "        i_weight -= i_weight_grad * lr\n",
    "        h_weight -= h_weight_grad * lr\n",
    "        h_bias -= h_bias_grad * lr\n",
    "        o_weight -= o_weight_grad * lr\n",
    "        o_bias -= o_bias_grad * lr\n",
    "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "    return layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train loss 804.9620953730424 valid loss 90.40233703317742\n",
      "Epoch: 1 train loss 82.69713282017419 valid loss 78.47189530714259\n",
      "Epoch: 2 train loss 70.77685712636145 valid loss 49.01585507861538\n",
      "Epoch: 3 train loss 38.41818262690567 valid loss 36.555979232365324\n",
      "Epoch: 4 train loss 31.863380885135314 valid loss 32.561127978468406\n",
      "Epoch: 5 train loss 29.972826562063037 valid loss 31.652696749147317\n",
      "Epoch: 6 train loss 28.60457583374585 valid loss 30.458948692171276\n",
      "Epoch: 7 train loss 26.95771515764718 valid loss 28.992946303213177\n",
      "Epoch: 8 train loss 25.661726291569032 valid loss 27.79304078325396\n",
      "Epoch: 9 train loss 24.83601102014949 valid loss 27.19468236513175\n",
      "Epoch: 10 train loss 24.65229802302576 valid loss 27.497742053032834\n",
      "Epoch: 11 train loss 25.10918752438356 valid loss 28.638203662998805\n",
      "Epoch: 12 train loss 26.05429525371166 valid loss 30.007835732401244\n",
      "Epoch: 13 train loss 26.319164263557646 valid loss 30.24723044182207\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m     hiddens, outputs \u001B[38;5;241m=\u001B[39m forward(seq_x, layers)\n\u001B[1;32m     17\u001B[0m     grad \u001B[38;5;241m=\u001B[39m mse_grad(seq_y, outputs[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 18\u001B[0m     params \u001B[38;5;241m=\u001B[39m \u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhiddens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m     epoch_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m mse(seq_y, outputs[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m     21\u001B[0m sequence_len \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m7\u001B[39m\n",
      "Cell \u001B[0;32mIn[43], line 38\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(layers, x, lr, grad, hiddens)\u001B[0m\n\u001B[1;32m     34\u001B[0m tanh_deriv \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m hidden[j] \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Effect is to pull value across nonlinearity\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m h_grad \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmultiply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh_grad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtanh_deriv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Store to compute hh grad for previous sequence position\u001B[39;00m\n\u001B[1;32m     41\u001B[0m next_h_grad \u001B[38;5;241m=\u001B[39m h_grad\u001B[38;5;241m.\u001B[39mcopy()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "lr = 1e-5\n",
    "\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": 3},\n",
    "    {\"type\": \"rnn\", \"hidden\": 4, \"output\": 1}\n",
    "]\n",
    "layers = init_params(layer_conf)\n",
    "\n",
    "for i in range(epochs):\n",
    "    sequence_len = 7\n",
    "    epoch_loss = 0\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j:(j+sequence_len),]\n",
    "        seq_y = train_y[j:(j+sequence_len),]\n",
    "        hiddens, outputs = forward(seq_x, layers)\n",
    "        grad = mse_grad(seq_y, outputs[0])\n",
    "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
    "        epoch_loss += mse(seq_y, outputs[0])\n",
    "\n",
    "    sequence_len = 7\n",
    "    valid_loss = 0\n",
    "    for j in range(valid_x.shape[0] - sequence_len):\n",
    "        seq_x = valid_x[j:(j+sequence_len),]\n",
    "        seq_y = valid_y[j:(j+sequence_len),]\n",
    "        _, outputs = forward(seq_x, layers)\n",
    "        valid_loss += mse(seq_y, outputs[0])\n",
    "\n",
    "    print(f\"Epoch: {i} train loss {epoch_loss / len(train_x)} valid loss {valid_loss / len(valid_x)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "cosh(x)**(-2)",
      "text/latex": "$\\displaystyle \\frac{1}{\\cosh^{2}{\\left(x \\right)}}$"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, exp, diff, simplify\n",
    "\n",
    "x = symbols(\"x\")\n",
    "tanh = (exp(2 * x) - 1) / (exp(2 * x) + 1)\n",
    "simplify(diff(tanh, x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
