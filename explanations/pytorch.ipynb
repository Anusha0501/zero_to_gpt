{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## PyTorch\n",
    "\n",
    "In the last few lessons, we learned how to build and optimize neural network architectures.  This gave us a grounding in how data flows between layers, how parameters get adjusted, and how loss decreases.  So far, we've been using NumPy to build and optimize our networks.  In this lesson, we'll learn about PyTorch, a framework that makes building and applying neural networks much simpler.\n",
    "\n",
    "We'll start off by taking a look at how PyTorch represents data, and we'll move to building a complete neural network in PyTorch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensors\n",
    "\n",
    "We'll first load in the same house prices dataset from the last lesson.  Each row in this dataset represents a single house.  The predictor columns are:\n",
    "\n",
    "- `interest`: The interest rate\n",
    "- `vacancy`: The vacancy rate\n",
    "- `cpi`: The consumer price index\n",
    "- `price`: The price of a house\n",
    "- `value`: The value of a house\n",
    "- `adj_price`: The price of a house, adjusted for inflation\n",
    "- `adj_value`: The value of a house, adjusted for inflation\n",
    "\n",
    "The predictor columns have all been scaled using the scikit-learn `StandardScaler`.  This gives each column a mean of 0 and a standard deviation of 1.  This makes it easier to activate our nonlinearities.\n",
    "\n",
    "The target column is `next_quarter`, which is the price of the house in three months.  `next_quarter` has been scaled so the minimum value is `0`, and it has been divided by `1000` and rounded to the nearest integer.  This makes the prediction task simpler for our network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "from csv_data import HousePricesDatasetWrapper\n",
    "\n",
    "# Load in data from csv file\n",
    "wrapper = HousePricesDatasetWrapper()\n",
    "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is currently loaded into NumPy arrays.  We can instead load the data into torch tensors.  Tensors are n-dimensional data structures similar to NumPy arrays.  The primary difference is that torch tensors can be loaded onto different devices, like GPUs.  We'll discuss this more later.\n",
    "\n",
    "For now, we'll load our training set predictors and targets into torch tensors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert the numpy arrays to torch tensors\n",
    "train_x = torch.from_numpy(train_data[0])\n",
    "train_y = torch.from_numpy(train_data[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.9451,  1.3964, -1.5228,  ..., -0.1168, -0.1389,  0.8226],\n        [ 1.9325,  1.3964, -1.4935,  ..., -0.1168, -0.1560,  0.8022],\n        [ 1.9955,  1.3964, -1.4935,  ..., -0.1168, -0.0446,  0.8022],\n        ...,\n        [-0.2595, -0.6860,  0.5061,  ...,  0.3840,  0.4345,  0.3539],\n        [-0.2469, -0.6860,  0.5061,  ...,  0.3840,  0.4217,  0.3539],\n        [-0.1839, -0.6860,  0.5061,  ...,  0.3840,  0.6257,  0.3539]],\n       dtype=torch.float64)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tensors work very similarly to NumPy arrays.  You can do operations using scalars:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.9451,  2.3964, -0.5228,  ...,  0.8832,  0.8611,  1.8226],\n        [ 2.9325,  2.3964, -0.4935,  ...,  0.8832,  0.8440,  1.8022],\n        [ 2.9955,  2.3964, -0.4935,  ...,  0.8832,  0.9554,  1.8022],\n        ...,\n        [ 0.7405,  0.3140,  1.5061,  ...,  1.3840,  1.4345,  1.3539],\n        [ 0.7531,  0.3140,  1.5061,  ...,  1.3840,  1.4217,  1.3539],\n        [ 0.8161,  0.3140,  1.5061,  ...,  1.3840,  1.6257,  1.3539]],\n       dtype=torch.float64)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x + 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One important difference is that you want to make sure to use torch functions instead of NumPy methods.  This ensures that the operation is done on the appropriate device.  There are torch equivalents for most NumPy functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.3947, 1.1817,    nan,  ...,    nan,    nan, 0.9070],\n        [1.3901, 1.1817,    nan,  ...,    nan,    nan, 0.8957],\n        [1.4126, 1.1817,    nan,  ...,    nan,    nan, 0.8957],\n        ...,\n        [   nan,    nan, 0.7114,  ..., 0.6196, 0.6591, 0.5949],\n        [   nan,    nan, 0.7114,  ..., 0.6196, 0.6494, 0.5949],\n        [   nan,    nan, 0.7114,  ..., 0.6196, 0.7910, 0.5949]],\n       dtype=torch.float64)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the square root of each value in the array.  Negative values have an undefined square root.\n",
    "torch.sqrt(train_x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autograd\n",
    "\n",
    "One big advantage that Torch has over NumPy for deep learning is autograd.  Autograd will automatically calculate the gradient, without you having to write a backward pass!\n",
    "\n",
    "To do this, we first need to define that parameter that we want a gradient for, then set `requires_grad` to `True`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define a matrix of weights\n",
    "# Torch.rand generates random numbers\n",
    "weights = torch.rand(train_x.shape[1], 1)\n",
    "# Set requires_grad to True so that autograd can work\n",
    "weights.requires_grad = True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can load in our training data and multiply it by the weights.  You may have noticed above that our `train_x` tensor is in `float64`.  This is because the NumPy arrays were in `float64`.  `float64` means that each number is stored using `64` bits of data.  In PyTorch, the default tends to be `float32`, which uses `32` bits to store each number.\n",
    "\n",
    "The main difference is the range of possible values that the number can store."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "1.7976931348623157e+308"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Display the maximum value of float64\n",
    "np.finfo(\"float64\").max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "3.4028235e+38"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the maximum value of float32\n",
    "np.finfo(\"float32\").max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`float32` can store large enough numbers that we rarely have issues when training deep learning models.  Thus, it's much more common to work with `float32` in PyTorch.  There are also times when you'll work with `float16` or `int8`, and we'll cover those in a later lesson.\n",
    "\n",
    "We'll convert our array to `float32`, which is just `torch.float`, since it's the default.  We can then multiply the weights and the `train_x` values:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "train_x = train_x.to(torch.float)\n",
    "predictions = train_x @ weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can find the gradient by finding the loss (mean squared error derivative), then calling `loss.backward()`.  This will automatically backpropagate from `loss` to `weights`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = (predictions - train_y).mean()\n",
    "loss.backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we can display the weight gradient:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.2605],\n        [ 0.4172],\n        [-0.5335],\n        [-0.5425],\n        [-0.5502],\n        [-0.5209],\n        [-0.5167]])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And make the gradient update with a `1e-5` learning rate:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "weights = weights - 1e-5 * weights.grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nn.Module\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also automate the backward pass entirely.  Inherit from nn.Module"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    def __init__(self, input_units, output_units):\n",
    "        super().__init__()\n",
    "\n",
    "        k = math.sqrt(1/input_units)\n",
    "        self.weight = nn.Parameter(torch.rand(input_units, output_units) * 2 * k - k)\n",
    "        self.bias = nn.Parameter(torch.rand(1, output_units) * 2 * k - k)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.weight + self.bias\n",
    "\n",
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        modules = []\n",
    "        for i in range(layers):\n",
    "            in_size = out_size = hidden_units\n",
    "            if i == 0:\n",
    "                in_size = input_units\n",
    "            elif i == layers - 1:\n",
    "                out_size = output_units\n",
    "            modules.append(DenseLayer(in_size, out_size))\n",
    "        self.module_list = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch has dataloaders to make it easy to work with data and batches:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PriceData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.float()\n",
    "        self.y = y.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y\n",
    "\n",
    "train_ds = PriceData(train_x, train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Automatic batching with DataLoader:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train = DataLoader(train_ds, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can run the full train loop:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.68326367922127\n",
      "30.341548889130355\n",
      "23.66963948508104\n",
      "20.26992240929976\n",
      "18.193296501636507\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "epochs = 50\n",
    "layers = 5\n",
    "hidden_size = 25\n",
    "lr = 5e-4\n",
    "\n",
    "net = DenseNetwork(train_x.shape[1], hidden_size, 1, layers)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "def train_loop(net, optimizer, epochs):\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (x, y) in enumerate(train):\n",
    "            optimizer.zero_grad()\n",
    "            pred = net(x)\n",
    "            loss = loss_fn(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(mean(train_losses))\n",
    "\n",
    "train_loop(net, optimizer, epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch also has prebuilt components:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class DenseNetwork(nn.Module):\n",
    "    def __init__(self, input_units, hidden_units, output_units, layers):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        modules = []\n",
    "        for i in range(layers):\n",
    "            in_size = out_size = hidden_units\n",
    "            if i == 0:\n",
    "                in_size = input_units\n",
    "            elif i == layers - 1:\n",
    "                out_size = output_units\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "        self.module_list = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for module in self.module_list:\n",
    "            x = module(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.05724552236497\n",
      "30.849216412380336\n",
      "23.951016113410393\n",
      "20.450236316770315\n",
      "18.319719779416918\n"
     ]
    }
   ],
   "source": [
    "net = DenseNetwork(train_x.shape[1], hidden_size, 1, layers)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "train_loop(net, optimizer, epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch makes it easy to swap components in and out to make a more complex network.  You can pick from:\n",
    "\n",
    "- Different layer types\n",
    "- Optimizers\n",
    "- Schedulers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch also makes your code portable.  So far, we've run on CPU, but PyTorch also lets you run code on a gpu, or on mps (specific to Macs)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.057245976105335\n",
      "30.84921663403511\n",
      "23.95101627384623\n",
      "20.450236380659042\n",
      "18.319719846621155\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "class PriceData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x.float()\n",
    "        self.y = y.float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x.to(device), y.to(device)\n",
    "\n",
    "train_ds = PriceData(train_x, train_y)\n",
    "train = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "net = DenseNetwork(train_x.shape[1], hidden_size, 1, layers).to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "train_loop(net, optimizer, epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll learn about other PyTorch features later, but this is the core set that you'll need."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
