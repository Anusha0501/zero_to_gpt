{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Read"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  en  \\\n0  In the society of his nephew and niece, and th...   \n1  By a former marriage, Mr. Henry Dashwood had o...   \n2  By his own marriage, likewise, which happened ...   \n3  But the fortune, which had been so tardy in co...   \n4  But Mrs. John Dashwood was a strong caricature...   \n\n                                                  es  \n0  En compañía de su sobrino y sobrina, y de los ...  \n1  De un matrimonio anterior, el señor Henry Dash...  \n2  Además, su propio matrimonio, ocurrido poco de...  \n3  Pero la fortuna, que había tardado tanto en ll...  \n4  Pero la señora de John Dashwood era una áspera...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>es</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the society of his nephew and niece, and th...</td>\n      <td>En compañía de su sobrino y sobrina, y de los ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>By a former marriage, Mr. Henry Dashwood had o...</td>\n      <td>De un matrimonio anterior, el señor Henry Dash...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>By his own marriage, likewise, which happened ...</td>\n      <td>Además, su propio matrimonio, ocurrido poco de...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>But the fortune, which had been so tardy in co...</td>\n      <td>Pero la fortuna, que había tardado tanto en ll...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>But Mrs. John Dashwood was a strong caricature...</td>\n      <td>Pero la señora de John Dashwood era una áspera...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "opus = pd.read_csv(\"../data/opus_books.csv\")\n",
    "opus.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create vocab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "token_limit = 11\n",
    "special_tokens = {\n",
    "    \"PAD\": 0,\n",
    "    \"UNK\": 1,\n",
    "    \"BOS\": 2,\n",
    "    \"EOS\": 3\n",
    "}\n",
    "vocab = special_tokens.copy()\n",
    "\n",
    "def clean(text):\n",
    "    # Use re to replace punctuation that is not a comma, question mark, or period with spaces\n",
    "    text = re.sub(r'[^\\w\\s,?.!]',' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Split on consecutive whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]+|[\\s]+', text)\n",
    "    return tokens[:token_limit]\n",
    "\n",
    "opus_tokens = defaultdict(int)\n",
    "for index, row in opus.iterrows():\n",
    "    cleaned = clean(row[\"en\"])\n",
    "    tokens = tokenize(cleaned)\n",
    "    for token in tokens:\n",
    "        opus_tokens[token] += 1\n",
    "\n",
    "counter = 4\n",
    "for index, token in enumerate(opus_tokens):\n",
    "    # Filter out uncommon tokens\n",
    "    # Add unknown token for rare words\n",
    "    if opus_tokens[token] > 2:\n",
    "        vocab[token] = counter\n",
    "        counter += 1\n",
    "    else:\n",
    "        vocab[token] = 1 # Assign unknown id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode(text):\n",
    "    # Encode text as a list of integers\n",
    "    tokens = tokenize(clean(text))\n",
    "    encoded = torch.tensor([vocab[token] for token in tokens])\n",
    "    return encoded\n",
    "\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "for k,v in special_tokens.items():\n",
    "    reverse_vocab[v] = k\n",
    "\n",
    "def decode(encoded):\n",
    "    # Decode a list of integers into text\n",
    "    if isinstance(encoded, torch.Tensor):\n",
    "        encoded = encoded.detach().cpu().tolist()\n",
    "    decoded = \"\".join([reverse_vocab[token] for token in encoded])\n",
    "    return decoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for index, row in opus.iterrows():\n",
    "    # Encode the English sentences\n",
    "    en_text = row[\"en\"]\n",
    "    en = encode(en_text)\n",
    "    if en.shape[0] < token_limit:\n",
    "        continue\n",
    "    tokenized.append(en)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([4, 5, 6, 5, 1, 5, 7, 5, 8, 5, 1])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create torch dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.tokens = torch.vstack(data).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return how many examples are in the dataset\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single training example\n",
    "        x = self.tokens[idx][:10]\n",
    "        y = self.tokens[idx][10]\n",
    "        return x, y\n",
    "\n",
    "# Initialize the dataset\n",
    "train_ds = TextData(tokenized)\n",
    "train = DataLoader(train_ds, batch_size=16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([4, 5, 6, 5, 1, 5, 7, 5, 8, 5]), tensor(1))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[ 4,  5,  6,  5,  1,  5,  7,  5,  8,  5],\n         [ 9,  5, 10,  5, 11,  5, 12, 13,  5, 14],\n         [ 9,  5,  8,  5, 16,  5, 12, 13,  5, 17],\n         [18,  5,  6,  5, 19, 13,  5, 20,  5, 21],\n         [18,  5, 22, 15,  5, 23,  5, 24,  5, 25],\n         [26,  5, 27,  5,  1,  5, 28, 13,  5, 29],\n         [30,  5, 25,  5, 31,  5, 32,  5, 33, 34],\n         [30,  5, 25,  5,  1, 13,  5,  1, 13,  5],\n         [37,  5, 38, 13,  5, 39,  5, 40, 13,  5],\n         [41,  5,  1,  5, 42,  5, 43,  5, 44,  5],\n         [45,  5,  1,  5,  7,  5, 46,  5, 20,  5],\n         [37, 13,  5, 48, 13,  5, 25,  5, 49,  5],\n         [50,  5,  1,  5, 29,  5, 10,  5, 51,  5],\n         [22, 15,  5, 23,  5, 24,  5, 53,  5, 54],\n         [55,  5, 56,  5, 57,  5, 58,  5, 59,  5],\n         [61,  5, 62,  5, 63,  5, 64,  5, 65,  5]]),\n tensor([ 1, 15, 13,  5,  5,  5, 35, 36,  6, 29, 47,  1, 52,  5, 60, 66])]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train))\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        k = 1/math.sqrt(embed_dim)\n",
    "        self.weights =  torch.rand(vocab_size, embed_dim) * 2 * k - k\n",
    "        self.weights[0] = 0 # Zero out the padding embedding\n",
    "        self.weights = nn.Parameter(self.weights)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Return a matrix of embeddings\n",
    "        # We could convert token_ids to a one_hot vector and multiply by the weights, but it is the same as selecting a single row of the matrix\n",
    "        return self.weights[token_ids]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0541, -0.0153,  0.0248,  ...,  0.0610,  0.0496, -0.0178],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [-0.0044,  0.0237,  0.0249,  ...,  0.0321,  0.0519, -0.0526],\n",
      "         ...,\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [-0.0120,  0.0600,  0.0007,  ..., -0.0287, -0.0445,  0.0606],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511]],\n",
      "\n",
      "        [[ 0.0279,  0.0159,  0.0190,  ..., -0.0409, -0.0622, -0.0318],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [ 0.0404,  0.0343, -0.0407,  ...,  0.0346, -0.0573,  0.0159],\n",
      "         ...,\n",
      "         [ 0.0536,  0.0152,  0.0078,  ...,  0.0356,  0.0586, -0.0563],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [ 0.0145,  0.0194, -0.0532,  ...,  0.0224,  0.0009,  0.0574]],\n",
      "\n",
      "        [[ 0.0279,  0.0159,  0.0190,  ..., -0.0409, -0.0622, -0.0318],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [-0.0120,  0.0600,  0.0007,  ..., -0.0287, -0.0445,  0.0606],\n",
      "         ...,\n",
      "         [ 0.0536,  0.0152,  0.0078,  ...,  0.0356,  0.0586, -0.0563],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [-0.0554,  0.0133, -0.0474,  ...,  0.0109,  0.0353, -0.0209]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0553,  0.0051, -0.0504,  ...,  0.0534, -0.0170, -0.0062],\n",
      "         [-0.0603, -0.0114,  0.0204,  ...,  0.0446, -0.0302, -0.0181],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         ...,\n",
      "         [-0.0379,  0.0122, -0.0167,  ..., -0.0567, -0.0083,  0.0151],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [-0.0228, -0.0008,  0.0245,  ..., -0.0348, -0.0224, -0.0256]],\n",
      "\n",
      "        [[-0.0248,  0.0302,  0.0464,  ...,  0.0487,  0.0369,  0.0172],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [ 0.0522,  0.0220,  0.0304,  ...,  0.0122,  0.0312,  0.0110],\n",
      "         ...,\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [-0.0326,  0.0340, -0.0084,  ...,  0.0507,  0.0137, -0.0462],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511]],\n",
      "\n",
      "        [[-0.0258,  0.0196,  0.0442,  ..., -0.0473, -0.0516,  0.0067],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [ 0.0170,  0.0334, -0.0621,  ...,  0.0449,  0.0216, -0.0176],\n",
      "         ...,\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511],\n",
      "         [ 0.0444, -0.0367, -0.0067,  ...,  0.0332, -0.0133, -0.0008],\n",
      "         [ 0.0082,  0.0146,  0.0010,  ..., -0.0208, -0.0350,  0.0511]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_embed = Embedding(len(vocab), 256)\n",
    "    print(input_embed(batch[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict next token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_count, hidden_units):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        self.embedding = Embedding(vocab_size, hidden_units)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units * input_token_count, hidden_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed from (token_count, vocab_size) to (token_count, hidden_size)\n",
    "        embedded = self.embedding(x)\n",
    "        # Run the network\n",
    "        x = self.dense2(self.relu(self.dense1(embedded)))\n",
    "        # Flatten the vectors into one large vector per sentence for the final layer\n",
    "        flat = torch.flatten(x, start_dim=1)\n",
    "        # Run the final layer to get an output\n",
    "        network_out = self.output(flat)\n",
    "        # Unembed, convert to (batch_size, vocab_size).  Argmax against last dim gives predicted token\n",
    "        out_vector = network_out @ self.embedding.weights.T\n",
    "        return out_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# Initialize W&B\n",
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def train_loop(net, optimizer, epochs):\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init(project=\"text\",\n",
    "               name=\"dense\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (x, y) in enumerate(train):\n",
    "            # zero_grad will set all the gradients to zero\n",
    "            # We need this because gradients will accumulate in the backward pass\n",
    "            optimizer.zero_grad()\n",
    "            # Make a prediction using the network\n",
    "            pred = net(x)\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred, y)\n",
    "            # Call loss.backward to run backpropagation\n",
    "            loss.backward()\n",
    "            # Step the optimizer to update the parameters\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            if batch % 10 == 0:\n",
    "                # Log training metrics\n",
    "                wandb.log({\n",
    "                    \"train_loss\": mean(train_losses)\n",
    "                })\n",
    "\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define our hyperparameters\n",
    "epochs = 25\n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize our network\n",
    "net = TokenPredictor(len(vocab), 10, 256)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "losses = train_loop(net, optimizer, epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch = next(iter(train))\n",
    "    pred = net(batch[0])\n",
    "    token_id = pred.argmax(-1)\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        text = decode(batch[0][i])\n",
    "        actual = decode(batch[1][i:(i+1)])\n",
    "        pred = decode(token_id[i:(i+1)])\n",
    "        print(f\"{text}<ACTUAL>{actual}<><PRED>{pred}<>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
