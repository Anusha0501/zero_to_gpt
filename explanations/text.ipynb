{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Working with text\n",
    "\n",
    "In this lesson, we'll learn how to work with text input to neural networks.  This is necessary to build a language model like GPT.\n",
    "\n",
    "Neural networks can't understand text directly, so we need to convert the text into a numerical representation.  How we choose to represent the text can give the model a lot of useful information, and make better predictions.\n",
    "\n",
    "The steps in converting text to a numerical representation are:\n",
    "\n",
    "1. Tokenize the text to convert it into discrete tokens (kind of like words)\n",
    "2. Assign a number to each token\n",
    "3. Convert each token id into a vector representation.  This is called embedding.\n",
    "\n",
    "We can then feed the vectors into a neural network layer.  Here's a diagram:\n",
    "\n",
    "![](images/text/text_to_vec.svg)\n",
    "\n",
    "You might be wondering why we don't directly feed the token ids into the neural network.\n",
    "\n",
    "Embedding enables the network to learn relationships between tokens.  For example, the token id for a `.` might be `2`, and the id for a ` ` might be `7`.  This doesn't help the network understand the relationship between the two tokens.  However, if the vector for a `.` is `[0,1,.1,2]`, and the vector for a ` ` is `[0,1,.1,1]`, this could indicate that the tokens are similar in their function.  Like weights, the embeddings are learned by the network, and will change during training.  Tokens that are conceptually similar will have vectors that are closer together than tokens that aren't."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data\n",
    "\n",
    "We'll be working with a dataset from [Opus books](https://huggingface.co/datasets/opus_books/viewer/en-es/train).  This dataset contains English sentences from books, and their Spanish translations.  We'll use the translation in the next lesson, but in this one, we'll only use the English sentence.\n",
    "\n",
    "There are about 24k sentence pairs in the dataset.  Here's an example:\n",
    "\n",
    "![](images/text/sentences.svg)\n",
    "\n",
    "These sentences in very Old(e) English, but that won't stop our AI from parsing them.  We'll first load in the data using `pandas` and explore it:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  en  \\\n0  In the society of his nephew and niece, and th...   \n1  By a former marriage, Mr. Henry Dashwood had o...   \n2  By his own marriage, likewise, which happened ...   \n3  But the fortune, which had been so tardy in co...   \n4  But Mrs. John Dashwood was a strong caricature...   \n\n                                                  es  \n0  En compañía de su sobrino y sobrina, y de los ...  \n1  De un matrimonio anterior, el señor Henry Dash...  \n2  Además, su propio matrimonio, ocurrido poco de...  \n3  Pero la fortuna, que había tardado tanto en ll...  \n4  Pero la señora de John Dashwood era una áspera...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>es</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the society of his nephew and niece, and th...</td>\n      <td>En compañía de su sobrino y sobrina, y de los ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>By a former marriage, Mr. Henry Dashwood had o...</td>\n      <td>De un matrimonio anterior, el señor Henry Dash...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>By his own marriage, likewise, which happened ...</td>\n      <td>Además, su propio matrimonio, ocurrido poco de...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>But the fortune, which had been so tardy in co...</td>\n      <td>Pero la fortuna, que había tardado tanto en ll...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>But Mrs. John Dashwood was a strong caricature...</td>\n      <td>Pero la señora de John Dashwood era una áspera...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "opus = pd.read_csv(\"../data/opus_books.csv\")\n",
    "opus.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create our vocabulary\n",
    "\n",
    "Now, we need to clean the data and define our token vocabulary.  Our vocabulary is how we map each token to a unique token id.  We'll be creating our own very simple tokenizer and vocabulary.  In practice, you'll use more powerful tokenizers like byte-pair encoding that look at sequences of characters to find the optimal tokenization scheme.\n",
    "\n",
    "We'll first setup some special tokens, that we'll be using ourselves:\n",
    "\n",
    "- `PAD` - this token is used to pad sequences to a given length.  When we're working with text data, sentences won't all be the same length.  However, a neural network needs all rows in a batch to have the same number of columns.  Padding enables us to make all sentences the same length.  We use a special token for this, and tell the network to ignore it.\n",
    "- `UNK` - some tokens don't occur often enough to add them to our vocabulary.  Imagine words like `Octothorpe`, or issues with data quality like `hello123bye`.  These long-tail words will add a lot to our vocabulary (and make our model slower), but don't add much value to the model.  More powerful tokenizers will split these up into individual characters, but in our simple tokenizer, we need `UNK`.\n",
    "- `BOS` - this special token is used to mark the beginning of a sentence, or a sequence.\n",
    "- `EOS` - used to mark the end of a sequence.\n",
    "\n",
    "Some tokenizers, like the GPT-2 tokenizer, use `PAD` instead of `BOS` and `EOS`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "special_tokens = {\n",
    "    \"PAD\": 0,\n",
    "    \"UNK\": 1,\n",
    "    \"BOS\": 2,\n",
    "    \"EOS\": 3\n",
    "}\n",
    "vocab = special_tokens.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "token_limit = 11\n",
    "\n",
    "def clean(text):\n",
    "    # Use re to replace punctuation that is not a comma, question mark, or period with spaces\n",
    "    text = re.sub(r'[^\\w\\s,?.!]',' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Split on consecutive whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]+|[\\s]+', text)\n",
    "    return tokens[:token_limit]\n",
    "\n",
    "opus_tokens = defaultdict(int)\n",
    "for index, row in opus.iterrows():\n",
    "    cleaned = clean(row[\"en\"])\n",
    "    tokens = tokenize(cleaned)\n",
    "    for token in tokens:\n",
    "        opus_tokens[token] += 1\n",
    "\n",
    "counter = 4\n",
    "for index, token in enumerate(opus_tokens):\n",
    "    # Filter out uncommon tokens\n",
    "    # Add unknown token for rare words\n",
    "    if opus_tokens[token] > 1:\n",
    "        vocab[token] = counter\n",
    "        counter += 1\n",
    "    else:\n",
    "        vocab[token] = 1 # Assign unknown id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize sentences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode(text):\n",
    "    # Encode text as a list of integers\n",
    "    tokens = tokenize(clean(text))\n",
    "    encoded = torch.tensor([vocab[token] for token in tokens])\n",
    "    return encoded\n",
    "\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "for k,v in special_tokens.items():\n",
    "    reverse_vocab[v] = k\n",
    "\n",
    "def decode(encoded):\n",
    "    # Decode a list of integers into text\n",
    "    if isinstance(encoded, torch.Tensor):\n",
    "        encoded = encoded.detach().cpu().tolist()\n",
    "    decoded = \"\".join([reverse_vocab[token] for token in encoded])\n",
    "    return decoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for index, row in opus.iterrows():\n",
    "    # Encode the English sentences\n",
    "    en_text = row[\"en\"]\n",
    "    en = encode(en_text)\n",
    "    if en.shape[0] < token_limit:\n",
    "        continue\n",
    "    tokenized.append(en)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4,  5,  6,  5,  7,  5,  8,  5,  9,  5, 10])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create torch dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.tokens = torch.vstack(data).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return how many examples are in the dataset\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single training example\n",
    "        x = self.tokens[idx][:10]\n",
    "        y = self.tokens[idx][10]\n",
    "        return x, y\n",
    "\n",
    "# Initialize the dataset\n",
    "train_ds = TextData(tokenized)\n",
    "train = DataLoader(train_ds, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([4, 5, 6, 5, 7, 5, 8, 5, 9, 5]), tensor(10))"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[  4,   5,   6,   5,   7,   5,   8,   5,   9,   5],\n         [ 11,   5,  12,   5,  13,   5,  14,  15,   5,  16],\n         [ 11,   5,   9,   5,  18,   5,  14,  15,   5,  19],\n         [ 20,   5,   6,   5,  21,  15,   5,  22,   5,  23],\n         [ 20,   5,  24,  17,   5,  25,   5,  26,   5,  27],\n         [ 28,   5,  29,   5,  30,   5,  31,  15,   5,  32],\n         [ 33,   5,  27,   5,  34,   5,  35,   5,  36,  37],\n         [ 33,   5,  27,   5,   1,  15,   5,  39,  15,   5],\n         [ 41,   5,  42,  15,   5,  43,   5,  44,  15,   5],\n         [ 45,   5,  46,   5,  47,   5,  48,   5,  49,   5],\n         [ 50,   5,  51,   5,   8,   5,  52,   5,  22,   5],\n         [ 41,  15,   5,  54,  15,   5,  27,   5,  55,   5],\n         [ 57,   5,   1,   5,  32,   5,  12,   5,  58,   5],\n         [ 24,  17,   5,  25,   5,  26,   5,  60,   5,  61],\n         [ 62,   5,  63,   5,  64,   5,  65,   5,  66,   5],\n         [ 68,   5,  69,   5,  70,   5,  71,   5,  72,   5],\n         [ 74,   5,  72,   5,  75,   5,  76,   5,  77,   5],\n         [ 20,   5,  78,   5,  70,   5,  79,   5,   6,   5],\n         [ 81,   5,  82,   5,  83,   5,  84,   5,  85,   5],\n         [ 87,  15,   5,  88,  15,   5,   1,   5,  89,   5],\n         [ 90,  15,  37,  91,   5,  92,  15,  37,  93,   5],\n         [ 95,  15,   5,  73,   5,  83,   5,  96,  15,  37],\n         [ 50,   5,  98,   5,  99,   5, 100,   5,  94,   5],\n         [101,   5,  70,   5, 102,   5,  76,   5,  12,   5],\n         [104,   5, 105,   5, 106,   5, 107,   5,  75,   5],\n         [109,   5,  78,   5,  72,   5, 110,  37, 111,   5],\n         [113,   5, 114,  15,   5, 115,   5, 116,  15,   5],\n         [118,  37,  35,   5, 119,   5, 120,   5, 119,   5],\n         [ 62,   5,  83,   5,  96,   5,  72,   5, 110,  37],\n         [121,   5, 122,   5, 123,   5,  12,   5, 124,  15],\n         [ 62,   5,  83,   5,  96,  15,  37,  97,   5,  91],\n         [ 20,  15,   5,  88,  15,   5, 125,   5,  24,  17],\n         [  1,   5, 126, 127,   5, 128,   5, 129,   5, 130],\n         [131,   5,   1,   5, 110,   5,  12,   5, 132,   5],\n         [134,   5, 110,   5, 135,   5, 136,   5, 137,   5],\n         [ 45,   5, 120,   5, 139,   5, 140,  15,   5, 141],\n         [101,   5, 119,   5,  31,   5, 141,  15,   5, 142],\n         [143,  15,   5,  73,   5, 144,   5,   6,   5, 145],\n         [119,   5, 146,   5,  96,   5, 119,   5, 147,   5],\n         [149,   5, 128,   5, 150,  15,  37,  97,   5,  16],\n         [151,   5, 152,   5, 135,   5,  69,   5, 153,   5],\n         [155,   5, 128,   5, 156,   5,   1,   5, 157,   5],\n         [159,   5, 160,   5, 161,   5, 162,   5,  83,   5],\n         [109,   5, 164,   5, 165,   5,   8,   5,   6,   5],\n         [167,  37,  35,   5,   6,   5, 168,   5,   8,   5],\n         [ 57,   5, 170,   5, 171,   5,  54,   5, 172,  15],\n         [ 20,   5,  24,  17,   5,  26,   5,  27,   5,   1],\n         [134,   5,  27,   5, 173,   5,  85,   5, 174,   5],\n         [175,   5,  27,   5,  61,   5, 172,  15,   5,  35],\n         [121,   5, 176,   5,  27,   5, 177,  15,   5,  35],\n         [ 45,   5, 178,   5, 179,   5,  73,   5, 180,   5],\n         [181,   5,   9,   5, 182,   5,   1,   5,  32,   5],\n         [ 33,   5,  42,   5, 111,   5,  93,   5,  70,   5],\n         [175,   5,  60,   5,  61,   5, 183,   5,   6,   5],\n         [119,   5, 185,   5, 179,   5, 186,  17, 187, 119],\n         [188,   5, 179, 127,  37, 189,   5, 174,   5, 156],\n         [  4,   5,  12,   5, 190,   5, 191,  15,   5, 128],\n         [192,  15,   5, 193,   5, 194,   5, 195,   5, 108],\n         [196,   5, 194,   5, 197,   5, 198,   5,  12,   5],\n         [ 20,   5, 141,   5, 199,   5, 200,  15,   5,  28],\n         [121,   5, 201,   5, 202,   5, 203,   5,  93,   5],\n         [134,   5, 110,   5, 205,  15,   5,  32,   5, 206],\n         [119,   5,  69,   5,  61,   5,  83,   5, 207,   5],\n         [175,   5,  82,   5, 208,   5, 157,   5, 203,   5]]),\n tensor([ 10,  17,  15,   5,   5,   5,  38,  40,   6,  32,  53,  56,  59,   5,\n          67,  73,  78,  80,  86,  83,  94,  97,   1, 103, 108, 112, 117,  99,\n          35,   5,  15,   5,  37, 133, 138,   5,   5,  15, 148,  17, 154, 158,\n         163, 166, 169,   5,   5,  93,   5,   5,  12,   1,  27, 184,   5,   5,\n           5,   5, 190,  37, 204,   5,  43, 128])]"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train))\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        k = 1/math.sqrt(embed_dim)\n",
    "        self.weights =  torch.rand(vocab_size, embed_dim) * 2 * k - k\n",
    "        self.weights[0] = 0 # Zero out the padding embedding\n",
    "        self.weights = nn.Parameter(self.weights)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Return a matrix of embeddings\n",
    "        # We could convert token_ids to a one_hot vector and multiply by the weights, but it is the same as selecting a single row of the matrix\n",
    "        return self.weights[token_ids]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 6.0825e-02, -2.1130e-02, -2.7503e-02,  ..., -3.9384e-02,\n",
      "           5.0046e-02, -3.2717e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [ 5.1662e-02, -2.1392e-02, -4.6467e-02,  ...,  4.5651e-02,\n",
      "          -2.2346e-02, -3.8449e-02],\n",
      "         ...,\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-1.3514e-02,  3.2856e-02,  3.3172e-02,  ..., -1.1294e-02,\n",
      "          -5.7684e-02,  5.7807e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02]],\n",
      "\n",
      "        [[-1.7709e-02,  3.0644e-02,  5.6269e-02,  ...,  5.9208e-02,\n",
      "           1.6609e-03,  4.6951e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-4.4252e-02,  4.9331e-03, -4.4722e-03,  ...,  1.3196e-02,\n",
      "          -3.1208e-02, -4.1165e-02],\n",
      "         ...,\n",
      "         [ 1.9889e-02,  3.8694e-02, -4.1448e-02,  ...,  2.4977e-02,\n",
      "           7.6745e-03,  1.2119e-03],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-1.8973e-02, -1.0933e-02,  1.6827e-02,  ..., -3.2738e-02,\n",
      "           2.5443e-02,  3.6301e-02]],\n",
      "\n",
      "        [[-1.7709e-02,  3.0644e-02,  5.6269e-02,  ...,  5.9208e-02,\n",
      "           1.6609e-03,  4.6951e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-1.3514e-02,  3.2856e-02,  3.3172e-02,  ..., -1.1294e-02,\n",
      "          -5.7684e-02,  5.7807e-02],\n",
      "         ...,\n",
      "         [ 1.9889e-02,  3.8694e-02, -4.1448e-02,  ...,  2.4977e-02,\n",
      "           7.6745e-03,  1.2119e-03],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-3.5961e-02, -6.0934e-02, -4.8836e-02,  ...,  4.1025e-02,\n",
      "           4.3192e-02, -7.7734e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-2.3624e-02, -2.0716e-02,  2.3017e-02,  ...,  2.7838e-02,\n",
      "           2.5497e-02, -1.2389e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-3.2553e-02,  1.5757e-02,  8.8697e-03,  ...,  4.1352e-02,\n",
      "          -2.8919e-02, -4.6542e-02],\n",
      "         ...,\n",
      "         [-7.5140e-03,  2.5549e-02,  5.2893e-02,  ..., -6.0559e-02,\n",
      "          -7.1883e-05, -2.8070e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-2.8757e-02,  4.1875e-02, -3.5895e-02,  ..., -4.5467e-02,\n",
      "           2.7198e-02, -2.5785e-02]],\n",
      "\n",
      "        [[ 4.8793e-02,  6.2458e-02, -3.9413e-02,  ...,  5.2830e-02,\n",
      "           5.1872e-02, -3.4847e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [ 2.8424e-02,  5.3775e-02,  3.1253e-02,  ..., -4.7685e-02,\n",
      "           5.6310e-02, -3.1137e-02],\n",
      "         ...,\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [-1.3419e-02,  2.1402e-02, -4.8516e-02,  ..., -2.5941e-02,\n",
      "           5.1791e-02,  6.3717e-04],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02]],\n",
      "\n",
      "        [[-5.1247e-02, -5.5546e-02,  1.2647e-04,  ...,  2.0126e-02,\n",
      "           4.8963e-02,  2.4887e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [ 4.6924e-02,  4.7528e-02, -6.7224e-04,  ...,  3.9849e-02,\n",
      "          -5.5805e-02,  8.7996e-04],\n",
      "         ...,\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02],\n",
      "         [ 2.2888e-02,  1.1485e-02,  2.2757e-02,  ...,  7.3261e-03,\n",
      "          -2.4508e-02,  3.5856e-02],\n",
      "         [ 2.2939e-02,  4.0495e-02, -2.1113e-02,  ...,  5.3952e-02,\n",
      "          -5.0945e-02, -2.4746e-02]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_embed = Embedding(len(vocab), 256)\n",
    "    print(input_embed(batch[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict next token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_count, hidden_units):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        self.embedding = Embedding(vocab_size, hidden_units)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units * input_token_count, hidden_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed from (token_count, vocab_size) to (token_count, hidden_size)\n",
    "        embedded = self.embedding(x)\n",
    "        # Run the network\n",
    "        x = self.dense2(self.relu(self.dense1(embedded)))\n",
    "        # Flatten the vectors into one large vector per sentence for the final layer\n",
    "        flat = torch.flatten(x, start_dim=1)\n",
    "        # Run the final layer to get an output\n",
    "        network_out = self.output(flat)\n",
    "        # Unembed, convert to (batch_size, vocab_size).  Argmax against last dim gives predicted token\n",
    "        out_vector = network_out @ self.embedding.weights.T\n",
    "        return out_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# Initialize W&B\n",
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def train_loop(net, optimizer, epochs):\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init(project=\"text\",\n",
    "               name=\"dense\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (x, y) in enumerate(train):\n",
    "            # zero_grad will set all the gradients to zero\n",
    "            # We need this because gradients will accumulate in the backward pass\n",
    "            optimizer.zero_grad()\n",
    "            # Make a prediction using the network\n",
    "            pred = net(x)\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred, y)\n",
    "            # Call loss.backward to run backpropagation\n",
    "            loss.backward()\n",
    "            # Step the optimizer to update the parameters\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            if batch % 10 == 0:\n",
    "                # Log training metrics\n",
    "                wandb.log({\n",
    "                    \"train_loss\": mean(train_losses)\n",
    "                })\n",
    "\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Define our hyperparameters\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize our network\n",
    "net = TokenPredictor(len(vocab), 10, 256)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "losses = train_loop(net, optimizer, epochs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the society of his <ACTUAL>nephew<><PRED>UNK<>\n",
      "By a former marriage, Mr<ACTUAL>.<><PRED> <>\n",
      "By his own marriage, likewise<ACTUAL>,<><PRED> <>\n",
      "But the fortune, which had<ACTUAL> <><PRED> <>\n",
      "But Mrs. John Dashwood was<ACTUAL> <><PRED> <>\n",
      "Marianne s abilities were, in<ACTUAL> <><PRED> <>\n",
      "She was sensible and clever  <ACTUAL>but<><PRED>UNK<>\n",
      "She was UNK, amiable, <ACTUAL>interesting<><PRED>that<>\n",
      "Elinor saw, with concern, <ACTUAL>the<><PRED>the<>\n",
      "They encouraged each other now <ACTUAL>in<><PRED>UNK<>\n",
      "The agony of grief which <ACTUAL>overpowered<><PRED>UNK<>\n",
      "Elinor, too, was deeply <ACTUAL>afflicted<><PRED>the<>\n",
      "A UNK in a place <ACTUAL>where<><PRED>UNK<>\n",
      "Mrs. John Dashwood did not<ACTUAL> <><PRED> <>\n",
      "To take three thousand pounds <ACTUAL>from<><PRED>UNK<>\n",
      "How could he answer it <ACTUAL>to<><PRED>UNK<>\n",
      "Perhaps it would have been <ACTUAL>as<><PRED>UNK<>\n",
      "But as he required the <ACTUAL>promise<><PRED>UNK<>\n",
      "Something must be done for <ACTUAL>them<><PRED>UNK<>\n",
      "Well, then, UNK something <ACTUAL>be<><PRED>UNK<>\n",
      "Consider,  she added,  that <ACTUAL>when<><PRED>the<>\n",
      "Why, to be sure,  <ACTUAL>said<><PRED>said<>\n",
      "The time may come when <ACTUAL>UNK<><PRED>UNK<>\n",
      "If he should have a <ACTUAL>numerous<><PRED>UNK<>\n",
      "What brother on earth would <ACTUAL>do<><PRED>UNK<>\n",
      "And as it is  only <ACTUAL>half<><PRED>the<>\n",
      "No one, at least, <ACTUAL>can<><PRED>the<>\n",
      "Certainly  and I think I <ACTUAL>may<><PRED>UNK<>\n",
      "To be sure it is  <ACTUAL>and<><PRED>UNK<>\n",
      "His wife hesitated a little,<ACTUAL> <><PRED> <>\n",
      "To be sure,  said she<ACTUAL>,<><PRED>,<>\n",
      "But, then, if Mrs.<ACTUAL> <><PRED>  <>\n",
      "UNK years! my dear Fanny<ACTUAL>  <><PRED> <>\n",
      "An UNK is a very <ACTUAL>serious<><PRED>UNK<>\n",
      "It is certainly an unpleasant <ACTUAL>thing<><PRED>UNK<>\n",
      "They think themselves secure, you<ACTUAL> <><PRED> <>\n",
      "If I were you, whatever<ACTUAL> <><PRED> <>\n",
      "Indeed, to say the truth<ACTUAL>,<><PRED> <>\n",
      "I am sure I cannot <ACTUAL>imagine<><PRED>UNK<>\n",
      "Upon my word,  said Mr<ACTUAL>.<><PRED>,<>\n",
      "My father certainly could mean <ACTUAL>nothing<><PRED>UNK<>\n",
      "When my mother UNK into <ACTUAL>another<><PRED>UNK<>\n",
      "Her house will therefore be <ACTUAL>almost<><PRED>UNK<>\n",
      "And yet some of the <ACTUAL>plate<><PRED>UNK<>\n",
      "Yes  and the set of <ACTUAL>breakfast<><PRED>UNK<>\n",
      "A great deal too handsome,<ACTUAL> <><PRED> <>\n",
      "But Mrs. Dashwood was UNK<ACTUAL> <><PRED> <>\n",
      "It was enough for her <ACTUAL>that<><PRED>UNK<>\n",
      "He was not handsome, and<ACTUAL> <><PRED> <>\n",
      "His understanding was good, and<ACTUAL> <><PRED> <>\n",
      "They wanted him to make <ACTUAL>a<><PRED>UNK<>\n",
      "All his wishes UNK in <ACTUAL>UNK<><PRED>UNK<>\n",
      "She saw only that he <ACTUAL>was<><PRED>UNK<>\n",
      "He did not disturb the <ACTUAL>wretchedness<><PRED>UNK<>\n",
      "I love him already.   I<ACTUAL> <><PRED> <>\n",
      "Like him!  replied her mother<ACTUAL> <><PRED> <>\n",
      "In a few months, my<ACTUAL> <><PRED> <>\n",
      "Mama, how shall we do<ACTUAL> <><PRED> <>\n",
      "We shall live within a <ACTUAL>few<><PRED>UNK<>\n",
      "But you look grave, Marianne<ACTUAL>  <><PRED> <>\n",
      "His eyes want all that <ACTUAL>spirit<><PRED>UNK<>\n",
      "It is evident, in spite<ACTUAL> <><PRED> <>\n",
      "I could not be happy <ACTUAL>with<><PRED>UNK<>\n",
      "He must enter into all <ACTUAL>my<><PRED>UNK<>\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch = next(iter(train))\n",
    "    pred = net(batch[0])\n",
    "    token_id = pred.argmax(-1)\n",
    "\n",
    "    for i in range(len(batch[0])):\n",
    "        text = decode(batch[0][i])\n",
    "        actual = decode(batch[1][i:(i+1)])\n",
    "        pred = decode(token_id[i:(i+1)])\n",
    "        print(f\"{text}<ACTUAL>{actual}<><PRED>{pred}<>\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  4,   5,   6,   5,   7,   5,   8,   5,   9,   5],\n        [ 11,   5,  12,   5,  13,   5,  14,  15,   5,  16],\n        [ 11,   5,   9,   5,  18,   5,  14,  15,   5,  19],\n        [ 20,   5,   6,   5,  21,  15,   5,  22,   5,  23],\n        [ 20,   5,  24,  17,   5,  25,   5,  26,   5,  27],\n        [ 28,   5,  29,   5,  30,   5,  31,  15,   5,  32],\n        [ 33,   5,  27,   5,  34,   5,  35,   5,  36,  37],\n        [ 33,   5,  27,   5,   1,  15,   5,  39,  15,   5],\n        [ 41,   5,  42,  15,   5,  43,   5,  44,  15,   5],\n        [ 45,   5,  46,   5,  47,   5,  48,   5,  49,   5],\n        [ 50,   5,  51,   5,   8,   5,  52,   5,  22,   5],\n        [ 41,  15,   5,  54,  15,   5,  27,   5,  55,   5],\n        [ 57,   5,   1,   5,  32,   5,  12,   5,  58,   5],\n        [ 24,  17,   5,  25,   5,  26,   5,  60,   5,  61],\n        [ 62,   5,  63,   5,  64,   5,  65,   5,  66,   5],\n        [ 68,   5,  69,   5,  70,   5,  71,   5,  72,   5],\n        [ 74,   5,  72,   5,  75,   5,  76,   5,  77,   5],\n        [ 20,   5,  78,   5,  70,   5,  79,   5,   6,   5],\n        [ 81,   5,  82,   5,  83,   5,  84,   5,  85,   5],\n        [ 87,  15,   5,  88,  15,   5,   1,   5,  89,   5],\n        [ 90,  15,  37,  91,   5,  92,  15,  37,  93,   5],\n        [ 95,  15,   5,  73,   5,  83,   5,  96,  15,  37],\n        [ 50,   5,  98,   5,  99,   5, 100,   5,  94,   5],\n        [101,   5,  70,   5, 102,   5,  76,   5,  12,   5],\n        [104,   5, 105,   5, 106,   5, 107,   5,  75,   5],\n        [109,   5,  78,   5,  72,   5, 110,  37, 111,   5],\n        [113,   5, 114,  15,   5, 115,   5, 116,  15,   5],\n        [118,  37,  35,   5, 119,   5, 120,   5, 119,   5],\n        [ 62,   5,  83,   5,  96,   5,  72,   5, 110,  37],\n        [121,   5, 122,   5, 123,   5,  12,   5, 124,  15],\n        [ 62,   5,  83,   5,  96,  15,  37,  97,   5,  91],\n        [ 20,  15,   5,  88,  15,   5, 125,   5,  24,  17],\n        [  1,   5, 126, 127,   5, 128,   5, 129,   5, 130],\n        [131,   5,   1,   5, 110,   5,  12,   5, 132,   5],\n        [134,   5, 110,   5, 135,   5, 136,   5, 137,   5],\n        [ 45,   5, 120,   5, 139,   5, 140,  15,   5, 141],\n        [101,   5, 119,   5,  31,   5, 141,  15,   5, 142],\n        [143,  15,   5,  73,   5, 144,   5,   6,   5, 145],\n        [119,   5, 146,   5,  96,   5, 119,   5, 147,   5],\n        [149,   5, 128,   5, 150,  15,  37,  97,   5,  16],\n        [151,   5, 152,   5, 135,   5,  69,   5, 153,   5],\n        [155,   5, 128,   5, 156,   5,   1,   5, 157,   5],\n        [159,   5, 160,   5, 161,   5, 162,   5,  83,   5],\n        [109,   5, 164,   5, 165,   5,   8,   5,   6,   5],\n        [167,  37,  35,   5,   6,   5, 168,   5,   8,   5],\n        [ 57,   5, 170,   5, 171,   5,  54,   5, 172,  15],\n        [ 20,   5,  24,  17,   5,  26,   5,  27,   5,   1],\n        [134,   5,  27,   5, 173,   5,  85,   5, 174,   5],\n        [175,   5,  27,   5,  61,   5, 172,  15,   5,  35],\n        [121,   5, 176,   5,  27,   5, 177,  15,   5,  35],\n        [ 45,   5, 178,   5, 179,   5,  73,   5, 180,   5],\n        [181,   5,   9,   5, 182,   5,   1,   5,  32,   5],\n        [ 33,   5,  42,   5, 111,   5,  93,   5,  70,   5],\n        [175,   5,  60,   5,  61,   5, 183,   5,   6,   5],\n        [119,   5, 185,   5, 179,   5, 186,  17, 187, 119],\n        [188,   5, 179, 127,  37, 189,   5, 174,   5, 156],\n        [  4,   5,  12,   5, 190,   5, 191,  15,   5, 128],\n        [192,  15,   5, 193,   5, 194,   5, 195,   5, 108],\n        [196,   5, 194,   5, 197,   5, 198,   5,  12,   5],\n        [ 20,   5, 141,   5, 199,   5, 200,  15,   5,  28],\n        [121,   5, 201,   5, 202,   5, 203,   5,  93,   5],\n        [134,   5, 110,   5, 205,  15,   5,  32,   5, 206],\n        [119,   5,  69,   5,  61,   5,  83,   5, 207,   5],\n        [175,   5,  82,   5, 208,   5, 157,   5, 203,   5]])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
