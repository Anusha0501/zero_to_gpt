{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Working with text\n",
    "\n",
    "In this lesson, we'll learn how to work with text input to neural networks.  This is necessary to build a language model like GPT.\n",
    "\n",
    "Neural networks can't understand text directly, so we need to convert the text into a numerical representation.  How we choose to represent the text can give the model a lot of useful information, and result in better predictions.\n",
    "\n",
    "The steps in converting text to a numerical representation are:\n",
    "\n",
    "1. Tokenize the text to convert it into discrete tokens (kind of like words)\n",
    "2. Assign a number to each token\n",
    "3. Convert each token id into a vector representation.  This is called embedding.\n",
    "\n",
    "We can then feed the vectors into a neural network layer.  Here's a diagram:\n",
    "\n",
    "![](images/text/text_to_vec.svg)\n",
    "\n",
    "You might be wondering why we don't directly feed the token ids into the neural network.\n",
    "\n",
    "Embedding enables the network to learn similarities between tokens.  For example, the token id for a `.` might be `2`, and the id for a ` ` might be `7`.  This doesn't help the network understand the relationship between the two tokens.  However, if the vector for a `.` is `[0,1,.1,2]`, and the vector for a ` ` is `[0,1,.1,1]`, the distance between the vectors could indicate that the tokens are similar in their function.  Like weights, the embeddings are learned by the network, and will change during training.  Tokens that are conceptually similar will have vectors that are closer together than tokens that aren't."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data\n",
    "\n",
    "We'll be working with a dataset from [Opus books](https://huggingface.co/datasets/opus_books/viewer/en-es/train).  This dataset contains English sentences from books, and their Spanish translations.  We'll use the translation in the next lesson, but in this one, we'll only use the English sentence.\n",
    "\n",
    "There are about 24k sentence pairs in the dataset.  Here's an example:\n",
    "\n",
    "![](images/text/sentences.svg)\n",
    "\n",
    "These sentences in very Old(e) English, but that won't stop our AI from parsing them.  We'll first load in the data using `pandas` and explore it:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  en  \\\n0  In the society of his nephew and niece, and th...   \n1  By a former marriage, Mr. Henry Dashwood had o...   \n2  By his own marriage, likewise, which happened ...   \n3  But the fortune, which had been so tardy in co...   \n4  But Mrs. John Dashwood was a strong caricature...   \n\n                                                  es  \n0  En compañía de su sobrino y sobrina, y de los ...  \n1  De un matrimonio anterior, el señor Henry Dash...  \n2  Además, su propio matrimonio, ocurrido poco de...  \n3  Pero la fortuna, que había tardado tanto en ll...  \n4  Pero la señora de John Dashwood era una áspera...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>es</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the society of his nephew and niece, and th...</td>\n      <td>En compañía de su sobrino y sobrina, y de los ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>By a former marriage, Mr. Henry Dashwood had o...</td>\n      <td>De un matrimonio anterior, el señor Henry Dash...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>By his own marriage, likewise, which happened ...</td>\n      <td>Además, su propio matrimonio, ocurrido poco de...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>But the fortune, which had been so tardy in co...</td>\n      <td>Pero la fortuna, que había tardado tanto en ll...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>But Mrs. John Dashwood was a strong caricature...</td>\n      <td>Pero la señora de John Dashwood era una áspera...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "opus = pd.read_csv(\"../data/opus_books.csv\")\n",
    "opus.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create our vocabulary\n",
    "\n",
    "Now, we need to clean the data and define our token vocabulary.  Our vocabulary is how we map each token to a unique token id.  We'll be creating our own very simple tokenizer and vocabulary.  In practice, you'll use more powerful tokenizers like byte-pair encoding that look at sequences of characters to find the optimal tokenization scheme.\n",
    "\n",
    "Optimal means accuracy and speed.  For example, we could look at individual characters (`a`, `b`, etc) instead of tokens.  This would result in a much smaller vocabulary (and run faster), but it would be much less accurate, since the model would get less information about entire words and concepts.\n",
    "\n",
    "We'll first setup some special tokens, that we'll be using ourselves:\n",
    "\n",
    "- `PAD` - this token is used to pad sequences to a given length.  When we're working with text data, sentences won't all be the same length.  However, a neural network needs all rows in a batch to have the same number of columns.  Padding enables us to make all sentences the same length.  We use a special token for this, and tell the network to ignore it.\n",
    "- `UNK` - some tokens don't occur often enough to add them to our vocabulary.  Imagine words like `Octothorpe`, or issues with data quality like `hello123bye`.  These long-tail words will add a lot to our vocabulary (and make our model slower), but don't add much value to the model.  More powerful tokenizers will split these up into individual characters, but in our simple tokenizer, we need `UNK`.\n",
    "- `BOS` - this special token is used to mark the beginning of a sentence, or a sequence.\n",
    "- `EOS` - used to mark the end of a sequence.\n",
    "\n",
    "Some tokenizers, like the GPT-2 tokenizer, use `PAD` instead of `BOS` and `EOS`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "special_tokens = {\n",
    "    \"PAD\": 0,\n",
    "    \"UNK\": 1,\n",
    "    \"BOS\": 2,\n",
    "    \"EOS\": 3\n",
    "}\n",
    "vocab = special_tokens.copy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll define our functions to clean and tokenize input text.  We're going to do some naive cleaning, and just strip anything that isn't in a small set of characters (letters, numbers, spaces, some punctuation).  We're doing this because our simple tokenizer needs a very small character set (a large character set will result in a larger vocabulary).  As you'll see later, the size of the vocabulary impacts the size of the embedding matrix, and thus the performance of the network.\n",
    "\n",
    "Our tokenization will just split on whitespace and punctuation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# This is the maximum numbers of tokens we'll keep from each sentence.  You can increase this, but training will take longer.\n",
    "token_limit = 11\n",
    "\n",
    "def clean(text):\n",
    "    # Use re to replace punctuation that is not a comma, question mark, or period with spaces\n",
    "    text = re.sub(r'[^\\w\\s,?.!]',' ', text)\n",
    "    # Strip leading/trailing space\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Split on consecutive whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]+|[\\s]+', text)\n",
    "    return tokens[:token_limit]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now create a vocabulary using our functions.  We'll first create a dictionary containing every token in our sentences, and the number of times it appears across the dataset.  Then, we'll create a vocab dictionary, only selecting the tokens that appear more than once.  Tokens that only appear once will be marked as unknown."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "opus_tokens = defaultdict(int)\n",
    "\n",
    "# Loop through the sentences, clean, tokenize, and assign token counts\n",
    "for index, row in opus.iterrows():\n",
    "    cleaned = clean(row[\"en\"])\n",
    "    tokens = tokenize(cleaned)\n",
    "    for token in tokens:\n",
    "        opus_tokens[token] += 1\n",
    "\n",
    "# Set to the current size of the vocabulary (special tokens)\n",
    "counter = len(vocab)\n",
    "# Assign a unique id to each token if it appears more than once\n",
    "for index, token in enumerate(opus_tokens):\n",
    "    # Filter out uncommon tokens\n",
    "    # Add unknown token for rare words\n",
    "    if opus_tokens[token] > 1:\n",
    "        vocab[token] = counter\n",
    "        counter += 1\n",
    "    else:\n",
    "        vocab[token] = 1 # Assign unknown id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "11731"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have about 11k tokens in our vocabulary.  In practice, tokenizers will usually have between 10k and 100k tokens.  This is a good tradeoff between thoroughness (having a unique id for every word), and vocabulary size (splitting some rare words into multiple tokens).  The GPT-2 tokenizer uses 50257 tokens.\n",
    "\n",
    "We'll also build a reverse vocab lookup, which we can use to decode token ids to tokens:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "reverse_vocab = {v: k for k, v in vocab.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize sentences\n",
    "\n",
    "We can now use our vocabulary to tokenize our sentences.  We'll create an encode function, that can turn a sentence into a torch tensor of token ids.\n",
    "\n",
    "We'll also write a decode function.  This will use a reverse lookup to go from token id to token.  This will enable us to decode our predictions and see how good they were."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode(text):\n",
    "    # Yokenize input text\n",
    "    tokens = tokenize(clean(text))\n",
    "    # Convert to token ids\n",
    "    encoded = torch.tensor([vocab[token] for token in tokens])\n",
    "    return encoded\n",
    "\n",
    "def decode(encoded):\n",
    "    # The input is a torch tensor - convert it to a list\n",
    "    encoded = encoded.detach().cpu().tolist()\n",
    "    # Decode a list of integers into text\n",
    "    decoded = \"\".join([reverse_vocab[token] for token in encoded])\n",
    "    return decoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can use the encode function to convert our English sentences into token ids.  We'll only take sentences that have at least as many tokens than the token limit we set earlier.  This will allow us to use the first `10` tokens of each sentence to predict token `11`.  Alternatively, we could pad the shorter sentences to the limit, but it's easier for now to avoid padding."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "tokenized = []\n",
    "for index, row in opus.iterrows():\n",
    "    # Encode the English sentences\n",
    "    en_text = row[\"en\"]\n",
    "    en = encode(en_text)\n",
    "    if en.shape[0] < token_limit:\n",
    "        continue\n",
    "    tokenized.append(en)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4,  5,  6,  5,  7,  5,  8,  5,  9,  5, 10])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create torch dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.tokens = torch.vstack(data).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return how many examples are in the dataset\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a single training example\n",
    "        x = self.tokens[idx][:10]\n",
    "        y = self.tokens[idx][10]\n",
    "        return x, y\n",
    "\n",
    "# Initialize the dataset\n",
    "train_ds = TextData(tokenized)\n",
    "train = DataLoader(train_ds, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([4, 5, 6, 5, 7, 5, 8, 5, 9, 5]), tensor(10))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "[tensor([[  4,   5,   6,   5,   7,   5,   8,   5,   9,   5],\n         [ 11,   5,  12,   5,  13,   5,  14,  15,   5,  16],\n         [ 11,   5,   9,   5,  18,   5,  14,  15,   5,  19],\n         [ 20,   5,   6,   5,  21,  15,   5,  22,   5,  23],\n         [ 20,   5,  24,  17,   5,  25,   5,  26,   5,  27],\n         [ 28,   5,  29,   5,  30,   5,  31,  15,   5,  32],\n         [ 33,   5,  27,   5,  34,   5,  35,   5,  36,  37],\n         [ 33,   5,  27,   5,   1,  15,   5,  39,  15,   5],\n         [ 41,   5,  42,  15,   5,  43,   5,  44,  15,   5],\n         [ 45,   5,  46,   5,  47,   5,  48,   5,  49,   5],\n         [ 50,   5,  51,   5,   8,   5,  52,   5,  22,   5],\n         [ 41,  15,   5,  54,  15,   5,  27,   5,  55,   5],\n         [ 57,   5,   1,   5,  32,   5,  12,   5,  58,   5],\n         [ 24,  17,   5,  25,   5,  26,   5,  60,   5,  61],\n         [ 62,   5,  63,   5,  64,   5,  65,   5,  66,   5],\n         [ 68,   5,  69,   5,  70,   5,  71,   5,  72,   5],\n         [ 74,   5,  72,   5,  75,   5,  76,   5,  77,   5],\n         [ 20,   5,  78,   5,  70,   5,  79,   5,   6,   5],\n         [ 81,   5,  82,   5,  83,   5,  84,   5,  85,   5],\n         [ 87,  15,   5,  88,  15,   5,   1,   5,  89,   5],\n         [ 90,  15,  37,  91,   5,  92,  15,  37,  93,   5],\n         [ 95,  15,   5,  73,   5,  83,   5,  96,  15,  37],\n         [ 50,   5,  98,   5,  99,   5, 100,   5,  94,   5],\n         [101,   5,  70,   5, 102,   5,  76,   5,  12,   5],\n         [104,   5, 105,   5, 106,   5, 107,   5,  75,   5],\n         [109,   5,  78,   5,  72,   5, 110,  37, 111,   5],\n         [113,   5, 114,  15,   5, 115,   5, 116,  15,   5],\n         [118,  37,  35,   5, 119,   5, 120,   5, 119,   5],\n         [ 62,   5,  83,   5,  96,   5,  72,   5, 110,  37],\n         [121,   5, 122,   5, 123,   5,  12,   5, 124,  15],\n         [ 62,   5,  83,   5,  96,  15,  37,  97,   5,  91],\n         [ 20,  15,   5,  88,  15,   5, 125,   5,  24,  17],\n         [  1,   5, 126, 127,   5, 128,   5, 129,   5, 130],\n         [131,   5,   1,   5, 110,   5,  12,   5, 132,   5],\n         [134,   5, 110,   5, 135,   5, 136,   5, 137,   5],\n         [ 45,   5, 120,   5, 139,   5, 140,  15,   5, 141],\n         [101,   5, 119,   5,  31,   5, 141,  15,   5, 142],\n         [143,  15,   5,  73,   5, 144,   5,   6,   5, 145],\n         [119,   5, 146,   5,  96,   5, 119,   5, 147,   5],\n         [149,   5, 128,   5, 150,  15,  37,  97,   5,  16],\n         [151,   5, 152,   5, 135,   5,  69,   5, 153,   5],\n         [155,   5, 128,   5, 156,   5,   1,   5, 157,   5],\n         [159,   5, 160,   5, 161,   5, 162,   5,  83,   5],\n         [109,   5, 164,   5, 165,   5,   8,   5,   6,   5],\n         [167,  37,  35,   5,   6,   5, 168,   5,   8,   5],\n         [ 57,   5, 170,   5, 171,   5,  54,   5, 172,  15],\n         [ 20,   5,  24,  17,   5,  26,   5,  27,   5,   1],\n         [134,   5,  27,   5, 173,   5,  85,   5, 174,   5],\n         [175,   5,  27,   5,  61,   5, 172,  15,   5,  35],\n         [121,   5, 176,   5,  27,   5, 177,  15,   5,  35],\n         [ 45,   5, 178,   5, 179,   5,  73,   5, 180,   5],\n         [181,   5,   9,   5, 182,   5,   1,   5,  32,   5],\n         [ 33,   5,  42,   5, 111,   5,  93,   5,  70,   5],\n         [175,   5,  60,   5,  61,   5, 183,   5,   6,   5],\n         [119,   5, 185,   5, 179,   5, 186,  17, 187, 119],\n         [188,   5, 179, 127,  37, 189,   5, 174,   5, 156],\n         [  4,   5,  12,   5, 190,   5, 191,  15,   5, 128],\n         [192,  15,   5, 193,   5, 194,   5, 195,   5, 108],\n         [196,   5, 194,   5, 197,   5, 198,   5,  12,   5],\n         [ 20,   5, 141,   5, 199,   5, 200,  15,   5,  28],\n         [121,   5, 201,   5, 202,   5, 203,   5,  93,   5],\n         [134,   5, 110,   5, 205,  15,   5,  32,   5, 206],\n         [119,   5,  69,   5,  61,   5,  83,   5, 207,   5],\n         [175,   5,  82,   5, 208,   5, 157,   5, 203,   5]]),\n tensor([ 10,  17,  15,   5,   5,   5,  38,  40,   6,  32,  53,  56,  59,   5,\n          67,  73,  78,  80,  86,  83,  94,  97,   1, 103, 108, 112, 117,  99,\n          35,   5,  15,   5,  37, 133, 138,   5,   5,  15, 148,  17, 154, 158,\n         163, 166, 169,   5,   5,  93,   5,   5,  12,   1,  27, 184,   5,   5,\n           5,   5, 190,  37, 204,   5,  43, 128])]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train))\n",
    "batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        k = 1/math.sqrt(embed_dim)\n",
    "        self.weights =  torch.rand(vocab_size, embed_dim) * 2 * k - k\n",
    "        self.weights[0] = 0 # Zero out the padding embedding\n",
    "        self.weights = nn.Parameter(self.weights)\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Return a matrix of embeddings\n",
    "        # We could convert token_ids to a one_hot vector and multiply by the weights, but it is the same as selecting a single row of the matrix\n",
    "        return self.weights[token_ids]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0377,  0.0337, -0.0130,  ...,  0.0319,  0.0504, -0.0156],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [ 0.0294,  0.0567,  0.0090,  ...,  0.0385,  0.0029, -0.0361],\n",
      "         ...,\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [ 0.0488,  0.0272, -0.0281,  ...,  0.0275,  0.0597, -0.0071],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193]],\n",
      "\n",
      "        [[ 0.0363,  0.0312,  0.0317,  ...,  0.0185,  0.0068,  0.0401],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [-0.0099,  0.0077,  0.0200,  ..., -0.0395, -0.0311,  0.0408],\n",
      "         ...,\n",
      "         [-0.0112,  0.0034, -0.0372,  ...,  0.0186,  0.0356,  0.0023],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [ 0.0040,  0.0188,  0.0364,  ...,  0.0054, -0.0259, -0.0394]],\n",
      "\n",
      "        [[ 0.0363,  0.0312,  0.0317,  ...,  0.0185,  0.0068,  0.0401],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [ 0.0488,  0.0272, -0.0281,  ...,  0.0275,  0.0597, -0.0071],\n",
      "         ...,\n",
      "         [-0.0112,  0.0034, -0.0372,  ...,  0.0186,  0.0356,  0.0023],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [ 0.0584, -0.0365,  0.0494,  ...,  0.0366, -0.0058,  0.0620]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0350, -0.0415,  0.0512,  ...,  0.0547, -0.0298, -0.0063],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [-0.0064, -0.0152,  0.0622,  ..., -0.0597, -0.0040, -0.0334],\n",
      "         ...,\n",
      "         [-0.0499,  0.0422, -0.0033,  ...,  0.0344,  0.0472, -0.0161],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [-0.0234,  0.0294, -0.0467,  ...,  0.0430,  0.0023,  0.0408]],\n",
      "\n",
      "        [[-0.0448, -0.0378,  0.0538,  ..., -0.0248, -0.0094,  0.0341],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [-0.0606,  0.0520, -0.0423,  ..., -0.0402, -0.0386,  0.0332],\n",
      "         ...,\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [-0.0362,  0.0089, -0.0253,  ..., -0.0406, -0.0623,  0.0266],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193]],\n",
      "\n",
      "        [[ 0.0135,  0.0032, -0.0079,  ...,  0.0153,  0.0590,  0.0017],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [ 0.0466, -0.0502, -0.0601,  ..., -0.0279, -0.0384,  0.0424],\n",
      "         ...,\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193],\n",
      "         [-0.0419, -0.0552,  0.0594,  ..., -0.0219, -0.0267,  0.0230],\n",
      "         [-0.0284, -0.0035,  0.0453,  ..., -0.0200, -0.0297,  0.0193]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    input_embed = Embedding(len(vocab), 256)\n",
    "    print(input_embed(batch[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict next token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class TokenPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, input_token_count, hidden_units):\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(0)\n",
    "        self.embedding = Embedding(vocab_size, hidden_units)\n",
    "        self.dense1 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(hidden_units, hidden_units)\n",
    "        self.output = nn.Linear(hidden_units * input_token_count, hidden_units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed from (token_count, vocab_size) to (token_count, hidden_size)\n",
    "        embedded = self.embedding(x)\n",
    "        # Run the network\n",
    "        x = self.dense2(self.relu(self.dense1(embedded)))\n",
    "        # Flatten the vectors into one large vector per sentence for the final layer\n",
    "        flat = torch.flatten(x, start_dim=1)\n",
    "        # Run the final layer to get an output\n",
    "        network_out = self.output(flat)\n",
    "        # Unembed, convert to (batch_size, vocab_size).  Argmax against last dim gives predicted token\n",
    "        out_vector = network_out @ self.embedding.weights.T\n",
    "        return out_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "# Initialize W&B\n",
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def train_loop(net, optimizer, epochs):\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init(project=\"text\",\n",
    "               name=\"dense\")\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    train_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for batch, (x, y) in enumerate(train):\n",
    "            # zero_grad will set all the gradients to zero\n",
    "            # We need this because gradients will accumulate in the backward pass\n",
    "            optimizer.zero_grad()\n",
    "            # Make a prediction using the network\n",
    "            pred = net(x)\n",
    "            # Calculate the loss\n",
    "            loss = loss_fn(pred, y)\n",
    "            # Call loss.backward to run backpropagation\n",
    "            loss.backward()\n",
    "            # Step the optimizer to update the parameters\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            if batch % 10 == 0:\n",
    "                # Log training metrics\n",
    "                wandb.log({\n",
    "                    \"train_loss\": mean(train_losses)\n",
    "                })\n",
    "\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define our hyperparameters\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "# Initialize our network\n",
    "net = TokenPredictor(len(vocab), 10, 256)\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "losses = train_loop(net, optimizer, epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    batch = next(iter(train))\n",
    "    pred = net(batch[0])\n",
    "    token_id = pred.argmax(-1)\n",
    "\n",
    "    for i in range(len(batch[0])):\n",
    "        text = decode(batch[0][i])\n",
    "        actual = decode(batch[1][i:(i+1)])\n",
    "        pred = decode(token_id[i:(i+1)])\n",
    "        print(f\"{text}<ACTUAL>{actual}<><PRED>{pred}<>\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
