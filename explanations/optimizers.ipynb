{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizers\n",
    "\n",
    "In the last few lessons, we learned how to build different types of neural network architectures.  In the [dense neural network](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/dense.ipynb) lesson, we learned how to adjust our neural network parameters with gradient descent.  We then used this technique in the subsequent lessons.\n",
    "\n",
    "Gradient descent is a type of optimizer.  Optimizers adjust neural network parameters to try to get loss to a minimum value.  In this lesson, we'll learn more about optimizers.  We'll first go into more depth on gradient descent, and discuss batch size, learning rate schedules, weight decay, and momentum.  We'll then discuss the AdamW optimizer, which is a popular optimizer that combines momentum and weight decay.  AdamW is the most commonly used optimizer in large language models like GPT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The type of optimizer we've used so far is called stochastic gradient descent, or SGD.  In SGD, we take a minibatch of training examples, then compute the average gradient across the minibatch.  We then adjust the parameters using this average gradient.\n",
    "\n",
    "Let's define a two-layer dense neural network, then explore the effect of batch size on our loss over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "from csv_data import WeatherDatasetWrapper\n",
    "\n",
    "# Load the data with 3 target values instead of the binary value from earlier\n",
    "wrapper = WeatherDatasetWrapper()\n",
    "[train_x, train_y], [valid_x, valid_y], [test_x, test_y] = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Dense():\n",
    "    def __init__(self, input_size, output_size, activation=True, seed=0):\n",
    "        self.add_activation = activation\n",
    "        self.hidden = None\n",
    "        self.prev_hidden = None\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        k = math.sqrt(1 / input_size)\n",
    "        self.weights = np.random.rand(input_size, output_size) * (2 * k) - k\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.prev_hidden = x.copy()\n",
    "        x = np.matmul(x, self.weights)\n",
    "        if self.add_activation:\n",
    "            x = np.maximum(x, 0)\n",
    "        self.hidden = x.copy()\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad, lr):\n",
    "        if self.add_activation:\n",
    "            grad = np.multiply(grad, np.heaviside(self.hidden, 0))\n",
    "\n",
    "        w_grad = self.prev_hidden.T @ grad\n",
    "        grad = grad @ self.weights.T\n",
    "        return w_grad, grad\n",
    "\n",
    "    def update(self, w_grad):\n",
    "        self.weights -= w_grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    for layer in layers:\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def backward(x, layers, lr):\n",
    "    w_grads = []\n",
    "    for layer in reversed(layers):\n",
    "        w_grad, _ = layer.backward(x, lr)\n",
    "        w_grads.append(w_grad)\n",
    "    return w_grads\n",
    "\n",
    "layers = [Dense(3, 10), Dense(10, 1, activation=False)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
