{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizers\n",
    "\n",
    "In the last few lessons, we learned how to build several neural network architectures.  We used gradient descent, a technique we originally learned in the [dense neural network](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/dense.ipynb) lesson, to adjust model parameters.\n",
    "\n",
    "Gradient descent is a type of optimizer.  Optimizers adjust neural network parameters to try to get loss to (hopefully) a global minimum value.  In this lesson, we'll learn more about optimizers.  We'll first go into more depth on gradient descent, and discuss batch size, learning rate schedules, weight decay, and momentum.  We'll then discuss the AdamW optimizer, which is a popular optimizer that combines momentum and weight decay.  AdamW is the most commonly used optimizer in large language models like GPT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The type of optimizer we've used so far is called stochastic gradient descent, or SGD.  In SGD, we take a minibatch of training examples, then compute the average gradient across the minibatch.  We then adjust the parameters using this average gradient.\n",
    "\n",
    "Let's define a two-layer dense neural network, then explore the effect of batch size on our loss over time.  We'll first load in a dataset of weather observations that we've used in previous lessons.\n",
    "\n",
    "Each row in this dataset is a weather observation from a given day.  We'll use one day's max temperature, min temperature, and rainfall to predict the next day's max temperature.  We have 3 predictors, and one value to predict.\n",
    "\n",
    "The predictor columns have all been scaled using the scikit-learn `StandardScaler`.  This gives each column a mean of 0 and a standard deviation of 1.  This makes it easier to activate our nonlinearities and have the network learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "from csv_data import WeatherDatasetWrapper\n",
    "\n",
    "# Load the data with 3 target values instead of the binary value from earlier\n",
    "wrapper = WeatherDatasetWrapper()\n",
    "[train_x, train_y], [valid_x, valid_y], [test_x, test_y] = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.72725587, -2.27150212, -0.25366126],\n       [-1.68779357, -1.6825982 , -0.25366126]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[52.],\n       [52.]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll define a single layer of our neural network.  We'll make a few modifications from earlier lessons:\n",
    "\n",
    "- Instead of directly updating the parameters, we'll return the weight and bias gradients\n",
    "- We'll add an update method that updates the weights later\n",
    "\n",
    "This will enable us to swap different optimizers in and out of our network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Dense():\n",
    "    def __init__(self, input_size, output_size, activation=True, seed=0):\n",
    "        self.add_activation = activation\n",
    "        self.hidden = None\n",
    "        self.prev_hidden = None\n",
    "\n",
    "        # Initialize the weights.  They'll be in the range -sqrt(k) to sqrt(k), where k = 1 / input_size\n",
    "        np.random.seed(seed)\n",
    "        k = math.sqrt(1 / input_size)\n",
    "        self.weights = np.random.rand(input_size, output_size) * (2 * k) - k\n",
    "\n",
    "        # Our bias will be initialized to 1\n",
    "        self.bias = np.ones((1,output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Copy the layer input for backprop\n",
    "        self.prev_hidden = x.copy()\n",
    "        # Multiply the input by the weights, then add the bias\n",
    "        x = np.matmul(x, self.weights) + self.bias\n",
    "        # Apply the activation function\n",
    "        if self.add_activation:\n",
    "            x = np.maximum(x, 0)\n",
    "        # Copy the layer output for backprop\n",
    "        self.hidden = x.copy()\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # \"Undo\" the activation function if it was added\n",
    "        if self.add_activation:\n",
    "            grad = np.multiply(grad, np.heaviside(self.hidden, 0))\n",
    "\n",
    "        # Calculate the parameter gradients\n",
    "        w_grad = self.prev_hidden.T @ grad # This is not averaged across the batch, due to the way matrix multiplication sums\n",
    "        b_grad = np.mean(grad, axis=0) # This is averaged across the batch\n",
    "        param_grads = [w_grad, b_grad]\n",
    "\n",
    "        # Calculate the next layer gradient\n",
    "        grad = grad @ self.weights.T\n",
    "        return param_grads, grad\n",
    "\n",
    "    def update(self, w_grad, b_grad):\n",
    "        # Update the weights given an update matrix\n",
    "        self.weights += w_grad\n",
    "        self.bias += b_grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then use the `Dense` class to create a 3-layer neural network.  The first layer will take in our predictors and generate `10` hidden features, the second layer combine those features into `10` new features, and the final layer will make a prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(3, 10),\n",
    "    Dense(10, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll define functions that run forward and backward passes across all the layers together.  `forward` will do a full forward pass across all 3 layers, and `backward` will do a full backward pass across all 3 layers.\n",
    "\n",
    "The backward pass will return the gradients instead of updating the parameters.  We'll use these gradients to update the parameters in our optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    # Loop through each layer\n",
    "    for layer in layers:\n",
    "        # Run the forward pass\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def backward(grad, layers):\n",
    "    # Save the gradients for each layer\n",
    "    layer_grads = []\n",
    "    # Loop through each layer in reverse order (starting from the output layer)\n",
    "    for layer in reversed(layers):\n",
    "        # Get the parameter gradients and the next layer gradient\n",
    "        param_grads, grad = layer.backward(grad)\n",
    "        layer_grads.append(param_grads)\n",
    "    return layer_grads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll then define our SGD optimizer, which will take in a set of gradients, and use it to update the network parameters.\n",
    "\n",
    "The process for SGD is:\n",
    "\n",
    "- Normalize the gradient by batch size, so that the gradient is the average gradient across the batch (in our case, our bias gradient is already normalized, but the weight gradient is not)\n",
    "- Multiply the gradient by learning rate\n",
    "- Multiply the gradient by `-1` so it is subtracted from the parameters in the `update` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def sgd(layer_grads, layers, lr, batch_size):\n",
    "    for layer_grad, layer in zip(layer_grads, reversed(layers)):\n",
    "        w_grad, b_grad = layer_grad\n",
    "\n",
    "        # Normalize the weight gradient by batch size\n",
    "        w_grad = w_grad / batch_size\n",
    "\n",
    "        # Calculate the update sizes\n",
    "        w_update = -lr * w_grad\n",
    "        b_update = -lr * b_grad\n",
    "\n",
    "        layer.update(w_update, b_update)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitoring\n",
    "\n",
    "We can now write a function to train the network using the SGD optimizer and our data.  Since we'll be testing different batch sizes and optimizers, we want a way to monitor our network and compare different runs to wach other.  So far, we've been using print statements to monitor per-epoch accuracy in our network, which is hard to compare to other runs.\n",
    "\n",
    "We'll use a tool called [Weights & Biases](https://wandb.ai) to monitor our network.  We can use it to track the loss and accuracy of our network, and compare different runs to each other.  W&B is a free tool, and you can sign up for an account [here](https://wandb.ai).  If you don't want to use W&B, you can also use TensorBoard, but it's harder to setup and use.\n",
    "\n",
    "We'll start by importing `wandb` and logging in.  We'll also set the `WANDB_SILENT` environment variable to `True`, so that W&B doesn't print out a lot of system messages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 2\n",
    "lr = 5e-6\n",
    "\n",
    "run = wandb.init(project=\"optimizers\",\n",
    "                 config={\"batch_size\": batch_size,\n",
    "                         \"lr\": lr,\n",
    "                         \"optimizer\": \"sgd\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<wandb.jupyter.IFrame at 0x1553799f0>",
      "text/html": "<iframe src=\"https://wandb.ai/vikp/optimizers/runs/n2sdz4pn?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%wandb\n",
    "\n",
    "layers = [\n",
    "    Dense(3, 10),\n",
    "    Dense(10,10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "\n",
    "    for i in range(0, len(train_x), batch_size):\n",
    "        x_batch = train_x[i:(i+batch_size)]\n",
    "        y_batch = train_y[i:(i+batch_size)]\n",
    "        pred = forward(x_batch, layers)\n",
    "\n",
    "        loss = pred - y_batch\n",
    "        running_loss += np.mean(loss ** 2)\n",
    "\n",
    "        layer_grads = backward(loss, layers)\n",
    "        sgd(layer_grads, layers, lr, batch_size)\n",
    "\n",
    "        wandb.log({\"running_loss\": running_loss / (i+1) * batch_size})\n",
    "\n",
    "    valid_preds = forward(valid_x, layers)\n",
    "    valid_loss = np.mean((valid_preds - valid_y) ** 2)\n",
    "    train_loss = running_loss / len(train_x) * batch_size\n",
    "    wandb.log({\"valid_loss\": valid_loss, \"epoch\": epoch, \"train_loss\": train_loss})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-6.50761231],\n       [ 8.03445872],\n       [ 2.60728585],\n       ...,\n       [-1.02646218],\n       [-5.6792079 ],\n       [ 8.22534764]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds - valid_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
