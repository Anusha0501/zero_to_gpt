{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizers\n",
    "\n",
    "In the last few lessons, we learned how to build several neural network architectures.  We used gradient descent, a technique we originally learned in the [dense neural network](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/dense.ipynb) lesson, to adjust model parameters.\n",
    "\n",
    "Gradient descent is a type of optimizer.  Optimizers adjust neural network parameters to try to get loss to (hopefully) a global minimum value.  In this lesson, we'll learn more about optimizers.  We'll first go into more depth on gradient descent, and discuss batch size, learning rate schedules, weight decay, and momentum.  We'll then discuss the AdamW optimizer, which is a popular optimizer that combines momentum and weight decay.  AdamW is the most commonly used optimizer in large language models like GPT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The type of optimizer we've used so far is called stochastic gradient descent, or SGD.  In SGD, we take a minibatch of training examples, then compute the average gradient across the minibatch.  We then adjust the parameters using this average gradient.\n",
    "\n",
    "Let's define a two-layer dense neural network, then explore the effect of batch size on our loss over time.  We'll first load in a dataset of weather observations that we've used in previous lessons.\n",
    "\n",
    "Each row in this dataset is a weather observation from a given day.  We'll use one day's max temperature, min temperature, and rainfall to predict the next day's max temperature.  We have 3 predictors, and one value to predict.\n",
    "\n",
    "The predictor columns have all been scaled using the scikit-learn `StandardScaler`.  This gives each column a mean of 0 and a standard deviation of 1.  This makes it easier to activate our nonlinearities and have the network learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "from csv_data import WeatherDatasetWrapper\n",
    "\n",
    "# Load the data with 3 target values instead of the binary value from earlier\n",
    "wrapper = WeatherDatasetWrapper()\n",
    "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.72725587, -2.27150212, -0.25366126],\n       [-1.68779357, -1.6825982 , -0.25366126]])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the train predictors\n",
    "train_data[0][:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[52.],\n       [52.]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the train target\n",
    "train_data[1][:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll define a single layer of our neural network.  We'll make a few modifications from earlier lessons:\n",
    "\n",
    "- Instead of directly updating the parameters, we'll return the weight and bias gradients\n",
    "- We'll add an update method that updates the weights later\n",
    "\n",
    "This will enable us to swap different optimizers in and out of our network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Dense():\n",
    "    def __init__(self, input_size, output_size, activation=True, seed=0):\n",
    "        self.add_activation = activation\n",
    "        self.hidden = None\n",
    "        self.prev_hidden = None\n",
    "\n",
    "        # Initialize the weights.  They'll be in the range -sqrt(k) to sqrt(k), where k = 1 / input_size\n",
    "        np.random.seed(seed)\n",
    "        k = math.sqrt(1 / input_size)\n",
    "        self.weights = np.random.rand(input_size, output_size) * (2 * k) - k\n",
    "\n",
    "        # Our bias will be initialized to 1\n",
    "        self.bias = np.ones((1,output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Copy the layer input for backprop\n",
    "        self.prev_hidden = x.copy()\n",
    "        # Multiply the input by the weights, then add the bias\n",
    "        x = np.matmul(x, self.weights) + self.bias\n",
    "        # Apply the activation function\n",
    "        if self.add_activation:\n",
    "            x = np.maximum(x, 0)\n",
    "        # Copy the layer output for backprop\n",
    "        self.hidden = x.copy()\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # \"Undo\" the activation function if it was added\n",
    "        if self.add_activation:\n",
    "            grad = np.multiply(grad, np.heaviside(self.hidden, 0))\n",
    "\n",
    "        # Calculate the parameter gradients\n",
    "        w_grad = self.prev_hidden.T @ grad # This is not averaged across the batch, due to the way matrix multiplication sums\n",
    "        b_grad = np.mean(grad, axis=0) # This is averaged across the batch\n",
    "        param_grads = [w_grad, b_grad]\n",
    "\n",
    "        # Calculate the next layer gradient\n",
    "        grad = grad @ self.weights.T\n",
    "        return param_grads, grad\n",
    "\n",
    "    def update(self, w_grad, b_grad):\n",
    "        # Update the weights given an update matrix\n",
    "        self.weights += w_grad\n",
    "        self.bias += b_grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then use the `Dense` class to create a 3-layer neural network.  The first layer will take in our predictors and generate `10` hidden features, the second layer combine those features into `10` new features, and the final layer will make a prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(3, 10),\n",
    "    Dense(10, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll define functions that run forward and backward passes across all the layers together.  `forward` will do a full forward pass across all 3 layers, and `backward` will do a full backward pass across all 3 layers.\n",
    "\n",
    "The backward pass will return the gradients instead of updating the parameters.  We'll use these gradients to update the parameters in our optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    # Loop through each layer\n",
    "    for layer in layers:\n",
    "        # Run the forward pass\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def backward(grad, layers):\n",
    "    # Save the gradients for each layer\n",
    "    layer_grads = []\n",
    "    # Loop through each layer in reverse order (starting from the output layer)\n",
    "    for layer in reversed(layers):\n",
    "        # Get the parameter gradients and the next layer gradient\n",
    "        param_grads, grad = layer.backward(grad)\n",
    "        layer_grads.append(param_grads)\n",
    "    return layer_grads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll then define our SGD optimizer, which will take in a set of gradients, and use it to update the network parameters.\n",
    "\n",
    "The process for SGD is:\n",
    "\n",
    "- Normalize the gradient by batch size, so that the gradient is the average gradient across the batch (in our case, our bias gradient is already normalized, but the weight gradient is not)\n",
    "- Multiply the gradient by learning rate\n",
    "- Multiply the gradient by `-1` so it is subtracted from the parameters in the `update` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def sgd(layer_grads, layers, lr, batch_size):\n",
    "    for layer_grad, layer in zip(layer_grads, reversed(layers)):\n",
    "        w_grad, b_grad = layer_grad\n",
    "\n",
    "        # Normalize the weight gradient by batch size\n",
    "        w_grad = w_grad / batch_size\n",
    "\n",
    "        # Calculate the update sizes\n",
    "        w_update = -lr * w_grad\n",
    "        b_update = -lr * b_grad\n",
    "\n",
    "        layer.update(w_update, b_update)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitoring\n",
    "\n",
    "We can now write a function to train the network using the SGD optimizer and our data.  Since we'll be testing different batch sizes and optimizers, we want a way to monitor our network and compare different runs to wach other.  So far, we've been using print statements to monitor per-epoch accuracy in our network, which is hard to compare to other runs.\n",
    "\n",
    "We'll use a tool called [Weights & Biases](https://wandb.ai) to monitor our network.  We can use it to track the loss and accuracy of our network, and compare different runs to each other.  W&B is a free tool, and you can sign up for an account [here](https://wandb.ai).  If you don't want to use W&B, you can also use TensorBoard, but it's harder to setup and use.\n",
    "\n",
    "We'll start by importing `wandb` and logging in.  We'll also set the `WANDB_SILENT` environment variable to `True`, so that W&B doesn't print out a lot of system messages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With W&B, we track each training run separately.  W&B will track the loss in each epoch, or anything else we want to keep track of.  It will also render graphs for us, so we can compare different runs against each other.\n",
    "\n",
    "The key W&B functions are:\n",
    "\n",
    "- `wandb.init` - This will initialize a new run.  We can use the `config` parameter to pass in run-specific information we want to view.\n",
    "- `wandb.log` - This will log a dictionary of metrics to the current run.\n",
    "- `wandb.define_metric` - This will define a metric that we want to track.  You normally don't need to define wandb metrics upfront, but we'll use a custom `step_metric` to ensure that results from runs with different batch sizes line up.\n",
    "\n",
    "We'll use W&B to log a running training loss, the final training set loss each epoch, and the validation set loss.  We'll also track how long each epoch takes to run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Training Loop\n",
    "\n",
    "We can now write a function that will train our network using SGD, and log the loss to W&B.  This function will be very similar to training loops that we've written in previous lessons.\n",
    "\n",
    "We didn't do this previously, but we'll shuffle our training data each epoch.  This will ensure that our batches are different each epoch.  Random shuffling is important when our batch size is greater than `1`.  As we saw earlier, the gradient is averaged across all the examples in a batch.  Random shuffling will put different training examples together each time, allowing the model to see more combinations of gradients.  This reduces overfitting, and improves the generalization of our network.  It can also make SGD converge faster (get to the global minimum error)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def training_run(epochs, batch_size, lr, optimizer, train_data, valid_data):\n",
    "    # Initialize a new W&B run, with the right parameters\n",
    "    run = wandb.init(project=\"optimizers\",\n",
    "               config={\"batch_size\": batch_size,\n",
    "                       \"lr\": lr,\n",
    "                       \"epochs\": epochs,\n",
    "                       \"optimizer\": optimizer.__name__})\n",
    "\n",
    "    # Setup the metrics we want to track with wandb\n",
    "    wandb.define_metric(\"batch_step\") # This will ensure that results from runs with different batch sizes line up\n",
    "    wandb.define_metric(\"epoch\") # This will ensure that results from runs with different batch sizes line up\n",
    "    wandb.define_metric(\"valid_loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"train_loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"runtime\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"running_loss\", step_metric=\"batch_step\")\n",
    "\n",
    "    # Setup the layers for the training run\n",
    "    layers = [\n",
    "        Dense(3, 10),\n",
    "        Dense(10,10),\n",
    "        Dense(10, 1, activation=False)\n",
    "    ]\n",
    "\n",
    "    # Set the numpy random seed so the random shuffles proceed in the same order every run\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Split the training and valid data into x and y\n",
    "    train_x, train_y = train_data\n",
    "    valid_x, valid_y = valid_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        start = time.time() # The start time of our run\n",
    "\n",
    "        np.random.shuffle(train_x) # Shuffle the training data\n",
    "        for i in range(0, len(train_x), batch_size):\n",
    "            # Get the x and y batches\n",
    "            x_batch = train_x[i:(i+batch_size)]\n",
    "            y_batch = train_y[i:(i+batch_size)]\n",
    "            # Make a prediction\n",
    "            pred = forward(x_batch, layers)\n",
    "\n",
    "            # Run the backward pass\n",
    "            loss = pred - y_batch\n",
    "            layer_grads = backward(loss, layers)\n",
    "\n",
    "            # Run the optimizer\n",
    "            optimizer(layer_grads, layers, lr, batch_size)\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += np.mean(loss ** 2)\n",
    "\n",
    "            batch_idx = i + batch_size # Get the last index of the current batch\n",
    "            batch_step = batch_idx + epoch * len(train_x)\n",
    "            # Log running loss.  We multiply by batch size to offset the mean from earlier.\n",
    "            wandb.log({\"running_loss\": running_loss / batch_idx * batch_size, \"batch_step\": batch_step})\n",
    "\n",
    "        # Calculate and log validation loss\n",
    "        valid_preds = forward(valid_x, layers)\n",
    "        valid_loss = np.mean((valid_preds - valid_y) ** 2)\n",
    "        train_loss = running_loss / len(train_x) * batch_size\n",
    "        wandb.log({\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"runtime\": time.time() - start\n",
    "        })\n",
    "\n",
    "    # Mark the run as complete\n",
    "    run.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then initialize the parameters we want to adjust (like batch size), and start the run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup our parameters\n",
    "epochs = 50\n",
    "batch_size = 2\n",
    "lr = 5e-6\n",
    "\n",
    "# Run our training loop\n",
    "training_run(epochs, batch_size, lr, sgd, train_data, valid_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should now be able to see your run in W&B.  You will need to click on the `optimizers` project in your [dashboard](https://wandb.ai/home).  Here's a screenshot of the run summary page:\n",
    "\n",
    "![W&B Dashboard](images/optimizers/wandb_dash.png)\n",
    "\n",
    "We can compare this run to a run with a higher batch size:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup our parameters\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "lr = 5e-6\n",
    "\n",
    "# Run our training loop\n",
    "training_run(epochs, batch_size, lr, sgd, train_data, valid_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should end up with a dashboard that looks like this:\n",
    "\n",
    "![W&B Dashboard](images/optimizers/wandb_comp.png)\n",
    "\n",
    "We can draw a few conclusions from this:\n",
    "\n",
    "- The higher batch size descends more slowly that a lower batch size.\n",
    "- The higher batch size runs much faster (around 16x faster)\n",
    "\n",
    "Batch size can make a big impact on the final accuracy of your model.  In general, a higher batch size is a form of regularization (which we'll talk about in depth in a later lesson).  Since you're averaging the gradients, you can't fit as tightly to any single training example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
