{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizers\n",
    "\n",
    "In the last few lessons, we learned how to build several neural network architectures.  We used gradient descent, a technique we originally learned in the [dense neural network](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/dense.ipynb) lesson, to adjust model parameters.\n",
    "\n",
    "Gradient descent is a type of optimizer.  Optimizers adjust neural network parameters to try to get loss to (hopefully) a global minimum value.  In this lesson, we'll learn more about optimizers.  We'll first go into more depth on gradient descent, and discuss batch size, learning rate schedules, weight decay, and momentum.  We'll then discuss the AdamW optimizer, which is a popular optimizer that combines momentum and weight decay.  AdamW is the most commonly used optimizer in large language models like GPT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "The type of optimizer we've used so far is called stochastic gradient descent, or SGD.  In SGD, we take a minibatch of training examples, then compute the average gradient across the minibatch.  We then adjust the parameters using this average gradient.\n",
    "\n",
    "Let's define a three-layer dense neural network, then explore the effect of batch size on our loss over time.  We'll first load in a dataset of house prices.\n",
    "\n",
    "Each row in this dataset represents a single house.  The predictor columns are:\n",
    "\n",
    "- `interest`: The interest rate\n",
    "- `vacancy`: The vacancy rate\n",
    "- `cpi`: The consumer price index\n",
    "- `price`: The price of a house\n",
    "- `value`: The value of a house\n",
    "- `adj_price`: The price of a house, adjusted for inflation\n",
    "- `adj_value`: The value of a house, adjusted for inflation\n",
    "\n",
    "The predictor columns have all been scaled using the scikit-learn `StandardScaler`.  This gives each column a mean of 0 and a standard deviation of 1.  This makes it easier to activate our nonlinearities and have the network learn.\n",
    "\n",
    "The target column is `next_quarter`, which is the price of the house in three months.  `next_quarter` has been scaled so the minimum value is `0`, and it has been divided by `1000` and rounded to the nearest integer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "from csv_data import HousePricesDatasetWrapper\n",
    "\n",
    "wrapper = HousePricesDatasetWrapper()\n",
    "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.94509048,  1.39642758, -1.52275521, -0.64519476, -0.11676266,\n        -0.13891486,  0.8225858 ],\n       [ 1.93249288,  1.39642758, -1.49354416, -0.64519476, -0.11676266,\n        -0.15597175,  0.80219322]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the train predictors\n",
    "train_data[0][:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[22.],\n       [22.]])"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the train target\n",
    "train_data[1][:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll define a single layer of our neural network.  We'll make a few modifications from earlier lessons:\n",
    "\n",
    "- Instead of directly updating the parameters, we'll return the weight and bias gradients\n",
    "- We'll add an update method that updates the weights later\n",
    "\n",
    "This will enable us to swap different optimizers in and out of our network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class Dense():\n",
    "    def __init__(self, input_size, output_size, activation=True, seed=0):\n",
    "        self.add_activation = activation\n",
    "        self.hidden = None\n",
    "        self.prev_hidden = None\n",
    "\n",
    "        # Initialize the weights.  They'll be in the range -sqrt(k) to sqrt(k), where k = 1 / input_size\n",
    "        np.random.seed(seed)\n",
    "        k = math.sqrt(1 / input_size)\n",
    "        self.weights = np.random.rand(input_size, output_size) * (2 * k) - k\n",
    "\n",
    "        # Our bias will be initialized to 1\n",
    "        self.bias = np.ones((1,output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Copy the layer input for backprop\n",
    "        self.prev_hidden = x.copy()\n",
    "        # Multiply the input by the weights, then add the bias\n",
    "        x = np.matmul(x, self.weights) + self.bias\n",
    "        # Apply the activation function\n",
    "        if self.add_activation:\n",
    "            x = np.maximum(x, 0)\n",
    "        # Copy the layer output for backprop\n",
    "        self.hidden = x.copy()\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # \"Undo\" the activation function if it was added\n",
    "        if self.add_activation:\n",
    "            grad = np.multiply(grad, np.heaviside(self.hidden, 0))\n",
    "\n",
    "        # Calculate the parameter gradients\n",
    "        w_grad = self.prev_hidden.T @ grad # This is not averaged across the batch, due to the way matrix multiplication sums\n",
    "        b_grad = np.mean(grad, axis=0) # This is averaged across the batch\n",
    "        param_grads = [w_grad, b_grad]\n",
    "\n",
    "        # Calculate the next layer gradient\n",
    "        grad = grad @ self.weights.T\n",
    "        return param_grads, grad\n",
    "\n",
    "    def update(self, w_grad, b_grad):\n",
    "        # Update the weights given an update matrix\n",
    "        self.weights += w_grad\n",
    "        self.bias += b_grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then use the `Dense` class to create a 3-layer neural network.  The first layer will take in our predictors and generate `25` hidden features, the second layer combine those features into `10` new features, and the final layer will make a prediction."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll define functions that run forward and backward passes across all the layers together.  `forward` will do a full forward pass across all 3 layers, and `backward` will do a full backward pass across all 3 layers.\n",
    "\n",
    "The backward pass will return the gradients instead of updating the parameters.  We'll use these gradients to update the parameters in our optimizer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    # Loop through each layer\n",
    "    for layer in layers:\n",
    "        # Run the forward pass\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "def backward(grad, layers):\n",
    "    # Save the gradients for each layer\n",
    "    layer_grads = []\n",
    "    # Loop through each layer in reverse order (starting from the output layer)\n",
    "    for layer in reversed(layers):\n",
    "        # Get the parameter gradients and the next layer gradient\n",
    "        param_grads, grad = layer.backward(grad)\n",
    "        layer_grads.append(param_grads)\n",
    "    return layer_grads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll then define our SGD optimizer, which will take in a set of gradients, and use it to update the network parameters.\n",
    "\n",
    "The process for SGD is:\n",
    "\n",
    "- Normalize the gradient by batch size, so that the gradient is the average gradient across the batch (in our case, our bias gradient is already normalized, but the weight gradient is not)\n",
    "- Multiply the gradient by learning rate\n",
    "- Multiply the gradient by `-1` so it is subtracted from the parameters in the `update` function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, layer_grads, layers, batch_size):\n",
    "        for layer_grad, layer in zip(layer_grads, reversed(layers)):\n",
    "            w_grad, b_grad = layer_grad\n",
    "\n",
    "            # Normalize the weight gradient by batch size\n",
    "            w_grad /= batch_size\n",
    "\n",
    "            # Calculate the update sizes\n",
    "            w_update = -self.lr * w_grad\n",
    "            b_update = -self.lr * b_grad\n",
    "\n",
    "            layer.update(w_update, b_update)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitoring\n",
    "\n",
    "We can now write a function to train the network using the SGD optimizer and our data.  Since we'll be testing different batch sizes and optimizers, we want a way to monitor our network and compare different runs to wach other.  So far, we've been using print statements to monitor per-epoch accuracy in our network, which is hard to compare to other runs.\n",
    "\n",
    "We'll use a tool called [Weights & Biases](https://wandb.ai) to monitor our network.  We can use it to track the loss and accuracy of our network, and compare different runs to each other.  W&B is a free tool, and you can sign up for an account [here](https://wandb.ai).  If you don't want to use W&B, you can also use TensorBoard, but it's harder to setup and use.\n",
    "\n",
    "We'll start by importing `wandb` and logging in.  We'll also set the `WANDB_SILENT` environment variable to `True`, so that W&B doesn't print out a lot of system messages."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With W&B, we track each training run separately.  W&B will track the loss in each epoch, or anything else we want to keep track of.  It will also render graphs for us, so we can compare different runs against each other.\n",
    "\n",
    "The key W&B functions are:\n",
    "\n",
    "- `wandb.init` - This will initialize a new run.  We can use the `config` parameter to pass in run-specific information we want to view.\n",
    "- `wandb.log` - This will log a dictionary of metrics to the current run.\n",
    "- `wandb.define_metric` - This will define a metric that we want to track.  You normally don't need to define wandb metrics upfront, but we'll use a custom `step_metric` to ensure that results from runs with different batch sizes line up.\n",
    "\n",
    "We'll use W&B to log a running training loss, the final training set loss each epoch, and the validation set loss.  We'll also track how long each epoch takes to run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Training Loop\n",
    "\n",
    "We can now write a function that will train our network using SGD, and log the loss to W&B.  This function will be very similar to training loops that we've written in previous lessons.\n",
    "\n",
    "We won't do this here, but you usually want to shuffle your training data each epoch to ensure that batch composition changes.  Random shuffling is important when our batch size is greater than `1`.  As we saw earlier, the gradient is averaged across all the examples in a batch.  Random shuffling will put different training examples together each time, allowing the model to see more combinations of gradients.  This reduces overfitting, and improves the generalization of our network.  It can also make SGD converge faster (get to the global minimum error).\n",
    "\n",
    "In our case, we won't do random shuffling, since the data is time series.  Shuffling can actually cause validation error to increase significantly with time series data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def training_run(epochs, batch_size, optimizer, train_data, valid_data):\n",
    "    # Initialize a new W&B run, with the right parameters\n",
    "    wandb.init(project=\"optimizers\",\n",
    "               config={\"batch_size\": batch_size,\n",
    "                       \"lr\": optimizer.lr,\n",
    "                       \"epochs\": epochs,\n",
    "                       \"optimizer\": type(optimizer).__name__})\n",
    "\n",
    "    # Setup the metrics we want to track with wandb\n",
    "    wandb.define_metric(\"batch_step\") # This will ensure that results from runs with different batch sizes line up\n",
    "    wandb.define_metric(\"epoch\") # This will ensure that results from runs with different batch sizes line up\n",
    "    wandb.define_metric(\"valid_loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"train_loss\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"runtime\", step_metric=\"epoch\")\n",
    "    wandb.define_metric(\"running_loss\", step_metric=\"batch_step\")\n",
    "\n",
    "    # Setup the layers for the training run\n",
    "    layers = [\n",
    "        Dense(7, 25),\n",
    "        Dense(25,10),\n",
    "        Dense(10, 1, activation=False)\n",
    "    ]\n",
    "\n",
    "    # Split the training and valid data into x and y\n",
    "    train_x, train_y = train_data\n",
    "    valid_x, valid_y = valid_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        start = time.time() # The start time of our run\n",
    "\n",
    "        for i in range(0, len(train_x), batch_size):\n",
    "            # Get the x and y batches\n",
    "            x_batch = train_x[i:(i+batch_size)]\n",
    "            y_batch = train_y[i:(i+batch_size)]\n",
    "            # Make a prediction\n",
    "            pred = forward(x_batch, layers)\n",
    "\n",
    "            # Run the backward pass\n",
    "            loss = pred - y_batch\n",
    "            layer_grads = backward(loss, layers)\n",
    "\n",
    "            # Run the optimizer\n",
    "            optimizer(layer_grads, layers, batch_size)\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += np.mean(loss ** 2)\n",
    "\n",
    "            batch_idx = i + batch_size # Get the last index of the current batch\n",
    "            batch_step = batch_idx + epoch * len(train_x)\n",
    "            # Log running loss.  We multiply by batch size to offset the mean from earlier.\n",
    "            wandb.log({\"running_loss\": running_loss / batch_idx * batch_size, \"batch_step\": batch_step})\n",
    "\n",
    "        # Calculate and log validation loss\n",
    "        valid_preds = forward(valid_x, layers)\n",
    "        valid_loss = np.mean((valid_preds - valid_y) ** 2)\n",
    "        train_loss = running_loss / len(train_x) * batch_size\n",
    "        wandb.log({\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"runtime\": time.time() - start\n",
    "        })\n",
    "\n",
    "    # Mark the run as complete\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then initialize the parameters we want to adjust (like batch size), and start the run."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Setup our parameters\n",
    "epochs = 100\n",
    "batch_size = 4\n",
    "lr = 1e-4\n",
    "\n",
    "# Run our training loop\n",
    "sgd = SGD(lr=lr)\n",
    "training_run(epochs, batch_size, sgd, train_data, valid_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should now be able to see your run in W&B.  You will need to click on the `optimizers` project in your [dashboard](https://wandb.ai/home).\n",
    "\n",
    "## Batch Size\n",
    "\n",
    "We can now compare our first run to a run with a higher batch size:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016751036117784678, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbe5e110693f4402a8a418d27ea1b539"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup our parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "\n",
    "# Run our training loop\n",
    "sgd = SGD(lr=lr)\n",
    "training_run(epochs, batch_size, sgd, train_data, valid_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You should end up with a dashboard that looks like this:\n",
    "\n",
    "![W&B Dashboard](images/optimizers/wandb_comp2.png)\n",
    "\n",
    "We can draw a few conclusions from this:\n",
    "\n",
    "1. The higher batch size descends more slowly that a lower batch size\n",
    "2. The higher batch size converges to a lower validation loss, but a higher training loss\n",
    "3. The higher batch size runs much faster (around 8x faster)\n",
    "\n",
    "Batch size can make a big impact on the final accuracy of your model.  In general, a higher batch size is a form of regularization (which we'll talk about in depth in a later lesson).  Regularization increases training loss, but decreases validation loss.  This happens because you're averaging over multiple training examples in the batch, and making it harder to overfit to specific examples.\n",
    "\n",
    "Higher batch sizes can also run much faster than lower batch sizes.  In practice, if the batch size is too high, then validation loss starts to suffer.  So you need to find the batch size that runs quickly and minimizes validation loss.  This is different for each dataset, so it can take some experimentation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Momentum\n",
    "\n",
    "One downside to the basic SGD optimizer is that it has no knowledge of past gradients.  It has to make the decision on how to adjust the parameters based only on the current gradient.  It's like asking you to predict if a stock price will go up or down tomorrow.  If you only know how the stock did today, your guess will be worse than if you know how the stock did over the past year.\n",
    "\n",
    "We can solve this with momentum.  With momentum, we give the optimizer knowledge of the direction the gradient is moving in."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class SGDMomentum():\n",
    "    def __init__(self, lr, alpha):\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.velocities = None\n",
    "\n",
    "    def initialize_velocities(self, layer_grads):\n",
    "        self.velocities = []\n",
    "        for layer_grad in layer_grads:\n",
    "            w_grad, b_grad = layer_grad\n",
    "            initial_velocities = [np.zeros_like(w_grad), np.zeros_like(b_grad)]\n",
    "            self.velocities.append(initial_velocities)\n",
    "\n",
    "    def __call__(self, layer_grads, layers, batch_size):\n",
    "        if self.velocities is None:\n",
    "            self.initialize_velocities(layer_grads)\n",
    "\n",
    "        new_velocities = []\n",
    "        for layer_grad, velocity, layer in zip(layer_grads, self.velocities, reversed(layers)):\n",
    "            w_grad, b_grad = layer_grad\n",
    "            w_vel, b_vel = velocity\n",
    "\n",
    "            # Normalize the weight gradient by batch size\n",
    "            w_grad /= batch_size\n",
    "\n",
    "            # Calculate the update sizes\n",
    "            w_vel = w_vel * self.alpha - self.lr * w_grad\n",
    "            b_vel = b_vel * self.alpha - self.lr * b_grad\n",
    "\n",
    "            layer.update(w_vel, b_vel)\n",
    "            new_velocities.append([w_vel, b_vel])\n",
    "        self.velocities = new_velocities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# Setup our parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "lr = 1e-4\n",
    "\n",
    "# Run our training loop\n",
    "sgd = SGDMomentum(lr=lr, alpha=.9)\n",
    "training_run(epochs, batch_size, sgd, train_data, valid_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, SGD with momentum decreases training loss, but increases validation loss."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Rate\n",
    "\n",
    "The most important hyperparameter for any optimizer is the learning rate.  The learning rate has a big effect on the performance of a model.  So far, we've used a fixed learning rate that doesn't change during training.\n",
    "\n",
    "In practice, it's common to use a learning rate scheduler.  A learning rate scheduler changes the learning rate during training.  This helps the model converge, and avoid taking gradient descent steps in the wrong direction.\n",
    "\n",
    "It's common to alter the learning rate in two ways:\n",
    "\n",
    "- Set the learning rate low initially as a warmup phase.  This helps to stabilize training, and avoid large gradient steps initially."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
