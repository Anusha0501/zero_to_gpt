{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Backpropagation in depth\n",
    "\n",
    "In the [last lesson](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/rnn.ipynb), we learned how to create a recurrent neural network.  We now know how to build several network architectures using components like dense layers, softmax, and recurrent layers.\n",
    "\n",
    "We've been a bit loose with how we cover backpropagation, so that neural network architecture is easier to understand.  Backpropagation is how a neural network calculates how much to change each parameter in the network (the gradient).  In this lesson, we'll do a deep dive into how backpropagation works.  We'll do this by building a computational graph to keep track of which changes we make to input data.\n",
    "\n",
    "A computational graph looks like this:\n",
    "\n",
    "![](comp_graph.png)\n",
    "\n",
    "It shows all the individual operations we performed (like multiplication) to modify the value of `X`, in order.  Keeping track of a computational graph is how we know how to reverse our operations to do backpropagation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Softmax function\n",
    "\n",
    "We'll first build a computational graph of the softmax function, then backpropagate through that graph to get the gradient against our inputs.  We introduced the softmax function in a previous lesson.  It's used to convert the output of a neural network into probabilities that can be used as predictions.  The softmax function is defined as:\n",
    "\n",
    "$$\\zeta=\\frac{e^{\\hat{y_{i}}}}{\\sum_{j=0}e^{\\hat{y_{j}}}}$$\n",
    "\n",
    "For each row of our neural network output, we raise $e$ to the power of our output value, then divide by the sum of $e$ raised to the power of each of the outputs for that row.\n",
    "\n",
    "The softmax function looks like this in code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_func(normalized):\n",
    "    raised = np.exp(normalized)\n",
    "    output = raised / np.sum(raised, axis=1).reshape(-1,1)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can test the softmax function using some fake data that we generate:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 5 rows and 3 columns of random numbers\n",
    "x = np.random.rand(5, 3)\n",
    "\n",
    "# Generate random correct labels for later\n",
    "# Exactly one label per row will be correct\n",
    "y = np.zeros_like(x)\n",
    "inds = (np.arange(0,y.shape[0]), np.random.randint(0, 3, size=y.shape[0]))\n",
    "y[inds] = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`x` is our input to the softmax function.  It has `3` columns.  `y` is our target, where each row is a one-hot encoded vector.  The `1` will correspond to the correct label for each row.\n",
    "\n",
    "We can then apply the softmax function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.36880196, 0.33076319, 0.30043485],\n       [0.35141995, 0.21574601, 0.43283403],\n       [0.42527348, 0.29515779, 0.27956873],\n       [0.25037301, 0.3852874 , 0.36433959],\n       [0.25885158, 0.36256327, 0.37858516]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = x - np.max(x, axis=-1).reshape(-1,1)\n",
    "softmax_func(normalized)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above code, we subtract the maximum from each element in the row before passing the data into the softmax function.  This prevents numerical underflow or overflow.  Each [numeric type](https://numpy.org/doc/stable/user/basics.types.html) (float, integer, etc) can only hold a certain number of digits.  For example, floating point 16 can store 5 exponent bits, and ten digit bits (each bit is only base 2, so this is less than the same number of base-10 digits).  The maximum value we can store in `float16` is `65500`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "65500.0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the maximum value we can assign to float16\n",
    "np.finfo('float16').max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/9z84c__j28g8tg28bmcthjj00000gn/T/ipykernel_32179/2542651828.py:3: RuntimeWarning: overflow encountered in cast\n",
      "  a[0] = 6.55e5\n"
     ]
    }
   ],
   "source": [
    "# This is an example of numeric overflow, where we store more digits than float16 can hold\n",
    "a = np.array([0], dtype=np.float16)\n",
    "a[0] = 6.55e5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we raise $e$ to a very large or small number, we can generate a number that is too large to store in our specific data type.  Subtracting the max gives us the same end result, but reduces the risk of overflow.  Feel free to try the softmax out with and without subtracting the max to see how it works!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Staged Softmax\n",
    "\n",
    "Instead of computing the softmax derivative, we previously used the fact that the derivative of the softmax and negative log likelihood functions \"cancel out\", and end up with a derivative of $p-y$.  But what if we want to find the derivative ourselves?\n",
    "\n",
    "We can approach it analytically, and find the derivative of the entire function.  We can even use SymPy to help us do the derivation, like we did in an earlier lesson.  Another method is to break the softmax function apart into individual operations.  Each operation will make a single modification to the data:\n",
    "\n",
    "![](images/comp_graph/softmax_steps.svg)\n",
    "\n",
    "We perform 3 operations on the data:\n",
    "\n",
    "- Exp - we raise e to the power x.\n",
    "- Sum - we add up the $e^x$ values for each row.\n",
    "- Divide - we divide the $e^x$ values by the sums.\n",
    "\n",
    "Note that the output of `Exp` is passed to both the `Sum` and `Divide` operations.\n",
    "\n",
    "By breaking up the softmax this way, we can take the derivative of each individual piece instead of the whole function at once.  By the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review), multiplying the derivative of each individual operation will result in the derivative of the whole function.  We used the chain rule in previous lessons to find the partial derivative of the loss with respect to the model weights and biases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can build the forward pass of our staged softmax.  The derivative of multiplication is easier to calculate than division, so we'll swap some of our operations to remove the division.\n",
    "\n",
    "Luckily for us, raising a value `x` to the power `-1` is the same as taking `1/x`.  So instead of dividing `Exp/Sum`, we can do `Exp * Sum ^ -1$, leaving us with these operations:\n",
    "\n",
    "- Exp\n",
    "- Sum\n",
    "- Pow - we invert the sum by raising to the power `-1`\n",
    "- Multiply - we multiply the inverted sum and the exp values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "raised = np.exp(x) # step 1\n",
    "summed = np.sum(raised, axis=-1).reshape(-1,1) # step 2.  reshape so each row has 1 column.\n",
    "pow = summed ** -1 # step 3\n",
    "staged_softmax = raised * pow # step 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.36880196, 0.33076319, 0.30043485],\n       [0.35141995, 0.21574601, 0.43283403],\n       [0.42527348, 0.29515779, 0.27956873],\n       [0.25037301, 0.3852874 , 0.36433959],\n       [0.25885158, 0.36256327, 0.37858516]])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "staged_softmax"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our staged softmax has the exact same output as our original function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Softmax Derivative\n",
    "\n",
    "To get the derivative of the softmax, we need to reverse the operations we did before:\n",
    "\n",
    "![](images/comp_graph/softmax_steps_full_bwd.svg)\n",
    "\n",
    "To compute the full derivative, here are the steps we need to follow:\n",
    "\n",
    "1. Start at the final step.  Take in the loss gradient as the input, and multiply by the derivative of the final step.\n",
    "2. Pass the gradient to the previous operation.\n",
    "2. Continue calculating the derivative of each operation, and multiplying by the gradient.  Note that `Exp` is input to two operations, so it will sum both gradients.\n",
    "4. Continue until we reach the first operation.\n",
    "\n",
    "To calculate loss, we'll use negative log likelihood, which is $NLL = - \\sum_{i=0} y_{i} \\log p_{i}$.  Since $y$ is only non-zero at one position per row, this will only have a single term (basically $-y_{i} * \\log p_{i}$ where $i$ is the correct label where $y$ equals `1`).\n",
    "\n",
    "We'll use the negative log likelihood derivative below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def nll_grad(y, pred):\n",
    "    return -1 * y / pred\n",
    "\n",
    "loss_grad = nll_grad(y, staged_softmax)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then calculate the softmax derivative by multiplying the derivatives of the individual operations.  Remember that a derivative is the rate of change of a function as we change the input.\n",
    "\n",
    "- Exp - the derivative of $e^x$ is $e^x$ (this is a very cool property of $e$!)\n",
    "- Sum - since a sum operation will combine input elements into one, we just distribute the gradient over all input elements.  A change to any of the input elements will have a direct impact on the output.\n",
    "- Pow - the derivative of $x^{-1}$ is $-1 * x^{-2}$.  More on this [here](https://www.khanacademy.org/math/old-ap-calculus-ab/ab-derivative-rules/ab-diff-negative-fraction-powers/a/power-rule-review).\n",
    "- Multiply - the derivatives of $x*y$ are $y$ wrt $x$ and $x$ wrt $y$.  This is because any change to $x$ is multiplied by $y$, and vice versa.  Thus the rate of change of $x$ is $y$, and vice versa.\n",
    "\n",
    "We can now create the backward pass of our staged softmax.  The backward pass will start with the loss gradient.  This will be a matrix showing how much we need to adjust each of the output values from our softmax to reduce our loss.  We can then compute gradients for each operation, ending with the gradient against the input, `x`.  If `x` was the output of a neural network, we would continue backpropagation at that point to adjust the network parameters.\n",
    "\n",
    "We'll name each gradient according to the step it is a gradient for, not the step it is coming from.  So `raised_grad` is the gradient on `raised`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Step 4 derivative\n",
    "raised_grad = loss_grad * pow\n",
    "pow_grad = loss_grad * raised\n",
    "pow_grad = np.sum(pow_grad, axis=-1).reshape(-1,1) # reshape gradient to match input data\n",
    "\n",
    "# Step 3\n",
    "summed_grad = (-1 * summed ** -2) * pow_grad\n",
    "\n",
    "# Step 2\n",
    "raised_grad_2 = np.ones_like(raised) * summed_grad # distribute gradient across inputs\n",
    "\n",
    "# Step 1\n",
    "raised_grad += raised_grad_2 # sum incoming gradients\n",
    "staged_softmax_grad = raised_grad * np.exp(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We did two things above that might be confusing.  The first is that we summed 2 gradients on raised.  This is because raised connects to 2 operations, and both have separate gradients.  Whenever this happens, we sum the gradients.\n",
    "\n",
    "The second is that we reshaped `pow_grad` to have a single column.  This is to match `pow`, which only had `1` column in the forward pass.  Whenever a gradient doesn't match the shape of the input data, we change the size of the gradient to match it.  This is because the gradient represents the partial derivative against the input data to the operation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can compare our result to the derivative of the softmax equation to make sure everything worked.  The derivative of the softmax is $S_{i}((i==j) - S_{j})$.  We take each element of a single row in the output of a softmax, like this:\n",
    "\n",
    "    [0.28, 0.25, 0.47]\n",
    "\n",
    "We then compare each element against each other element.  So we could start at element `0` (`.28`), and compare it to itself.  Then `i` is `0` and `j` is `0`.  So the equation is `.28 * (1 - .28)`.  When we then keep `i` the same, but move `j` to `1`.  The equation becomes `.28 * (0 - .25)`.  And so on, until we construct a matrix like this:\n",
    "\n",
    "![](images/comp_graph/softmax_deriv.svg)\n",
    "\n",
    "We then sum across the rows and multiply by the incoming gradient to get the partial derivative against the inputs.  We can define the softmax derivative in code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def softmax_grad_func(softmax, loss_grad):\n",
    "    output = np.zeros_like(softmax)\n",
    "    for i in range(softmax.shape[0]):\n",
    "        sm_row = softmax[i,:]\n",
    "        sm_grad = (-np.outer(sm_row, sm_row) + np.diag(sm_row.flatten()))\n",
    "        row_grad = sm_grad * loss_grad[i,:].reshape(1,-1)\n",
    "        output[i,:] = np.sum(row_grad, -1)\n",
    "    return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then compare our derivative by stages with the derivation.  The `np.allclose` function tells us if all the values in an array are close to another array.  We use this instead of `==` because there are small numerical differences in similar computations with numpy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_softmax_grad = softmax_grad_func(staged_softmax, loss_grad)\n",
    "np.allclose(derived_softmax_grad, staged_softmax_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also compare the gradient we computed of the loss and softmax together with the derivative of both together, which is `p-y`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(staged_softmax_grad, staged_softmax - y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Operations\n",
    "\n",
    "Breaking the softmax into stages helped us understand the basic units of a computational graph.  But what if we don't want to have to type out all the code for the forward and backward pass every time?  It would be nice if we could only define the forward pass, and automatically have the backward pass happen.\n",
    "\n",
    "We can do this by individually defining each operation, then mixing and matching the operations to create a more complex equation.  By doing this, we can create a computational graph like this one:\n",
    "\n",
    "![](comp_graph.png)\n",
    "\n",
    "Each node in the graph will be a separate class that knows how to do a forward and backward pass.  So we can just execute the graph to run forward and backward passes.\n",
    "\n",
    "We can start out by defining the operations.  I've written a class called `Node`, which we can subclass to define each operation.  You can look at the code for `Node` if you want.  It gives us some nice methods for running the operations in a graph in order, both forward and backward:\n",
    "\n",
    "- `apply_fwd` - runs the forward pass up to the node it is called on.\n",
    "- `apply_bwd` - runs the backward pass from the node it is called on backwards.\n",
    "- `generate_graph` - helps us visualize the computational graph.\n",
    "- `generate_derivative_chains` - shows us the equation for calculating the partial derivative at a node.\n",
    "\n",
    "For each operation, we just implement the `forward` and `backward` methods, which take in input data, and pass them through the operation.  The `Node` class takes care of the rest.\n",
    "\n",
    "We'll use the same formulas for each operation that we used in our staged softmax earlier:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"../nnets\"))\n",
    "from graph import Node, Parameter, display_chain\n",
    "\n",
    "class Exp(Node):\n",
    "    def forward(self, x):\n",
    "        return np.exp(x) # raise e to the power x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0] # Pull the x value used in the forward pass\n",
    "        return np.exp(x) * grad # multiply the incoming gradient by the derivative\n",
    "\n",
    "class Sum(Node):\n",
    "    def forward(self, x):\n",
    "        return np.sum(x, axis=-1).reshape(-1,1)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0] # Pull the x value used in the forward pass\n",
    "        return np.ones_like(x) * grad # distribute the gradient over the input data shape\n",
    "\n",
    "class Pow(Node):\n",
    "    def forward(self, x, exponent):\n",
    "        return x ** exponent\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, exponent = self.cache # Pull the x and exponent values used in the forward pass\n",
    "        return grad * exponent * x ** (exponent - 1), 1\n",
    "\n",
    "class Multiply(Node):\n",
    "    def forward(self, x, y):\n",
    "        return x * y\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, y = self.cache # Pull the x and y values used in the forward pass\n",
    "        return grad * y, grad * x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see above, we've defined the 4 operations that we need for our softmax in code.\n",
    "\n",
    "We can now define our whole softmax operation as a computational graph.  When we initialize a `Node`, we pass in the nodes that feed into it.  If we're using the node to feed in data (like inputs), we use a special `Parameter` node:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# The input data for our network.  We set needs_grad=True so the gradient is calculated for this parameter in the backward pass.\n",
    "# Desc is a short description of what the data in this node is.\n",
    "X = Parameter(x, desc=\"X\", needs_grad=True)\n",
    "\n",
    "# Raise e to the power x.  Out is a description of the output of the node.\n",
    "raised = Exp(X, out=\"e^X\")\n",
    "# Sum the raised values.\n",
    "summed = Sum(raised, out=\"sum(e^X)\")\n",
    "\n",
    "# Define -1 as a parameter, so we can use it as an exponent.\n",
    "negative_one = Parameter(-1, desc=\"-1\", needs_grad=False)\n",
    "# Invert our sums\n",
    "inverted = Pow(summed, negative_one, out=\"1 / sum(e^X)\")\n",
    "# Multiply the inverted sums by e^X\n",
    "softmax = Multiply(raised, inverted, out=\"softmax(X)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now use the `generate_graph` method on the `softmax` node to visualize the computational graph."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 7.0.6 (20230106.0513)\n -->\n<!-- Title: fwd_pass Pages: 1 -->\n<svg width=\"614pt\" height=\"128pt\"\n viewBox=\"0.00 0.00 614.09 128.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 124)\">\n<title>fwd_pass</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-124 610.09,-124 610.09,4 -4,4\"/>\n<!-- 5356451008 -->\n<g id=\"node1\" class=\"node\">\n<title>5356451008</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"563.84\" cy=\"-18\" rx=\"42.49\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"563.84\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Multiply</text>\n</g>\n<!-- 5352333344 -->\n<g id=\"node2\" class=\"node\">\n<title>5352333344</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"127\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"127\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">Exp</text>\n</g>\n<!-- 5352333344&#45;&gt;5356451008 -->\n<g id=\"edge2\" class=\"edge\">\n<title>5352333344&#45;&gt;5356451008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M154.22,-18C224.04,-18 415.52,-18 509.46,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"509.44,-21.5 519.44,-18 509.44,-14.5 509.44,-21.5\"/>\n<text text-anchor=\"middle\" x=\"313.1\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\">e^X</text>\n</g>\n<!-- 5356446880 -->\n<g id=\"node5\" class=\"node\">\n<title>5356446880</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"240.3\" cy=\"-102\" rx=\"27.1\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"240.3\" y=\"-98.3\" font-family=\"Times,serif\" font-size=\"14.00\">Sum</text>\n</g>\n<!-- 5352333344&#45;&gt;5356446880 -->\n<g id=\"edge3\" class=\"edge\">\n<title>5352333344&#45;&gt;5356446880</title>\n<path fill=\"none\" stroke=\"black\" d=\"M145.39,-31.12C163.42,-44.73 191.82,-66.17 212.68,-81.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"210.36,-84.55 220.45,-87.78 214.58,-78.96 210.36,-84.55\"/>\n<text text-anchor=\"middle\" x=\"183.5\" y=\"-70.8\" font-family=\"Times,serif\" font-size=\"14.00\">e^X</text>\n</g>\n<!-- 5356444720 -->\n<g id=\"node3\" class=\"node\">\n<title>5356444720</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n</g>\n<!-- 5356444720&#45;&gt;5352333344 -->\n<g id=\"edge1\" class=\"edge\">\n<title>5356444720&#45;&gt;5352333344</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.26,-18C64.7,-18 76.9,-18 88.25,-18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"88.15,-21.5 98.15,-18 88.15,-14.5 88.15,-21.5\"/>\n<text text-anchor=\"middle\" x=\"77\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n</g>\n<!-- 5356446208 -->\n<g id=\"node4\" class=\"node\">\n<title>5356446208</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"385.6\" cy=\"-48\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"385.6\" y=\"-44.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pow</text>\n</g>\n<!-- 5356446208&#45;&gt;5356451008 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5356446208&#45;&gt;5356451008</title>\n<path fill=\"none\" stroke=\"black\" d=\"M412.22,-43.64C438.44,-39.17 479.87,-32.12 512.64,-26.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"513.07,-30.02 522.34,-24.89 511.9,-23.12 513.07,-30.02\"/>\n<text text-anchor=\"middle\" x=\"467.1\" y=\"-42.8\" font-family=\"Times,serif\" font-size=\"14.00\">1 / sum(e^X)</text>\n</g>\n<!-- 5356446880&#45;&gt;5356446208 -->\n<g id=\"edge4\" class=\"edge\">\n<title>5356446880&#45;&gt;5356446208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M265.14,-94.11C285.36,-87.3 315.06,-77 340.6,-67 344.46,-65.48 348.5,-63.84 352.49,-62.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"353.57,-65.51 361.39,-58.36 350.82,-59.07 353.57,-65.51\"/>\n<text text-anchor=\"middle\" x=\"313.1\" y=\"-89.8\" font-family=\"Times,serif\" font-size=\"14.00\">sum(e^X)</text>\n</g>\n<!-- 5356450624 -->\n<g id=\"node6\" class=\"node\">\n<title>5356450624</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"240.3\" cy=\"-48\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"240.3\" y=\"-44.3\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1</text>\n</g>\n<!-- 5356450624&#45;&gt;5356446208 -->\n<g id=\"edge5\" class=\"edge\">\n<title>5356450624&#45;&gt;5356446208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M267.58,-48C289.9,-48 322.2,-48 347.21,-48\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"346.97,-51.5 356.97,-48 346.97,-44.5 346.97,-51.5\"/>\n<text text-anchor=\"middle\" x=\"313.1\" y=\"-51.8\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x13f44e800>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations_softmax = softmax.apply_fwd()\n",
    "\n",
    "np.allclose(staged_softmax, operations_softmax)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 7.0.6 (20230106.0513)\n -->\n<!-- Title: fwd_pass Pages: 1 -->\n<svg width=\"678pt\" height=\"139pt\"\n viewBox=\"0.00 0.00 678.09 139.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 135)\">\n<title>fwd_pass</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-135 674.09,-135 674.09,4 -4,4\"/>\n<!-- 4460316848 -->\n<g id=\"node1\" class=\"node\">\n<title>4460316848</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"42.25\" cy=\"-95\" rx=\"42.49\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"42.25\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">Multiply</text>\n</g>\n<!-- 4460324864 -->\n<g id=\"node2\" class=\"node\">\n<title>4460324864</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"527.09\" cy=\"-95\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"527.09\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">Exp</text>\n</g>\n<!-- 4460316848&#45;&gt;4460324864 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4460316848&#45;&gt;4460324864</title>\n<path fill=\"none\" stroke=\"red\" d=\"M82.03,-101.75C88.83,-102.69 95.85,-103.5 102.49,-104 270.72,-116.7 314.24,-120.99 482.09,-104 484.61,-103.74 487.2,-103.41 489.8,-103.02\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"490.36,-106.47 499.6,-101.29 489.15,-99.58 490.36,-106.47\"/>\n<text text-anchor=\"middle\" x=\"316.99\" y=\"-119.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(e^X)</text>\n</g>\n<!-- 4460322032 -->\n<g id=\"node4\" class=\"node\">\n<title>4460322032</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"236.49\" cy=\"-72\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"236.49\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">Pow</text>\n</g>\n<!-- 4460316848&#45;&gt;4460322032 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4460316848&#45;&gt;4460322032</title>\n<path fill=\"none\" stroke=\"red\" d=\"M83.55,-90.18C117.38,-86.13 165.33,-80.39 198.26,-76.45\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"198.65,-79.93 208.16,-75.27 197.82,-72.98 198.65,-79.93\"/>\n<text text-anchor=\"middle\" x=\"146.99\" y=\"-91.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(1 / sum(e^X))</text>\n</g>\n<!-- 4460317472 -->\n<g id=\"node3\" class=\"node\">\n<title>4460317472</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"643.09\" cy=\"-95\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"643.09\" y=\"-91.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n</g>\n<!-- 4460324864&#45;&gt;4460317472 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4460324864&#45;&gt;4460317472</title>\n<path fill=\"none\" stroke=\"red\" d=\"M554.53,-95C569.35,-95 588.08,-95 604.41,-95\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"604.28,-98.5 614.28,-95 604.28,-91.5 604.28,-98.5\"/>\n<text text-anchor=\"middle\" x=\"585.09\" y=\"-98.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(X)</text>\n</g>\n<!-- 4460318432 -->\n<g id=\"node5\" class=\"node\">\n<title>4460318432</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"397.79\" cy=\"-72\" rx=\"27.1\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"397.79\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">Sum</text>\n</g>\n<!-- 4460322032&#45;&gt;4460318432 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4460322032&#45;&gt;4460318432</title>\n<path fill=\"none\" stroke=\"red\" d=\"M263.8,-72C289.74,-72 329.65,-72 358.99,-72\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"358.93,-75.5 368.93,-72 358.93,-68.5 358.93,-75.5\"/>\n<text text-anchor=\"middle\" x=\"316.99\" y=\"-75.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(sum(e^X))</text>\n</g>\n<!-- 4460319296 -->\n<g id=\"node6\" class=\"node\">\n<title>4460319296</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"397.79\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"397.79\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;1</text>\n</g>\n<!-- 4460322032&#45;&gt;4460319296 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4460322032&#45;&gt;4460319296</title>\n<path fill=\"none\" stroke=\"red\" d=\"M261,-64.03C287.82,-54.94 331.93,-39.99 362.57,-29.6\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"363.41,-33.01 371.76,-26.49 361.16,-26.38 363.41,-33.01\"/>\n</g>\n<!-- 4460318432&#45;&gt;4460324864 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4460318432&#45;&gt;4460324864</title>\n<path fill=\"none\" stroke=\"red\" d=\"M424.24,-76.6C442.98,-79.98 468.72,-84.63 489.63,-88.41\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"488.72,-91.81 499.19,-90.14 489.97,-84.92 488.72,-91.81\"/>\n<text text-anchor=\"middle\" x=\"462.59\" y=\"-90.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(e^X)</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x109db1fc0>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph(backward=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.zero_grad()\n",
    "softmax.apply_bwd(loss_grad)\n",
    "operations_softmax_grad = X.grad\n",
    "\n",
    "np.allclose(staged_softmax_grad, operations_softmax_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$\\frac{\\partial}{\\partial X} = \\frac{\\partial softmax(X)}{\\partial e^X}*\\frac{\\partial e^X}{\\partial X} + \\\\\\frac{\\partial softmax(X)}{\\partial 1 / sum(e^X)}*\\frac{\\partial 1 / sum(e^X)}{\\partial sum(e^X)}*\\frac{\\partial sum(e^X)}{\\partial e^X}*\\frac{\\partial e^X}{\\partial X}$"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_derivative_chains()\n",
    "display_chain(X.display_partial_derivative())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class MatMul(Node):\n",
    "    def forward(self, x, w):\n",
    "        return x @ w\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, w = self.cache\n",
    "        return grad @ w.T, x.T @ grad\n",
    "\n",
    "class Add(Node):\n",
    "    def forward(self, x, b):\n",
    "        return x + b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad, grad\n",
    "\n",
    "class Relu(Node):\n",
    "    def forward(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0]\n",
    "        new_grad = np.array(grad)\n",
    "        new_grad[x < 0] = 0\n",
    "        return new_grad\n",
    "\n",
    "class Subtract(Node):\n",
    "    def forward(self, x, y):\n",
    "        return x - y\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return grad, -grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class Softmax(Node):\n",
    "    def forward(self, x):\n",
    "        return softmax_func(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0]\n",
    "        softmax = self.forward(x)\n",
    "        return softmax_grad_func(softmax, grad)\n",
    "\n",
    "class Dense(Node):\n",
    "    def forward(self, x, w, b):\n",
    "        return x @ w + b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, w, b = self.cache\n",
    "        return grad @ w, x.T @ grad, grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../data\"))\n",
    "from csv_data import SkyServerDatasetWrapper\n",
    "\n",
    "# Load the data with 3 target values instead of the binary value from earlier\n",
    "wrapper = SkyServerDatasetWrapper()\n",
    "[train_x, train_y], [valid_x, valid_y], [test_x, test_y] = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def encode(target, max_value=3):\n",
    "    encoded = np.zeros((target.shape[0], max_value))\n",
    "    inds = (np.arange(0,target.shape[0]), target.reshape(-1))\n",
    "    encoded[inds] = 1\n",
    "    return encoded\n",
    "\n",
    "train_y = encode(train_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 0., 0.],\n       [1., 0., 0.],\n       [0., 1., 0.],\n       ...,\n       [0., 1., 0.],\n       [0., 1., 0.],\n       [0., 1., 0.]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Random weight init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "w1 = np.random.rand(13, 10)\n",
    "b1 = np.random.rand(1, 10)\n",
    "w2 = np.random.rand(10, 3)\n",
    "b2 = np.random.rand(1, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "X = Parameter(train_x, desc=\"X\", needs_grad=False)\n",
    "Y = Parameter(train_y, desc=\"y\", needs_grad=False)\n",
    "\n",
    "w1_param = Parameter(w1, desc=\"W1\")\n",
    "b1_param = Parameter(b1, desc=\"b1\")\n",
    "\n",
    "matmul1 = MatMul(X, w1_param, out=\"X @ W1\")\n",
    "add1 = Add(matmul1, b1_param, out=\"Z1\")\n",
    "\n",
    "layer1 = Relu(add1, out=\"A1\")\n",
    "\n",
    "w2_param = Parameter(w2, desc=\"W2\")\n",
    "b2_param = Parameter(b2, desc=\"b2\")\n",
    "matmul2 = MatMul(layer1, w2_param, out=\"Z1 @ W2\")\n",
    "add2 = Add(matmul2, b2_param, out=\"Z2\")\n",
    "\n",
    "softmax = Softmax(add2, out=\"softmax(Z2)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 7.0.6 (20230106.0513)\n -->\n<!-- Title: fwd_pass Pages: 1 -->\n<svg width=\"862pt\" height=\"191pt\"\n viewBox=\"0.00 0.00 861.67 191.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n<title>fwd_pass</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-187 857.67,-187 857.67,4 -4,4\"/>\n<!-- 5350225040 -->\n<g id=\"node1\" class=\"node\">\n<title>5350225040</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"812.73\" cy=\"-37\" rx=\"40.89\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"812.73\" y=\"-33.3\" font-family=\"Times,serif\" font-size=\"14.00\">Softmax</text>\n</g>\n<!-- 5350225088 -->\n<g id=\"node2\" class=\"node\">\n<title>5350225088</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"692.78\" cy=\"-37\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"692.78\" y=\"-33.3\" font-family=\"Times,serif\" font-size=\"14.00\">Add</text>\n</g>\n<!-- 5350225088&#45;&gt;5350225040 -->\n<g id=\"edge10\" class=\"edge\">\n<title>5350225088&#45;&gt;5350225040</title>\n<path fill=\"none\" stroke=\"black\" d=\"M720.25,-37C732.03,-37 746.33,-37 760.08,-37\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"759.93,-40.5 769.93,-37 759.93,-33.5 759.93,-40.5\"/>\n<text text-anchor=\"middle\" x=\"745.78\" y=\"-40.8\" font-family=\"Times,serif\" font-size=\"14.00\">Z2</text>\n</g>\n<!-- 5350225136 -->\n<g id=\"node3\" class=\"node\">\n<title>5350225136</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"534.48\" cy=\"-72\" rx=\"40.09\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"534.48\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">MatMul</text>\n</g>\n<!-- 5350225136&#45;&gt;5350225088 -->\n<g id=\"edge8\" class=\"edge\">\n<title>5350225136&#45;&gt;5350225088</title>\n<path fill=\"none\" stroke=\"black\" d=\"M570.95,-64.06C596.26,-58.39 630.18,-50.8 655.63,-45.09\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"656.2,-48.55 665.2,-42.95 654.67,-41.72 656.2,-48.55\"/>\n<text text-anchor=\"middle\" x=\"620.28\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">Z1 @ W2</text>\n</g>\n<!-- 5350225472 -->\n<g id=\"node4\" class=\"node\">\n<title>5350225472</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"409.89\" cy=\"-103\" rx=\"27.1\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"409.89\" y=\"-99.3\" font-family=\"Times,serif\" font-size=\"14.00\">Relu</text>\n</g>\n<!-- 5350225472&#45;&gt;5350225136 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5350225472&#45;&gt;5350225136</title>\n<path fill=\"none\" stroke=\"black\" d=\"M435.7,-96.73C450.69,-92.94 470.27,-87.99 487.96,-83.51\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"488.65,-86.95 497.49,-81.1 486.94,-80.16 488.65,-86.95\"/>\n<text text-anchor=\"middle\" x=\"465.69\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\">A1</text>\n</g>\n<!-- 5350225424 -->\n<g id=\"node5\" class=\"node\">\n<title>5350225424</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"303.59\" cy=\"-103\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"303.59\" y=\"-99.3\" font-family=\"Times,serif\" font-size=\"14.00\">Add</text>\n</g>\n<!-- 5350225424&#45;&gt;5350225472 -->\n<g id=\"edge5\" class=\"edge\">\n<title>5350225424&#45;&gt;5350225472</title>\n<path fill=\"none\" stroke=\"black\" d=\"M330.91,-103C343.14,-103 357.9,-103 371.28,-103\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"371.07,-106.5 381.07,-103 371.07,-99.5 371.07,-106.5\"/>\n<text text-anchor=\"middle\" x=\"356.59\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">Z1</text>\n</g>\n<!-- 4460323808 -->\n<g id=\"node6\" class=\"node\">\n<title>4460323808</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"151.3\" cy=\"-138\" rx=\"40.09\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"151.3\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\">MatMul</text>\n</g>\n<!-- 4460323808&#45;&gt;5350225424 -->\n<g id=\"edge3\" class=\"edge\">\n<title>4460323808&#45;&gt;5350225424</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.53,-129.79C211.39,-124.24 242.82,-116.92 266.79,-111.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"267.48,-114.77 276.43,-109.09 265.89,-107.95 267.48,-114.77\"/>\n<text text-anchor=\"middle\" x=\"234.09\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\">X @ W1</text>\n</g>\n<!-- 4460316944 -->\n<g id=\"node7\" class=\"node\">\n<title>4460316944</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-165\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-161.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n</g>\n<!-- 4460316944&#45;&gt;4460323808 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4460316944&#45;&gt;4460323808</title>\n<path fill=\"none\" stroke=\"black\" d=\"M53.04,-159.47C67.6,-156.26 86.42,-152.1 103.62,-148.31\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"104.13,-151.78 113.14,-146.2 102.62,-144.94 104.13,-151.78\"/>\n<text text-anchor=\"middle\" x=\"82.5\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n</g>\n<!-- 4460323424 -->\n<g id=\"node8\" class=\"node\">\n<title>4460323424</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-111\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-107.3\" font-family=\"Times,serif\" font-size=\"14.00\">W1</text>\n</g>\n<!-- 4460323424&#45;&gt;4460323808 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4460323424&#45;&gt;4460323808</title>\n<path fill=\"none\" stroke=\"black\" d=\"M53.04,-116.53C67.6,-119.74 86.42,-123.9 103.62,-127.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"102.62,-131.06 113.14,-129.8 104.13,-124.22 102.62,-131.06\"/>\n<text text-anchor=\"middle\" x=\"82.5\" y=\"-128.8\" font-family=\"Times,serif\" font-size=\"14.00\">W1</text>\n</g>\n<!-- 4460314736 -->\n<g id=\"node9\" class=\"node\">\n<title>4460314736</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"151.3\" cy=\"-84\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"151.3\" y=\"-80.3\" font-family=\"Times,serif\" font-size=\"14.00\">b1</text>\n</g>\n<!-- 4460314736&#45;&gt;5350225424 -->\n<g id=\"edge4\" class=\"edge\">\n<title>4460314736&#45;&gt;5350225424</title>\n<path fill=\"none\" stroke=\"black\" d=\"M178.63,-85.21C200.21,-86.45 231.51,-88.8 258.59,-93 261.27,-93.42 264.03,-93.9 266.8,-94.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"266.05,-97.85 276.56,-96.48 267.49,-91 266.05,-97.85\"/>\n<text text-anchor=\"middle\" x=\"234.09\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">b1</text>\n</g>\n<!-- 5350225520 -->\n<g id=\"node10\" class=\"node\">\n<title>5350225520</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"409.89\" cy=\"-49\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"409.89\" y=\"-45.3\" font-family=\"Times,serif\" font-size=\"14.00\">W2</text>\n</g>\n<!-- 5350225520&#45;&gt;5350225136 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5350225520&#45;&gt;5350225136</title>\n<path fill=\"none\" stroke=\"black\" d=\"M436.73,-51.94C448.77,-53.47 463.26,-55.54 476.19,-58 480.03,-58.73 483.99,-59.56 487.96,-60.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"487.06,-63.81 497.59,-62.64 488.63,-56.99 487.06,-63.81\"/>\n<text text-anchor=\"middle\" x=\"465.69\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">W2</text>\n</g>\n<!-- 5350225568 -->\n<g id=\"node11\" class=\"node\">\n<title>5350225568</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"534.48\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"534.48\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">b2</text>\n</g>\n<!-- 5350225568&#45;&gt;5350225088 -->\n<g id=\"edge9\" class=\"edge\">\n<title>5350225568&#45;&gt;5350225088</title>\n<path fill=\"none\" stroke=\"black\" d=\"M561.93,-19.05C584.76,-20.21 618.59,-22.54 647.78,-27 650.46,-27.41 653.22,-27.89 655.99,-28.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"655.25,-31.84 665.75,-30.46 656.68,-24.99 655.25,-31.84\"/>\n<text text-anchor=\"middle\" x=\"620.28\" y=\"-30.8\" font-family=\"Times,serif\" font-size=\"14.00\">b2</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x109db1e70>"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.14085793e-01, 2.95571761e-01, 1.90342446e-01],\n       [4.25086348e-01, 2.66558492e-01, 3.08355161e-01],\n       [5.68681654e-01, 2.87285683e-01, 1.44032662e-01],\n       ...,\n       [9.39774768e-01, 5.76104164e-02, 2.61481553e-03],\n       [9.12083861e-01, 8.18366292e-02, 6.07951003e-03],\n       [8.64214297e-01, 1.34993035e-01, 7.92668045e-04]])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = softmax.apply_fwd()\n",
    "\n",
    "predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 7.0.6 (20230106.0513)\n -->\n<!-- Title: fwd_pass Pages: 1 -->\n<svg width=\"955pt\" height=\"191pt\"\n viewBox=\"0.00 0.00 954.67 191.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n<title>fwd_pass</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-187 950.67,-187 950.67,4 -4,4\"/>\n<!-- 5350225040 -->\n<g id=\"node1\" class=\"node\">\n<title>5350225040</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"40.95\" cy=\"-37\" rx=\"40.89\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"40.95\" y=\"-33.3\" font-family=\"Times,serif\" font-size=\"14.00\">Softmax</text>\n</g>\n<!-- 5350225088 -->\n<g id=\"node2\" class=\"node\">\n<title>5350225088</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"175.89\" cy=\"-37\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"175.89\" y=\"-33.3\" font-family=\"Times,serif\" font-size=\"14.00\">Add</text>\n</g>\n<!-- 5350225040&#45;&gt;5350225088 -->\n<g id=\"edge10\" class=\"edge\">\n<title>5350225040&#45;&gt;5350225088</title>\n<path fill=\"none\" stroke=\"red\" d=\"M82.2,-37C99.71,-37 120.11,-37 137.31,-37\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"137.11,-40.5 147.11,-37 137.11,-33.5 137.11,-40.5\"/>\n<text text-anchor=\"middle\" x=\"115.39\" y=\"-40.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(Z2)</text>\n</g>\n<!-- 5350225136 -->\n<g id=\"node3\" class=\"node\">\n<title>5350225136</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"349.19\" cy=\"-72\" rx=\"40.09\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"349.19\" y=\"-68.3\" font-family=\"Times,serif\" font-size=\"14.00\">MatMul</text>\n</g>\n<!-- 5350225088&#45;&gt;5350225136 -->\n<g id=\"edge8\" class=\"edge\">\n<title>5350225088&#45;&gt;5350225136</title>\n<path fill=\"none\" stroke=\"red\" d=\"M202.17,-42.16C228.14,-47.47 269.19,-55.86 301.16,-62.39\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"300.06,-65.74 310.56,-64.31 301.46,-58.88 300.06,-65.74\"/>\n<text text-anchor=\"middle\" x=\"255.89\" y=\"-62.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(Z1 @ W2)</text>\n</g>\n<!-- 5350225568 -->\n<g id=\"node11\" class=\"node\">\n<title>5350225568</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"349.19\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"349.19\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">b2</text>\n</g>\n<!-- 5350225088&#45;&gt;5350225568 -->\n<g id=\"edge9\" class=\"edge\">\n<title>5350225088&#45;&gt;5350225568</title>\n<path fill=\"none\" stroke=\"red\" d=\"M201.43,-30.72C207.74,-29.31 214.54,-27.94 220.89,-27 250.79,-22.57 284.96,-20.31 310.37,-19.16\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"310.37,-22.66 320.22,-18.75 310.08,-15.67 310.37,-22.66\"/>\n<text text-anchor=\"middle\" x=\"255.89\" y=\"-30.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(b2)</text>\n</g>\n<!-- 5350225472 -->\n<g id=\"node4\" class=\"node\">\n<title>5350225472</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"489.78\" cy=\"-103\" rx=\"27.1\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"489.78\" y=\"-99.3\" font-family=\"Times,serif\" font-size=\"14.00\">Relu</text>\n</g>\n<!-- 5350225136&#45;&gt;5350225472 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5350225136&#45;&gt;5350225472</title>\n<path fill=\"none\" stroke=\"red\" d=\"M385.52,-79.9C406,-84.48 431.73,-90.24 452.38,-94.86\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"451.57,-98.26 462.1,-97.03 453.1,-91.43 451.57,-98.26\"/>\n<text text-anchor=\"middle\" x=\"425.99\" y=\"-95.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(A1)</text>\n</g>\n<!-- 5350225520 -->\n<g id=\"node10\" class=\"node\">\n<title>5350225520</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"489.78\" cy=\"-49\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"489.78\" y=\"-45.3\" font-family=\"Times,serif\" font-size=\"14.00\">W2</text>\n</g>\n<!-- 5350225136&#45;&gt;5350225520 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5350225136&#45;&gt;5350225520</title>\n<path fill=\"none\" stroke=\"red\" d=\"M384.27,-62.91C391.87,-61.09 399.92,-59.34 407.49,-58 421.8,-55.47 437.66,-53.52 451.54,-52.1\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"451.54,-55.62 461.16,-51.17 450.87,-48.65 451.54,-55.62\"/>\n<text text-anchor=\"middle\" x=\"425.99\" y=\"-61.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(W2)</text>\n</g>\n<!-- 5350225424 -->\n<g id=\"node5\" class=\"node\">\n<title>5350225424</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"611.08\" cy=\"-103\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"611.08\" y=\"-99.3\" font-family=\"Times,serif\" font-size=\"14.00\">Add</text>\n</g>\n<!-- 5350225472&#45;&gt;5350225424 -->\n<g id=\"edge5\" class=\"edge\">\n<title>5350225472&#45;&gt;5350225424</title>\n<path fill=\"none\" stroke=\"red\" d=\"M517.26,-103C533.4,-103 554.29,-103 572.17,-103\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"572.17,-106.5 582.17,-103 572.17,-99.5 572.17,-106.5\"/>\n<text text-anchor=\"middle\" x=\"550.58\" y=\"-106.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(Z1)</text>\n</g>\n<!-- 4460323808 -->\n<g id=\"node6\" class=\"node\">\n<title>4460323808</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"779.38\" cy=\"-138\" rx=\"40.09\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"779.38\" y=\"-134.3\" font-family=\"Times,serif\" font-size=\"14.00\">MatMul</text>\n</g>\n<!-- 5350225424&#45;&gt;4460323808 -->\n<g id=\"edge3\" class=\"edge\">\n<title>5350225424&#45;&gt;4460323808</title>\n<path fill=\"none\" stroke=\"red\" d=\"M637.34,-108.32C662.15,-113.54 700.62,-121.64 731.12,-128.05\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"730.39,-131.48 740.9,-130.11 731.83,-124.63 730.39,-131.48\"/>\n<text text-anchor=\"middle\" x=\"688.58\" y=\"-128.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(X @ W1)</text>\n</g>\n<!-- 4460314736 -->\n<g id=\"node9\" class=\"node\">\n<title>4460314736</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"779.38\" cy=\"-84\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"779.38\" y=\"-80.3\" font-family=\"Times,serif\" font-size=\"14.00\">b1</text>\n</g>\n<!-- 5350225424&#45;&gt;4460314736 -->\n<g id=\"edge4\" class=\"edge\">\n<title>5350225424&#45;&gt;4460314736</title>\n<path fill=\"none\" stroke=\"red\" d=\"M636.62,-96.74C642.93,-95.32 649.73,-93.95 656.08,-93 684.31,-88.78 716.51,-86.51 740.78,-85.31\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"740.67,-88.82 750.5,-84.87 740.35,-81.82 740.67,-88.82\"/>\n<text text-anchor=\"middle\" x=\"688.58\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(b1)</text>\n</g>\n<!-- 4460316944 -->\n<g id=\"node7\" class=\"node\">\n<title>4460316944</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"919.67\" cy=\"-165\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"919.67\" y=\"-161.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n</g>\n<!-- 4460323808&#45;&gt;4460316944 -->\n<g id=\"edge1\" class=\"edge\">\n<title>4460323808&#45;&gt;4460316944</title>\n<path fill=\"none\" stroke=\"red\" d=\"M816.71,-145.09C836.92,-149.04 861.99,-153.93 882.21,-157.88\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"881.51,-161.31 892,-159.79 882.85,-154.44 881.51,-161.31\"/>\n</g>\n<!-- 4460323424 -->\n<g id=\"node8\" class=\"node\">\n<title>4460323424</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"919.67\" cy=\"-111\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"919.67\" y=\"-107.3\" font-family=\"Times,serif\" font-size=\"14.00\">W1</text>\n</g>\n<!-- 4460323808&#45;&gt;4460323424 -->\n<g id=\"edge2\" class=\"edge\">\n<title>4460323808&#45;&gt;4460323424</title>\n<path fill=\"none\" stroke=\"red\" d=\"M816.71,-130.91C836.92,-126.96 861.99,-122.07 882.21,-118.12\"/>\n<polygon fill=\"red\" stroke=\"red\" points=\"882.85,-121.56 892,-116.21 881.51,-114.69 882.85,-121.56\"/>\n<text text-anchor=\"middle\" x=\"856.17\" y=\"-129.8\" font-family=\"Times,serif\" font-size=\"14.00\">d(W1)</text>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x13ee5da50>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph(backward=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "softmax.zero_grad()\n",
    "loss_grad = nll_grad(train_y, predictions)\n",
    "softmax.apply_bwd(loss_grad)\n",
    "softmax.generate_derivative_chains()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1078.86623385,  1452.92238635, -2531.7886202 ],\n       [  685.34169939,  1438.06439063, -2123.40609002],\n       [   11.39607931,  3488.94754329, -3500.3436226 ],\n       [ 1990.1147882 ,   471.22083925, -2461.33562744],\n       [ -538.48614737,  1989.95610171, -1451.46995434],\n       [  823.86282115,  1728.75677685, -2552.619598  ],\n       [  876.8179049 ,  2091.18377317, -2968.00167807],\n       [ 1375.39946375,   693.7355912 , -2069.13505495],\n       [  705.656685  ,  2007.41199158, -2713.06867658],\n       [ 1247.97160254,  1539.37464607, -2787.34624861]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2_param.grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$\\frac{\\partial}{\\partial W2} = \\frac{\\partial softmax(Z2)}{\\partial Z2}*\\frac{\\partial Z2}{\\partial Z1 @ W2}*\\frac{\\partial Z1 @ W2}{\\partial W2}$"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_chain(w2_param.display_partial_derivative())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Latex object>",
      "text/latex": "$\\frac{\\partial}{\\partial W1} = \\frac{\\partial softmax(Z2)}{\\partial Z2}*\\frac{\\partial Z2}{\\partial Z1 @ W2}*\\frac{\\partial Z1 @ W2}{\\partial A1}*\\frac{\\partial A1}{\\partial Z1}*\\frac{\\partial Z1}{\\partial X @ W1}*\\frac{\\partial X @ W1}{\\partial W1}$"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_chain(w1_param.display_partial_derivative())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "l1 = train_x @ w1 + b1\n",
    "l1_activated = np.maximum(l1, 0)\n",
    "l2 = l1_activated @ w2 + b2\n",
    "probs = softmax_func(l2)\n",
    "\n",
    "loss_grad = nll_grad(train_y, probs)\n",
    "\n",
    "sm_grad = softmax_grad_func(probs, loss_grad)\n",
    "l2_w_grad = l1_activated.T @ sm_grad\n",
    "l2_b_grad = sm_grad.sum(axis=0)\n",
    "\n",
    "l1_grad = sm_grad @ w2.T\n",
    "l1_grad[l1 < 0] = 0\n",
    "\n",
    "l1_w_grad = train_x.T @ l1_grad\n",
    "l1_b_grad = l1_grad.sum(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(l1_w_grad, w1_param.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
