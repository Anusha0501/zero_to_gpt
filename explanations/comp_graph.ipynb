{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Backpropagation in depth\n",
    "\n",
    "In the [last lesson](https://github.com/VikParuchuri/zero_to_gpt/blob/master/explanations/rnn.ipynb), we learned how to create a recurrent neural network.  We now know how to build several network architectures using components like dense layers, softmax, and recurrent layers.\n",
    "\n",
    "We've been a bit loose with how we cover backpropagation, to make neural network architecture easier to understand.  In this lesson, we'll do a deep dive into how backpropagation works.  We'll do this by building a computational graph that keeps track of the different operations that transform the input data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Softmax function\n",
    "\n",
    "In a previous lesson, we introduced the softmax function.  This is used to convert the output of a neural network into probabilities that can be used to make predictions.  The softmax function is defined as:\n",
    "\n",
    "$$\\zeta=\\frac{e^{\\hat{y_{i}}}}{\\sum_{j=0}e^{\\hat{y_{j}}}}$$\n",
    "\n",
    "For each row of our neural network output, we raise $e$ to the power of our output value, then divide by the sum of $e$ raised to the power of each of the outputs for that row.\n",
    "\n",
    "The softmax function looks like this in code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0.09003057, 0.24472847, 0.66524096]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(preds):\n",
    "    # Subtract the max of each row from each row element to stabilize the softmax\n",
    "    # If we don't do this, we could raise e to very large or small values, and cause numerical overflow or underflow\n",
    "    normalized = preds - np.max(preds, axis=-1).reshape(-1,1)\n",
    "    raised = np.exp(normalized)\n",
    "    output = raised / np.sum(raised, axis=1).reshape(-1,1)\n",
    "    return output\n",
    "\n",
    "# Demonstrate how the softmax works\n",
    "input_row = np.arange(0,3).reshape(1,-1)\n",
    "print(input_row)\n",
    "softmax(input_row)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We didn't do this previously in the softmax function, but in the above code, we subtract the maximum from each element in the row.  This prevents numerical underflow or overflow.  Each [numeric type](https://numpy.org/doc/stable/user/basics.types.html) (float, integer, etc) can only hold a certain number of digits.  For example, floating point 16 can store 5 exponent bits, and ten digit bits (each bit is only base 2, so this is less than the same number of base-10 digits).  The maximum value we can store in `float16` is `65500`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "65500.0"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the maximum value we can assign to float16\n",
    "np.finfo('float16').max"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/9z84c__j28g8tg28bmcthjj00000gn/T/ipykernel_82094/2542651828.py:3: RuntimeWarning: overflow encountered in cast\n",
      "  a[0] = 6.55e5\n"
     ]
    }
   ],
   "source": [
    "# This is an example of numeric overflow, where we store more digits than float16 can hold\n",
    "a = np.array([0], dtype=np.float16)\n",
    "a[0] = 6.55e5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we raise $e$ to a very large or small number, we can generate a number that is too large to store in our specific data type.  Subtracting the max gives us the same end result, but reduces the risk of overflow.  Feel free to try the softmax out with and without subtracting the max to see how it works!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Softmax derivative\n",
    "\n",
    "Instead of computing the softmax derivative, we previously used the fact that the derivative of the softmax and negative log likelihood functions \"cancel out\", and end up with a derivative of $p-y$.  But what if we want to find the derivative ourselves?  We can approach it analytically, and find the derivative of the entire function.\n",
    "\n",
    "But an easier method is to break the softmax function apart into individual operations.  Each operation will make a single modification to the data:\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To start, let's read in some data and define a 2-layer neural network that can make predictions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in our data, and fill missing values\n",
    "data = pd.read_csv(\"../../data/clean_weather.csv\", index_col=0)\n",
    "data = data.ffill()\n",
    "\n",
    "# Create data sets of our predictors and targets (x and y)\n",
    "x = data[:10][[\"tmax\", \"tmin\", \"rain\"]].to_numpy()\n",
    "y = data[:10][[\"tmax_tomorrow\"]].to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once we have the data, we'll initialize our parameters for 2 layers.  To keep things simple, we'll omit the bias, so we just need weights for each layer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "w1 = np.random.rand(3, 3)\n",
    "w2 = np.random.rand(3,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
