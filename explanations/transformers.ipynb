{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  en  \\\n0  In the society of his nephew and niece, and th...   \n1  By a former marriage, Mr. Henry Dashwood had o...   \n2  By his own marriage, likewise, which happened ...   \n3  But the fortune, which had been so tardy in co...   \n4  But Mrs. John Dashwood was a strong caricature...   \n\n                                                  es  \n0  En compañía de su sobrino y sobrina, y de los ...  \n1  De un matrimonio anterior, el señor Henry Dash...  \n2  Además, su propio matrimonio, ocurrido poco de...  \n3  Pero la fortuna, que había tardado tanto en ll...  \n4  Pero la señora de John Dashwood era una áspera...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>es</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>In the society of his nephew and niece, and th...</td>\n      <td>En compañía de su sobrino y sobrina, y de los ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>By a former marriage, Mr. Henry Dashwood had o...</td>\n      <td>De un matrimonio anterior, el señor Henry Dash...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>By his own marriage, likewise, which happened ...</td>\n      <td>Además, su propio matrimonio, ocurrido poco de...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>But the fortune, which had been so tardy in co...</td>\n      <td>Pero la fortuna, que había tardado tanto en ll...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>But Mrs. John Dashwood was a strong caricature...</td>\n      <td>Pero la señora de John Dashwood era una áspera...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "opus = pd.read_csv(\"../data/opus_books.csv\")\n",
    "opus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "special_tokens = {\n",
    "    \"PAD\": 0,\n",
    "    \"UNK\": 1,\n",
    "    \"BOS\": 2,\n",
    "    \"EOS\": 3\n",
    "}\n",
    "vocab = special_tokens.copy()\n",
    "\n",
    "def clean(text):\n",
    "    # Use re to replace punctuation that is not a comma, question mark, or period with spaces\n",
    "    text = re.sub(r'[^\\w\\s,?.!]',' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    # Split on consecutive whitespace and punctuation\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]+|[\\s]+', text)\n",
    "    return tokens\n",
    "\n",
    "opus_tokens = defaultdict(int)\n",
    "for index, row in opus.iterrows():\n",
    "    for key in [\"en\", \"es\"]:\n",
    "        cleaned = clean(row[key])\n",
    "        tokens = tokenize(cleaned)\n",
    "        for token in tokens:\n",
    "            opus_tokens[token] += 1\n",
    "\n",
    "counter = 4\n",
    "for index, token in enumerate(opus_tokens):\n",
    "    # Filter out uncommon tokens\n",
    "    # Add unknown token for rare words\n",
    "    if opus_tokens[token] > 3:\n",
    "        vocab[token] = counter\n",
    "        counter += 1\n",
    "    else:\n",
    "        vocab[token] = 1 # Assign unknown id"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def encode(text, vocab):\n",
    "    # Encode text as a list of integers\n",
    "    tokens = tokenize(clean(text))\n",
    "    encoded = np.array([vocab[token] for token in tokens])\n",
    "    return encoded\n",
    "\n",
    "reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "for k,v in special_tokens.items():\n",
    "    reverse_vocab[v] = k\n",
    "\n",
    "def decode(encoded, reverse_vocab):\n",
    "    # Decode a list of integers into text\n",
    "    decoded = \"\".join([reverse_vocab[token] for token in encoded])\n",
    "    return decoded"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "data = []\n",
    "for index, row in opus.iterrows():\n",
    "    # Encode the English and Spanish sentences\n",
    "    en_text = row[\"en\"]\n",
    "    es_text = row[\"es\"]\n",
    "    en = encode(en_text, vocab)\n",
    "    es = encode(es_text, vocab)\n",
    "\n",
    "    # Generate our prediction target\n",
    "    target = np.roll(es, -1)\n",
    "    target[-1] = 3 # EOS\n",
    "    data.append({\"en\": en, \"es\": es, \"en_text\": en_text, \"es_text\": es_text, \"target\": target})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'en': array([1415,    5, 4384,    5,  853,    5,   87,    5,    6,    5, 4385,\n           5,    8,    5,  508,    5,    1,    5,   46,   12,    5,   10,\n           5,    8,    5,  240,    5,    9,    5,   50,    5,    1,   21]),\n 'es': array([4386,    5, 4387,    5,  102,    5,   35,    5, 4388,    5,   24,\n           5,  715,    5,   28,    5, 4389,    5, 4390,    5,   96,    5,\n         233,    5, 4391,    5,   26,    5,   24,    5,  155,    5,   25,\n           5,   62,    5, 4392,   21]),\n 'en_text': 'That crime has been the origin of every lesser one, and of all his present discontents.\"',\n 'es_text': 'Ese crimen fue el origen de todos los males menores que le siguieron y de todo su actual descontento.',\n 'target': array([   5, 4387,    5,  102,    5,   35,    5, 4388,    5,   24,    5,\n         715,    5,   28,    5, 4389,    5, 4390,    5,   96,    5,  233,\n           5, 4391,    5,   26,    5,   24,    5,  155,    5,   25,    5,\n          62,    5, 4392,   21,    3])}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import math\n",
    "class Embedding():\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        k = 1/math.sqrt(embed_dim)\n",
    "        self.weights = np.random.rand(vocab_size, embed_dim) * 2 * k - k\n",
    "        self.weights[0] = 0 # Zero out the padding embedding\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        # Cache for backward pass\n",
    "        self.token_ids = token_ids\n",
    "        # Return a matrix of embeddings\n",
    "        # We could convert token_ids to a one_hot vector and multiply by the weights, but it is the same as selecting a single row of the matrix\n",
    "        return self.weights[token_ids]\n",
    "\n",
    "    def backward(self, grad, lr):\n",
    "        for i, token_id in enumerate(self.token_ids):\n",
    "            # Add the gradient to the embedding\n",
    "            # We could convert the input to one-hot, and do input.T * grad, but it is the same as adding the gradient to the embedding\n",
    "            # Subtracting the gradient is SGD optimization\n",
    "            self.weights[token_id] -= grad[i] * lr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.02405603,  0.04253936, -0.01771518, ...,  0.04196606,\n         0.01059922,  0.00290236],\n       [ 0.00526091, -0.03544216,  0.0096714 , ...,  0.0370609 ,\n         0.04180169, -0.03076386],\n       [-0.03354411, -0.02107315, -0.03113892, ...,  0.01260971,\n        -0.04156332, -0.01547263],\n       ...,\n       [ 0.00526091, -0.03544216,  0.0096714 , ...,  0.0370609 ,\n         0.04180169, -0.03076386],\n       [-0.03811562,  0.03132126, -0.01054451, ..., -0.00798627,\n        -0.03331769,  0.0065835 ],\n       [ 0.01550159,  0.00783828,  0.01947204, ...,  0.0398476 ,\n        -0.0129053 , -0.03832633]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embed = Embedding(len(set(vocab.values())), 512)\n",
    "input_embed.forward(data[0][\"en\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.        ,  0.04253936,  0.        , ...,  0.04196606,\n         0.01059922,  0.00290236],\n       [ 0.00526091,  0.        ,  0.        , ...,  0.        ,\n         0.04180169,  0.        ],\n       [ 0.        ,  0.        ,  0.        , ...,  0.01260971,\n         0.        ,  0.        ],\n       ...,\n       [ 0.00526091,  0.        ,  0.0096714 , ...,  0.0370609 ,\n         0.04180169, -0.03076386],\n       [-0.03811562,  0.        , -0.01054451, ...,  0.        ,\n         0.        ,  0.0065835 ],\n       [ 0.        ,  0.00783828,  0.01947204, ...,  0.        ,\n        -0.0129053 ,  0.        ]])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.92855015,  1.66255411, -0.73025494, ...,  1.63978755,\n         0.39415737,  0.08850216],\n       [ 0.23480195, -1.33519437,  0.40492317, ...,  1.46138918,\n         1.64425061, -1.15474332],\n       [-1.32633023, -0.83376943, -1.23133335, ...,  0.49658966,\n        -1.64306226, -0.61256811],\n       ...,\n       [ 0.23480195, -1.33519437,  0.40492317, ...,  1.46138918,\n         1.64425061, -1.15474332],\n       [-1.49511875,  1.23420366, -0.41139434, ..., -0.31083881,\n        -1.30652879,  0.26184852],\n       [ 0.6034468 ,  0.31321756,  0.75381792, ...,  1.52549273,\n        -0.47239533, -1.4351553 ]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
