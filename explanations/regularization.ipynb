{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization\n",
    "\n",
    "You may be surprised to learn that the goal of training a neural network is not to minimize the training loss.  This might seem counterintuitive.  We've been training models using gradient descent, where training loss decreases over time.\n",
    "\n",
    "However, the training data is just a sample of the population data.  For example,we have data on house prices, where each row in this dataset represents a single house.  Here are the columns:\n",
    "\n",
    "- `interest`: The interest rate\n",
    "- `vacancy`: The vacancy rate\n",
    "- `cpi`: The consumer price index\n",
    "- `price`: The price of a house\n",
    "- `value`: The value of a house\n",
    "- `adj_price`: The price of a house, adjusted for inflation\n",
    "- `adj_value`: The value of a house, adjusted for inflation\n",
    "\n",
    "We have about `700` different rows of data - this is called our sample.  There are many more houses that aren't in our training data - this is called the population.  What we actually want to do is train the model on our sample of data, but use it to make predictions on the population.\n",
    "\n",
    "If our model makes good predictions in our sample, but bad predictions in the population, then that's called overfitting.  Overfitting happens when the model learns random characteristics of our sample instead of the underlying characteristics of our population. For example, if the vacancy rate is always high before house prices drop in the sample, but not in the population, then this is a random quirk of our sample.\n",
    "\n",
    "Overfitting means that the model won't be useful in the real world.  Neural networks are prone to overfitting, and there are many techniques that have been developed to prevent it.  These techniques are broadly called regularization, which adds constraints or penalties to the model's parameters, in order to encourage it to learn simpler and more generalizable representations.  These will usually increase loss in the sample, but decrease loss in the population.\n",
    "\n",
    "Of course, we don't have access to data from the population.  This means that we need to split our sample up into a training set, a validation set, and a test set.  While we're training the model, we'll evaluate on the validation set.  After we've optimized our training method and parameters, we'll evaluate on the test set.  This ensures that we're not using knowledge of the test set when we tune our parameters (this could cause overfitting to the test set).\n",
    "\n",
    "We'll learn three forms of regularization:\n",
    "\n",
    "- Weight decay, which decreases the magnitude of the weights in the optimizer.  This pushes most of the weights towards zero, which encourages the model to learn simpler representations.\n",
    "- Dropout, which randomly sets activations to zero.  This prevents the model from relying on any single activation, and encourages it to learn more generalizable representations.\n",
    "- Early stopping, which stops training when the validation loss starts to increase.  This prevents overfitting by stopping training before the model starts to memorize the training data.\n",
    "\n",
    "There are other forms of regularization, like data augmentation, but these are the most common when working with text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll first load in the data, which is the same from the last lesson:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "sys.path.append(os.path.abspath('../nnets'))\n",
    "from dense import DenseManualUpdate as Dense, forward, backward\n",
    "from csv_data import HousePricesDatasetWrapper\n",
    "import numpy as np\n",
    "from optimizer import Optimizer\n",
    "\n",
    "wrapper = HousePricesDatasetWrapper()\n",
    "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up a training run\n",
    "\n",
    "As you can see, we split the data into three sets.  We'll use the training set to train the model, the validation set to evaluate the model, and the test set to evaluate the model after we've finished training.\n",
    "\n",
    "We'll write a training loop function, which will allow us to test different types of regularization.  The function:\n",
    "\n",
    "- Sets up a new W&B run for monitoring\n",
    "- Loops through the training data with batch size 1:\n",
    "    - Makes a prediction\n",
    "    - Finds the error\n",
    "    - Computes the gradient\n",
    "    - Updates the parameters\n",
    "- Logs the loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def training_run(epochs, regularization, layers, optimizer, train_data, valid_data, name=None):\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init(project=\"regularization\",\n",
    "               name=name,\n",
    "               config={\"regularization\": regularization})\n",
    "\n",
    "    # Split the training and valid data into x and y\n",
    "    train_x, train_y = train_data\n",
    "    valid_x, valid_y = valid_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i in range(len(train_x)):\n",
    "            # Get the x and y batches\n",
    "            x_batch = train_x[i:(i+1)]\n",
    "            y_batch = train_y[i:(i+1)]\n",
    "            # Make a prediction\n",
    "            pred = forward(x_batch, layers, training=True)\n",
    "\n",
    "            # Run the backward pass\n",
    "            loss = pred - y_batch\n",
    "            layer_grads = backward(loss, layers)\n",
    "            running_loss += np.mean(loss ** 2)\n",
    "\n",
    "            # Run the optimizer\n",
    "            optimizer(layer_grads, layers, 1)\n",
    "\n",
    "        # Calculate and log validation loss\n",
    "        valid_preds = forward(valid_x, layers, training=False)\n",
    "        valid_loss = np.mean((valid_preds - valid_y) ** 2)\n",
    "        train_loss = running_loss / len(train_x)\n",
    "        wandb.log({\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "        })\n",
    "\n",
    "    # Mark the run as complete\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll also write a function to calculate test set loss.  This will help us evaluate different regularization models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def test_loss(layers, test_data):\n",
    "    test_x, test_y = test_data\n",
    "    preds = forward(test_x, layers, training=False)\n",
    "    loss = np.mean((preds - test_y) ** 2)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weight decay\n",
    "\n",
    "Weight decay is similar to L2 regularization.  The goal is to shrink the weights towards 0 (reduce the L2 norm of the weights).  This means that the model will be less likely to learn very large weights.  Very large weights can cause overfitting, because it means that the model is putting too much importance on a single value.\n",
    "\n",
    "The difference between L2 regularization and weight decay is small, and has to do with where the weights are shrunk.  In SGD, they are the same, but this is not the case in other optimizers like SGD with momentum.  You can read more about the difference [here](https://arxiv.org/pdf/1711.05101.pdf).\n",
    "\n",
    "As an example, let's look at the weights of a model with and without weight decay.  First, we'll define our SGDW optimizer.  This is SGD, but with weight decay.  The main addition is the line `w_update -= self.decay * layer.weights`.  This multiplies a decay coefficient (usually a number like `.01`) by the weights, then adds it to the weight update.  This will shrink the weights over time.\n",
    "\n",
    "For example, if the decay coefficient is `.1`, and we ignore the gradient, then after the first update, the weights will be `90%` of their original magnitude, `81%` after the second update, and so on.  This means that the shrinkage will be proportional to the size of the weight.  So large weights will be reduced more in absolute terms.\n",
    "\n",
    "Now, we can define the optimizer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "class SGDW(Optimizer):\n",
    "    def __init__(self, lr, decay):\n",
    "        # Store the learning rate and decay coefficient\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, layer_grads, layers, batch_size):\n",
    "        # Loop through the layer grads.  Reverse the layers to match the grads (from output backward to input).\n",
    "        for layer_grad, layer in zip(layer_grads, reversed(layers)):\n",
    "            if layer_grad is None:\n",
    "                # Account for dropout layers\n",
    "                continue\n",
    "            w_grad, b_grad = layer_grad\n",
    "\n",
    "            # Calculate the gradient update size\n",
    "            w_update = -self.lr * w_grad\n",
    "            # Calculate weight decay\n",
    "            w_update -= self.decay * layer.weights\n",
    "\n",
    "            # We don't usually decay the bias\n",
    "            b_update = -self.lr * b_grad\n",
    "\n",
    "            # Actually do the update\n",
    "            layer.update(w_update, b_update)\n",
    "\n",
    "        self.save_vector(layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can look at an example without any weight decay.  If we set the decay coefficient to `0`, it's equivalent to plain SGD.  We'll define a two-layer neural network, then run our training loop.  We'll look at our final weights to check the distribution:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Two-layer neural network\n",
    "layers_sgd = [\n",
    "    Dense(7, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-3, 0)\n",
    "# Normal SGD\n",
    "training_run(10, \"None\", layers_sgd, sgd, train_data, valid_data, name=\"sgd\")\n",
    "\n",
    "# Plot the final weights\n",
    "sgd.plot_final_weights()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see above, there are two larger weight values.\n",
    "\n",
    "We can now do the same thing with weight decay.  We'll use a decay coefficient of `1e-3`.  You'll need to experiment with the learning rate and the decay coefficient.  Too large of a decay coefficient relative to the learning rate can prevent the model from learning.  Too small of a decay coefficient won't penalize the large weights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016751176383695564, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6b33ac7abf042a583e2004ae9db04a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_sgdw = [\n",
    "    Dense(7, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-3, 5e-4)\n",
    "# Weight decay\n",
    "training_run(10, \"Weight Decay\", layers_sgdw, sgd, train_data, valid_data, name=\"sgdw\")\n",
    "sgd.plot_final_weights()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, our weight values are closer together, and the largest values are now smaller.  This can help the network generalize better to new data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 28.325657408857865\n",
      "SGDW: 107.32555507049128\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGD: {test_loss(layers_sgd, test_data)}\")\n",
    "print(f\"SGDW: {test_loss(layers_sgdw, test_data)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, our test set loss is higher with weight decay than without it.  Weight decay won't always reduce test set loss - it depends on how similar the training and test set are, as well as other characteristics of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dropout\n",
    "\n",
    "Dropout is a regularization technique that randomly sets some activations in the network to zero.  We add in dropout right after the activation function:\n",
    "\n",
    "![](images/regularization/network_dropout.svg)\n",
    "\n",
    "A dropout layer will randomly set some inputs to zero.  The layer takes one parameter, which is the probability that any single value will be set to `0`.  This is usually called `p` or `drop_p`.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "![](images/regularization/dropout.svg)\n",
    "\n",
    "In this example, the dropout layer sets a single activation to 0, based on the probability of `.5`.\n",
    "\n",
    "You might be wondering what the point of this is - it seems like it just throws away valuable inputs.  Dropout forces the network to make predictions with a subset of the full network.  This means that the network is more robust to noisy input or other issues. It can even be thought of as a form of ensembling, where multiple smaller models are combined into a larger, more powerful one.\n",
    "\n",
    "Here's another look at the same operations from before:\n",
    "\n",
    "![](images/regularization/ensemble_network.svg)\n",
    "\n",
    "As you can see, in the example with dropout, predictions are made by a subset of the network.  If we repeatedly apply dropout, we force the network to become more resilient, and minimize overfitting.\n",
    "\n",
    "Just as important as when we apply dropout is when we don't apply it.  We don't apply dropout after the final output layer. This will result in many of our predictions being zero, which isn't what we want.  We also don't use dropout directly on the inputs. We basically only want to use dropout inside the network, to force the network to use a subset to make the predictions.\n",
    "\n",
    "We also only use dropout in training, not in inference.  In inference, using dropout will result in lower accuracy without any gains (the gain is in training the network to be more resilient)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can define a dropout layer that we can then use in our network.  The layer will be relatively straightforward:\n",
    "\n",
    "- We'll set a `drop_p`, which is the probability that we'll drop out (set to 0) any single value.\n",
    "- In the forward pass, we'll randomly set some values to 0.\n",
    "- In the backward pass, we'll set the gradient to zero where we applied dropout.  Similarly to Relu, when we zero out a value, it doesn't contribute to the prediction (and loss), so we don't want to use it in the backward pass."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, drop_p):\n",
    "        # Probability that we'll drop out any single input\n",
    "        self.drop_p = drop_p\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            # Generate a mask of 0s and 1s, using the drop probability\n",
    "            self.mask = np.random.binomial(1, 1-self.drop_p, input.shape)\n",
    "        else:\n",
    "            # No dropout in inference\n",
    "            self.mask = np.ones_like(input)\n",
    "        # Apply the mask.  If the mask is 0, the input is set to 0\n",
    "        return np.where(self.mask, input, 0)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Use np.where to zero out the gradient where we did dropout\n",
    "        return None, np.where(self.mask, grad, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_dropout = [\n",
    "    Dense(7, 10),\n",
    "    Dropout(.2),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-3, 5e-4)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Weight Decay + Dropout\", layers_dropout, sgd, train_data, valid_data, name=\"dropout\")\n",
    "sgd.plot_final_weights()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDW: 107.32555507049128\n",
      "SGDW + Dropout: 13.244365053084687\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGDW: {test_loss(layers_sgdw, test_data)}\")\n",
    "print(f\"SGDW + Dropout: {test_loss(layers_dropout, test_data)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, combining SGDW and dropout worked effectively to prevent overfitting.  I would encourage you to play around with the drop probability, and look at the loss in the W&B dashboard."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Early Stopping\n",
    "\n",
    "Early stopping can prevent overfitting by stopping training when the validation loss is plateauing or increasing.\n",
    "\n",
    "It's common to save checkpoints regularly and then choose the best one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dropout(.05),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(4, \"Early Stopping\", layers, sgd, train_data, valid_data, name=\"early_stopping\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training convergence\n",
    "\n",
    "Not strictly regularization, but can help with overfitting.  Also help the model converge better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Improve generalization and convergence\n",
    "# Not strictly regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layernorm\n",
    "\n",
    "Normalize the values in a layer.  Stabilize training.\n",
    "\n",
    "Next, compare this with PyTorch's layernorm.  You can do that using something like this - https://discuss.pytorch.org/t/how-to-call-the-backward-function-of-a-custom-module/7853/2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class LayerNorm():\n",
    "    def __init__(self, embed_dim, eps):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Cache for backward pass\n",
    "        self.input = input\n",
    "        # Calculate the mean and standard deviation\n",
    "        self.mean = np.sum(input, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.normed = (input - self.mean)\n",
    "        variance = np.sum(self.normed**2, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.std = np.sqrt(variance + self.eps)\n",
    "        # Normalize the input\n",
    "        return self.normed / self.std\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Find the derivative of numerator (normed)\n",
    "        grad_normed_1 = grad * 1 / self.std\n",
    "\n",
    "        # Derivative of denominator (std)\n",
    "        grad_std = grad * self.normed\n",
    "        # std is a single number\n",
    "        grad_std = np.sum(grad_std, axis=1, keepdims=True)\n",
    "        # Derivative of 1 / std\n",
    "        grad_std = grad_std * -1 / (self.std**2)\n",
    "\n",
    "        # Find gradient against the variance\n",
    "        grad_variance = grad_std * .5 * 1 / self.std\n",
    "\n",
    "        # Find gradient against normed\n",
    "        grad_normed_2 = grad_variance * 1 / self.embed_dim\n",
    "        grad_normed_2 = np.ones_like(self.normed, dtype=self.input.dtype) * grad_normed_2\n",
    "        grad_normed_2 = grad_normed_2 * 2 * self.normed\n",
    "\n",
    "        # Combine two gradients against normed\n",
    "        grad_normed = grad_normed_1 + grad_normed_2\n",
    "\n",
    "        # Find gradient against mean\n",
    "        grad_mean = grad_normed * -1\n",
    "        grad_mean = np.sum(grad_mean, axis=1, keepdims=True)\n",
    "\n",
    "        # Find gradient against input\n",
    "        grad_input_1 = grad_normed\n",
    "        grad_input_2 =  grad_mean * 1 / self.embed_dim\n",
    "        grad_input_2 = grad_input_2 * np.ones_like(self.input, dtype=self.input.dtype)\n",
    "\n",
    "        # Combine two gradients against input\n",
    "        grad_input = grad_input_1 + grad_input_2\n",
    "        return None, grad_input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    LayerNorm(25, 1e-6),\n",
    "    Dense(25, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(5e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Layer Norm\", layers, sgd, train_data, valid_data, name=\"layer_norm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(512)\n",
    "layer_norm.forward(input_embed.forward(data[0][\"en\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Residual connections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
