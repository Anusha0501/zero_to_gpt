{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization\n",
    "\n",
    "You may be surprised to learn that the goal of training a neural network is not to minimize the training loss.  This is counterintuitive - of course, we've been training models using gradient descent, where loss decreases over time.\n",
    "\n",
    "However, the training data is just a sample of the population data.  For example, let's say that we have data on house prices, where each row in this dataset represents a single house.  The other columns are:\n",
    "\n",
    "- `interest`: The interest rate\n",
    "- `vacancy`: The vacancy rate\n",
    "- `cpi`: The consumer price index\n",
    "- `price`: The price of a house\n",
    "- `value`: The value of a house\n",
    "- `adj_price`: The price of a house, adjusted for inflation\n",
    "- `adj_value`: The value of a house, adjusted for inflation\n",
    "\n",
    "We have about `700` different rows of data - this is called our sample.  There are many more houses that aren't in our training data - this is called the population.  What we actually want to do is train the model on our sample of data, but use it to make predictions on the population.\n",
    "\n",
    "If our model makes good predictions in our sample, but bad predictions in the population, then that's called overfitting.  Overfitting means that the model won't be useful in the real world.  Neural networks are prone to overfitting, and there are many techniques that have been developed to prevent this.\n",
    "\n",
    "These techniques are broadly called regularization, which adds constraints or penalties to the model's parameters, in order to encourage it to learn simpler and more generalizable representations.  These will usually increase loss in the sample, but decrease loss in the population.\n",
    "\n",
    "Of course, we don't have access to data from the population.  This means that we need to split our sample up into a training set, a validation set, and a test set.  While we're training the model, we'll evaluate on the validation set.  After we've optimized our training method and parameters, we'll evaluate on the test set.  This ensures that we're not using knowledge of the test set when we tune our parameters (this could cause overfitting to the test set).\n",
    "\n",
    "We'll learn three forms of regularization:\n",
    "\n",
    "- Weight decay, which decreases the magnitude of the weights in the optimizer.  This pushes most of the weights towards zero, which encourages the model to learn simpler representations.\n",
    "- Dropout, which randomly sets activations to zero.  This prevents the model from relying on any single activation, and encourages it to learn more generalizable representations.\n",
    "- Early stopping, which stops training when the validation loss starts to increase.  This prevents overfitting by stopping training before the model starts to memorize the training data.\n",
    "\n",
    "There are other forms of regularization, like data augmentation, but these are the most common when working with text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll first load in the data, which is the same from the last lesson:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "sys.path.append(os.path.abspath('../nnets'))\n",
    "from dense import DenseManualUpdate as Dense, forward, backward\n",
    "from csv_data import HousePricesDatasetWrapper\n",
    "import numpy as np\n",
    "from optimizer import Optimizer\n",
    "\n",
    "wrapper = HousePricesDatasetWrapper()\n",
    "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up a training run\n",
    "\n",
    "As you can see, we split the data into three sets.  We'll use the training set to train the model, the validation set to evaluate the model, and the test set to evaluate the model after we've finished training.\n",
    "\n",
    "We'll write a training loop function, which will allow us to test different types of regularization.  The function:\n",
    "\n",
    "- Sets up a new W&B run for monitoring\n",
    "- Loops through the training data with batch size 1:\n",
    "    - Makes a prediction\n",
    "    - Finds the error\n",
    "    - Computes the gradient\n",
    "    - Updates the parameters\n",
    "- Logs the loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def training_run(epochs, regularization, layers, optimizer, train_data, valid_data, name=None):\n",
    "    # Initialize a new W&B run\n",
    "    wandb.init(project=\"regularization\",\n",
    "               name=name,\n",
    "               config={\"regularization\": regularization})\n",
    "\n",
    "    # Split the training and valid data into x and y\n",
    "    train_x, train_y = train_data\n",
    "    valid_x, valid_y = valid_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i in range(len(train_x)):\n",
    "            # Get the x and y batches\n",
    "            x_batch = train_x[i:(i+1)]\n",
    "            y_batch = train_y[i:(i+1)]\n",
    "            # Make a prediction\n",
    "            pred = forward(x_batch, layers, training=True)\n",
    "\n",
    "            # Run the backward pass\n",
    "            loss = pred - y_batch\n",
    "            layer_grads = backward(loss, layers)\n",
    "            running_loss += np.mean(loss ** 2)\n",
    "\n",
    "            # Run the optimizer\n",
    "            optimizer(layer_grads, layers, 1)\n",
    "\n",
    "        # Calculate and log validation loss\n",
    "        valid_preds = forward(valid_x, layers, training=False)\n",
    "        valid_loss = np.mean((valid_preds - valid_y) ** 2)\n",
    "        train_loss = running_loss / len(train_x)\n",
    "        wandb.log({\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "        })\n",
    "\n",
    "    # Mark the run as complete\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll also write a function to calculate test set loss.  This will help us evaluate different regularization models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def test_loss(layers, test_data):\n",
    "    test_x, test_y = test_data\n",
    "    preds = forward(test_x, layers, training=False)\n",
    "    loss = np.mean((preds - test_y) ** 2)\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weight decay\n",
    "\n",
    "Weight decay is similar to L2 regularization.  The goal is to shrink the weights towards 0 (reduce the L2 norm of the weights).  This means that the model will be less likely to learn very large weights.  Very large weights can cause overfitting, because it means that the model is putting too much importance on a single value.\n",
    "\n",
    "The difference between L2 regularization and weight decay is small, and has to do with where the weights are shrunk.  In SGD, they are the same, but this is not the case in other optimizers like SGD with momentum.  You can read more about the difference [here](https://arxiv.org/pdf/1711.05101.pdf).\n",
    "\n",
    "As an example, let's look at the weights of a model with and without weight decay.  First, we'll define our SGDW optimizer.  This is SGD, but with weight decay.  The main addition is the line `w_update -= self.decay * layer.weights`.  This multiplies a decay coefficient (usually a number like `.01`) by the weights, then adds it to the weight update.  This will shrink the weights over time.\n",
    "\n",
    "For example, if the decay coefficient is `.1`, and we ignore the gradient, then after the first update, the weights will be `90%` of their original magnitude, `81%` after the second update, and so on.  This means that the shrinkage will be proportional to the size of the weight.  So large weights will be reduced more in absolute terms.\n",
    "\n",
    "Now, we can define the optimizer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SGDW(Optimizer):\n",
    "    def __init__(self, lr, decay):\n",
    "        # Store the learning rate and decay coefficient\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, layer_grads, layers, batch_size):\n",
    "        # Loop through the layer grads.  Reverse the layers to match the grads (from output backward to input).\n",
    "        for layer_grad, layer in zip(layer_grads, reversed(layers)):\n",
    "            if layer_grad is None:\n",
    "                # Account for dropout layers\n",
    "                continue\n",
    "            w_grad, b_grad = layer_grad\n",
    "\n",
    "            # Calculate the gradient update size\n",
    "            w_update = -self.lr * w_grad\n",
    "            # Calculate weight decay\n",
    "            w_update -= self.decay * layer.weights\n",
    "\n",
    "            # We don't usually decay the bias\n",
    "            b_update = -self.lr * b_grad\n",
    "\n",
    "            # Actually do the update\n",
    "            layer.update(w_update, b_update)\n",
    "\n",
    "        self.save_vector(layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can look at an example without any weight decay.  If we set the decay coefficient to `0`, it's equivalent to plain SGD.  We'll define a two-layer neural network, then run our training loop.  We'll look at our final weights to check the distribution:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Two-layer neural network\n",
    "layers_sgd = [\n",
    "    Dense(7, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-3, 0)\n",
    "# Normal SGD\n",
    "training_run(10, \"None\", layers_sgd, sgd, train_data, valid_data, name=\"sgd\")\n",
    "\n",
    "# Plot the final weights\n",
    "sgd.plot_final_weights()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see above, there are two larger weight values.\n",
    "\n",
    "We can now do the same thing with weight decay.  We'll use a decay coefficient of `1e-3`.  You'll need to experiment with the learning rate and the decay coefficient.  Too large of a decay coefficient relative to the learning rate can prevent the model from learning.  Too small of a decay coefficient won't penalize the large weights."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers_sgdw = [\n",
    "    Dense(7, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-3, 5e-4)\n",
    "# Weight decay\n",
    "training_run(10, \"Weight Decay\", layers_sgdw, sgd, train_data, valid_data, name=\"sgdw\")\n",
    "sgd.plot_final_weights()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, our weight values are closer together.  This can help make the network more generalizable to new data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD: 28.325657408857865\n",
      "SGDW: 107.32555507049128\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGD: {test_loss(layers_sgd, test_data)}\")\n",
    "print(f\"SGDW: {test_loss(layers_sgdw, test_data)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, our test set loss is higher with weight decay than without it.  Weight decay won't always reduce test set loss - it depends on how similar the training and test set are, as well as other characteristics of the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dropout\n",
    "\n",
    "Dropout prevents overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, drop_p):\n",
    "        self.drop_p = drop_p\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            # Generate a mask of 0s and 1s\n",
    "            self.mask = np.random.binomial(1, 1-self.drop_p, input.shape)\n",
    "        else:\n",
    "            # No dropout in inference\n",
    "            self.mask = np.ones_like(input)\n",
    "        # Apply the mask.  If the mask is 0, the input is set to 0\n",
    "        return np.where(self.mask, input, 0)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Use np.where to apply the mask\n",
    "        return None, np.where(self.mask, grad, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dropout(.02),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Weight Decay + Dropout\", layers, sgd, train_data, valid_data, name=\"dropout\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Early Stopping\n",
    "\n",
    "Early stopping can prevent overfitting by stopping training when the validation loss is plateauing or increasing.\n",
    "\n",
    "It's common to save checkpoints regularly and then choose the best one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dropout(.05),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(4, \"Early Stopping\", layers, sgd, train_data, valid_data, name=\"early_stopping\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training convergence\n",
    "\n",
    "Not strictly regularization, but can help with overfitting.  Also help the model converge better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Improve generalization and convergence\n",
    "# Not strictly regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layernorm\n",
    "\n",
    "Normalize the values in a layer.  Stabilize training.\n",
    "\n",
    "Next, compare this with PyTorch's layernorm.  You can do that using something like this - https://discuss.pytorch.org/t/how-to-call-the-backward-function-of-a-custom-module/7853/2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class LayerNorm():\n",
    "    def __init__(self, embed_dim, eps):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Cache for backward pass\n",
    "        self.input = input\n",
    "        # Calculate the mean and standard deviation\n",
    "        self.mean = np.sum(input, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.normed = (input - self.mean)\n",
    "        variance = np.sum(self.normed**2, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.std = np.sqrt(variance + self.eps)\n",
    "        # Normalize the input\n",
    "        return self.normed / self.std\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Find the derivative of numerator (normed)\n",
    "        grad_normed_1 = grad * 1 / self.std\n",
    "\n",
    "        # Derivative of denominator (std)\n",
    "        grad_std = grad * self.normed\n",
    "        # std is a single number\n",
    "        grad_std = np.sum(grad_std, axis=1, keepdims=True)\n",
    "        # Derivative of 1 / std\n",
    "        grad_std = grad_std * -1 / (self.std**2)\n",
    "\n",
    "        # Find gradient against the variance\n",
    "        grad_variance = grad_std * .5 * 1 / self.std\n",
    "\n",
    "        # Find gradient against normed\n",
    "        grad_normed_2 = grad_variance * 1 / self.embed_dim\n",
    "        grad_normed_2 = np.ones_like(self.normed, dtype=self.input.dtype) * grad_normed_2\n",
    "        grad_normed_2 = grad_normed_2 * 2 * self.normed\n",
    "\n",
    "        # Combine two gradients against normed\n",
    "        grad_normed = grad_normed_1 + grad_normed_2\n",
    "\n",
    "        # Find gradient against mean\n",
    "        grad_mean = grad_normed * -1\n",
    "        grad_mean = np.sum(grad_mean, axis=1, keepdims=True)\n",
    "\n",
    "        # Find gradient against input\n",
    "        grad_input_1 = grad_normed\n",
    "        grad_input_2 =  grad_mean * 1 / self.embed_dim\n",
    "        grad_input_2 = grad_input_2 * np.ones_like(self.input, dtype=self.input.dtype)\n",
    "\n",
    "        # Combine two gradients against input\n",
    "        grad_input = grad_input_1 + grad_input_2\n",
    "        return None, grad_input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    LayerNorm(25, 1e-6),\n",
    "    Dense(25, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(5e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Layer Norm\", layers, sgd, train_data, valid_data, name=\"layer_norm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(512)\n",
    "layer_norm.forward(input_embed.forward(data[0][\"en\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Residual connections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
