{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization\n",
    "\n",
    "You may be surprised to learn that the goal of training a neural network is not to minimize the training loss.  This is counterintuitive - of course, we've been training models using gradient descent, where loss decreases over time.\n",
    "\n",
    "However, the training data is just a sample of the population data.  For example, let's say that we have data on house prices, where each row in this dataset represents a single house.  The other columns are:\n",
    "\n",
    "- `interest`: The interest rate\n",
    "- `vacancy`: The vacancy rate\n",
    "- `cpi`: The consumer price index\n",
    "- `price`: The price of a house\n",
    "- `value`: The value of a house\n",
    "- `adj_price`: The price of a house, adjusted for inflation\n",
    "- `adj_value`: The value of a house, adjusted for inflation\n",
    "\n",
    "We have about `700` different rows of data - this is called our sample.  There are many more houses that aren't in our training data - this is called the population.  What we actually want to do is train the model on our sample of data, but use it to make predictions on the population.\n",
    "\n",
    "If our model makes good predictions in our sample, but bad predictions in the population, then that's called overfitting.  Overfitting means that the model won't be useful in the real world.  Neural networks are prone to overfitting, and there are many techniques that have been developed to prevent this.\n",
    "\n",
    "These techniques are broadly called regularization, which adds constraints or penalties to the model's parameters, in order to encourage it to learn simpler and more generalizable representations.  These will usually increase loss in the sample, but decrease loss in the population.\n",
    "\n",
    "Of course, we don't have access to data from the population.  This means that we need to split our sample up into a training set, a validation set, and a test set.  While we're training the model, we'll evaluate on the validation set.  After we've optimized our training method and parameters, we'll evaluate on the test set.  This ensures that we're not using knowledge of the test set when we tune our parameters (this could cause overfitting to the test set).\n",
    "\n",
    "We'll learn three forms of regularization:\n",
    "\n",
    "- Weight decay, which decreases the magnitude of the weights in the optimizer.  This pushes most of the weights towards zero, which encourages the model to learn simpler representations.\n",
    "- Dropout, which randomly sets activations to zero.  This prevents the model from relying on any single activation, and encourages it to learn more generalizable representations.\n",
    "- Early stopping, which stops training when the validation loss starts to increase.  This prevents overfitting by stopping training before the model starts to memorize the training data.\n",
    "\n",
    "There are other forms of regularization, like data augmentation, but these are the most common when working with text."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the data\n",
    "\n",
    "We'll first load in the data.  We'll be using the same data from the last lesson:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.abspath('../data'))\n",
    "sys.path.append(os.path.abspath('../nnets'))\n",
    "from dense import DenseManualUpdate as Dense, forward, backward\n",
    "from csv_data import HousePricesDatasetWrapper\n",
    "import numpy as np\n",
    "from optimizer import Optimizer\n",
    "\n",
    "wrapper = HousePricesDatasetWrapper()\n",
    "train_data, valid_data, test_data = wrapper.get_flat_datasets()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up a training run\n",
    "\n",
    "As you can see, we split the data into three sets.  We'll use the training set to train the model, the validation set to evaluate the model, and the test set to evaluate the model after we've finished training.\n",
    "\n",
    "We'll write a training loop function, which will allow us to test different types of regularization.  The function:\n",
    "\n",
    "- Sets up a new W&B run for monitoring\n",
    "- Loops through the training data batch by batch:\n",
    "    - Makes a prediction\n",
    "    - Finds the error\n",
    "    - Computes the gradient\n",
    "    - Updates the parameters\n",
    "- Logs the loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_SILENT=True\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_SILENT=True\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "def training_run(epochs, regularization, layers, optimizer, train_data, valid_data, name=None):\n",
    "    # Initialize a new W&B run, with the right parameters\n",
    "    wandb.init(project=\"regularization\",\n",
    "               name=name,\n",
    "               config={\"regularization\": regularization})\n",
    "\n",
    "    # Split the training and valid data into x and y\n",
    "    train_x, train_y = train_data\n",
    "    valid_x, valid_y = valid_data\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for i in range(len(train_x)):\n",
    "            # Get the x and y batches\n",
    "            x_batch = train_x[i:(i+1)]\n",
    "            y_batch = train_y[i:(i+1)]\n",
    "            # Make a prediction\n",
    "            pred = forward(x_batch, layers, training=True)\n",
    "\n",
    "            # Run the backward pass\n",
    "            loss = pred - y_batch\n",
    "            layer_grads = backward(loss, layers)\n",
    "            running_loss += np.mean(loss ** 2)\n",
    "\n",
    "            # Run the optimizer\n",
    "            optimizer(layer_grads, layers, 1)\n",
    "\n",
    "        # Calculate and log validation loss\n",
    "        valid_preds = forward(valid_x, layers, training=False)\n",
    "        valid_loss = np.mean((valid_preds - valid_y) ** 2)\n",
    "        train_loss = running_loss / len(train_x)\n",
    "        wandb.log({\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "        })\n",
    "\n",
    "    # Mark the run as complete\n",
    "    wandb.finish()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weight decay\n",
    "\n",
    "Weight decay is l2 regularization.  Goal is to shrink the weights towards 0 (lower the l2 norm).\n",
    "\n",
    "l1 regularization lowers the l1 norm (sum of absolute values of weights)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SGDW(Optimizer):\n",
    "    def __init__(self, lr, decay):\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, layer_grads, layers, batch_size):\n",
    "        # Loop through the layer grads.  Reverse the layers to match the grads (from output backward to input).\n",
    "        for layer_grad, layer in zip(layer_grads, reversed(layers)):\n",
    "            if layer_grad is None:\n",
    "                # Account for dropout layers\n",
    "                continue\n",
    "            w_grad, b_grad = layer_grad\n",
    "\n",
    "            # Normalize the weight gradient by batch size\n",
    "            w_grad /= batch_size\n",
    "\n",
    "            # Calculate the update sizes\n",
    "            w_update = w_grad + self.decay * layer.weights\n",
    "            w_update *= -self.lr\n",
    "            # We don't usually decay the bias\n",
    "            b_update = -self.lr * b_grad\n",
    "\n",
    "            # Actually do the update\n",
    "            layer.update(w_update, b_update)\n",
    "\n",
    "        self.save_vector(layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-4, 0)\n",
    "# Normal SGD\n",
    "training_run(10, \"None\", layers, sgd, train_data, valid_data, name=\"sgd\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "# No decay is equal to SGD\n",
    "sgd = SGDW(1e-4, .1)\n",
    "# Weight decay\n",
    "training_run(10, \"Weight Decay\", layers, sgd, train_data, valid_data, name=\"sgdw\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dropout\n",
    "\n",
    "Dropout prevents overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class Dropout():\n",
    "    def __init__(self, drop_p):\n",
    "        self.drop_p = drop_p\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            # Generate a mask of 0s and 1s\n",
    "            self.mask = np.random.binomial(1, 1-self.drop_p, input.shape)\n",
    "        else:\n",
    "            # No dropout in inference\n",
    "            self.mask = np.ones_like(input)\n",
    "        # Apply the mask.  If the mask is 0, the input is set to 0\n",
    "        return np.where(self.mask, input, 0)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Use np.where to apply the mask\n",
    "        return None, np.where(self.mask, grad, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dropout(.02),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Weight Decay + Dropout\", layers, sgd, train_data, valid_data, name=\"dropout\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Early Stopping\n",
    "\n",
    "Early stopping can prevent overfitting by stopping training when the validation loss is plateauing or increasing.\n",
    "\n",
    "It's common to save checkpoints regularly and then choose the best one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    Dense(25, 10),\n",
    "    Dropout(.05),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(1e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(4, \"Early Stopping\", layers, sgd, train_data, valid_data, name=\"early_stopping\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training convergence\n",
    "\n",
    "Not strictly regularization, but can help with overfitting.  Also help the model converge better."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Improve generalization and convergence\n",
    "# Not strictly regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layernorm\n",
    "\n",
    "Normalize the values in a layer.  Stabilize training.\n",
    "\n",
    "Next, compare this with PyTorch's layernorm.  You can do that using something like this - https://discuss.pytorch.org/t/how-to-call-the-backward-function-of-a-custom-module/7853/2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "class LayerNorm():\n",
    "    def __init__(self, embed_dim, eps):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Cache for backward pass\n",
    "        self.input = input\n",
    "        # Calculate the mean and standard deviation\n",
    "        self.mean = np.sum(input, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.normed = (input - self.mean)\n",
    "        variance = np.sum(self.normed**2, axis=1, keepdims=True) / self.embed_dim\n",
    "        self.std = np.sqrt(variance + self.eps)\n",
    "        # Normalize the input\n",
    "        return self.normed / self.std\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Find the derivative of numerator (normed)\n",
    "        grad_normed_1 = grad * 1 / self.std\n",
    "\n",
    "        # Derivative of denominator (std)\n",
    "        grad_std = grad * self.normed\n",
    "        # std is a single number\n",
    "        grad_std = np.sum(grad_std, axis=1, keepdims=True)\n",
    "        # Derivative of 1 / std\n",
    "        grad_std = grad_std * -1 / (self.std**2)\n",
    "\n",
    "        # Find gradient against the variance\n",
    "        grad_variance = grad_std * .5 * 1 / self.std\n",
    "\n",
    "        # Find gradient against normed\n",
    "        grad_normed_2 = grad_variance * 1 / self.embed_dim\n",
    "        grad_normed_2 = np.ones_like(self.normed, dtype=self.input.dtype) * grad_normed_2\n",
    "        grad_normed_2 = grad_normed_2 * 2 * self.normed\n",
    "\n",
    "        # Combine two gradients against normed\n",
    "        grad_normed = grad_normed_1 + grad_normed_2\n",
    "\n",
    "        # Find gradient against mean\n",
    "        grad_mean = grad_normed * -1\n",
    "        grad_mean = np.sum(grad_mean, axis=1, keepdims=True)\n",
    "\n",
    "        # Find gradient against input\n",
    "        grad_input_1 = grad_normed\n",
    "        grad_input_2 =  grad_mean * 1 / self.embed_dim\n",
    "        grad_input_2 = grad_input_2 * np.ones_like(self.input, dtype=self.input.dtype)\n",
    "\n",
    "        # Combine two gradients against input\n",
    "        grad_input = grad_input_1 + grad_input_2\n",
    "        return None, grad_input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(7, 25),\n",
    "    LayerNorm(25, 1e-6),\n",
    "    Dense(25, 10),\n",
    "    Dense(10, 1, activation=False)\n",
    "]\n",
    "\n",
    "sgd = SGDW(5e-4, .1)\n",
    "# Weight decay and dropout\n",
    "training_run(10, \"Layer Norm\", layers, sgd, train_data, valid_data, name=\"layer_norm\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "layer_norm = LayerNorm(512)\n",
    "layer_norm.forward(input_embed.forward(data[0][\"en\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Residual connections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
